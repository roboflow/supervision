{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70868bca",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ BenchmarkEvaluator Demo\n",
    "\n",
    "This notebook demonstrates how to use `BenchmarkEvaluator` to compute precision/recall metrics for object detection tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee3b103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from supervision.detection.core import Detections\n",
    "from supervision.metrics.benchmark import BenchmarkEvaluator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f806eff5",
   "metadata": {},
   "source": [
    "## Step 1: Create Ground Truth and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65183606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth with 2 boxes\n",
    "gt = Detections(\n",
    "    xyxy=np.array([[10, 10, 100, 100], [150, 150, 300, 300]]),\n",
    "    class_id=np.array([0, 1])\n",
    ")\n",
    "\n",
    "# Predictions: One perfect match, one wrong class\n",
    "pred = Detections(\n",
    "    xyxy=np.array([[10, 10, 100, 100], [150, 150, 300, 300]]),\n",
    "    class_id=np.array([0, 2])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529f0ef0",
   "metadata": {},
   "source": [
    "## Step 2: Run BenchmarkEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5615d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BenchmarkEvaluator(ground_truth=gt, predictions=pred)\n",
    "metrics = evaluator.compute_precision_recall()\n",
    "print(\"Precision:\", metrics[\"precision\"])\n",
    "print(\"Recall:\", metrics[\"recall\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab6f923",
   "metadata": {},
   "source": [
    "## Step 3: Per-Class Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde2bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_class = evaluator.compute_precision_recall_per_class()\n",
    "for cls, metric in per_class.items():\n",
    "    print(f\"Class {cls} - Precision: {metric['precision']:.2f}, Recall: {metric['recall']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1f1e5",
   "metadata": {},
   "source": [
    "## Step 4: Visualize Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a6ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image, detections, color, label):\n",
    "    for box, cls in zip(detections.xyxy, detections.class_id):\n",
    "        x1, y1, x2, y2 = box.astype(int)\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(image, f\"{label}:{cls}\", (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "canvas = np.ones((350, 350, 3), dtype=np.uint8) * 255\n",
    "draw_boxes(canvas, gt, (0, 255, 0), \"GT\")\n",
    "draw_boxes(canvas, pred, (0, 0, 255), \"Pred\")\n",
    "\n",
    "plt.imshow(canvas[..., ::-1])\n",
    "plt.title(\"Ground Truth (Green) vs Prediction (Red)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3d6112",
   "metadata": {},
   "source": [
    "ðŸŽ‰ That's it! You've run a complete object detection benchmark with precision/recall metrics and visualization."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
