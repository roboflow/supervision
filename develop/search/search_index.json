{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Supervision","text":""},{"location":"#hello","title":"\ud83d\udc4b Hello","text":"<p>We write your reusable computer vision tools. Whether you need to load your dataset from your hard drive, draw detections on an image or video, or count how many detections are in a zone. You can count on us!</p>"},{"location":"#install","title":"\ud83d\udcbb Install","text":"<p>You can install <code>supervision</code> in a Python&gt;=3.8 environment.</p> <p>pip install (recommended)</p> headlessdesktop <p>The headless installation of <code>supervision</code> is designed for environments where graphical user interfaces (GUI) are not needed, making it more lightweight and suitable for server-side applications.</p> <pre><code>pip install supervision\n</code></pre> <p>If you require the full version of <code>supervision</code> with GUI support you can install the desktop version. This version includes the GUI components of OpenCV, allowing you to display images and videos on the screen.</p> <pre><code>pip install \"supervision[desktop]\"\n</code></pre> <p>conda/mamba install</p> condamamba <p> </p> <pre><code>conda install -c conda-forge supervision\n</code></pre> <p> </p> <pre><code>mamba install -c conda-forge supervision\n</code></pre> <p>git clone (for development)</p> virtualenvpoetry <pre><code># clone repository and navigate to root directory\ngit clone https://github.com/roboflow/supervision.git\ncd supervision\n\n# setup python environment and activate it\npython3 -m venv venv\nsource venv/bin/activate\npip install --upgrade pip\n\n# headless install\npip install -e \".\"\n\n# desktop install\npip install -e \".[desktop]\"\n</code></pre> <pre><code># clone repository and navigate to root directory\ngit clone https://github.com/roboflow/supervision.git\ncd supervision\n\n# setup python environment and activate it\npoetry env use python3.10\npoetry shell\n\n# headless install\npoetry install\n\n# desktop install\npoetry install --extras \"desktop\"\n</code></pre>"},{"location":"#quickstart","title":"\ud83d\ude80 Quickstart","text":"<ul> <li> <p>Detect and Annotate</p> <p>Annotate predictions from a range of object detection and segmentation models</p> <p> Tutorial</p> </li> <li> <p>Track Objects</p> <p>Discover how to enhance video analysis by implementing seamless object tracking</p> <p> Tutorial</p> </li> <li> <p>Detect Small Objects</p> <p>Learn how to detect small objects in images</p> <p> Tutorial</p> </li> <li> <p>Count Objects Crossing Line</p> <p>Explore methods to accurately count and analyze objects crossing a predefined line</p> </li> <li> <p>Filter Objects in Zone</p> <p>Master the techniques to selectively filter and focus on objects within a specific zone</p> </li> <li> <p>Cheatsheet</p> <p>Access a quick reference guide to the most common <code>supervision</code> functions</p> <p> Cheatsheet</p> </li> </ul>"},{"location":"assets/","title":"Assets","text":"<p>Supervision offers an assets download utility that allows you to download video files that you can use in your demos.</p>"},{"location":"assets/#install-extra","title":"Install extradownload_assetsVideoAssets","text":"<p>To install the Supervision assets utility, you can use <code>pip</code>. This utility is available as an extra within the Supervision package.</p> <p>pip install</p> <pre><code>pip install \"supervision[assets]\"\n</code></pre> <p>Download a specified asset if it doesn't already exist or is corrupted.</p> <p>Parameters:</p> Name Type Description Default <code>asset_name</code> <code>Union[VideoAssets, str]</code> <p>The name or type of the asset to be downloaded.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The filename of the downloaded asset.</p> Example <pre><code>from supervision.assets import download_assets, VideoAssets\n\ndownload_assets(VideoAssets.VEHICLES)\n\"vehicles.mp4\"\n</code></pre> Source code in <code>supervision/assets/downloader.py</code> <pre><code>def download_assets(asset_name: Union[VideoAssets, str]) -&gt; str:\n    \"\"\"\n    Download a specified asset if it doesn't already exist or is corrupted.\n\n    Parameters:\n        asset_name (Union[VideoAssets, str]): The name or type of the asset to be\n            downloaded.\n\n    Returns:\n        str: The filename of the downloaded asset.\n\n    Example:\n        ```python\n        from supervision.assets import download_assets, VideoAssets\n\n        download_assets(VideoAssets.VEHICLES)\n        \"vehicles.mp4\"\n        ```\n    \"\"\"\n\n    filename = asset_name.value if isinstance(asset_name, VideoAssets) else asset_name\n\n    if not Path(filename).exists() and filename in VIDEO_ASSETS:\n        print(f\"Downloading {filename} assets \\n\")\n        response = get(VIDEO_ASSETS[filename][0], stream=True, allow_redirects=True)\n        response.raise_for_status()\n\n        file_size = int(response.headers.get(\"Content-Length\", 0))\n        folder_path = Path(filename).expanduser().resolve()\n        folder_path.parent.mkdir(parents=True, exist_ok=True)\n\n        with tqdm.wrapattr(\n            response.raw, \"read\", total=file_size, desc=\"\", colour=\"#a351fb\"\n        ) as raw_resp:\n            with folder_path.open(\"wb\") as file:\n                copyfileobj(raw_resp, file)\n\n    elif Path(filename).exists():\n        if not is_md5_hash_matching(filename, VIDEO_ASSETS[filename][1]):\n            print(\"File corrupted. Re-downloading... \\n\")\n            os.remove(filename)\n            return download_assets(filename)\n\n        print(f\"{filename} asset download complete. \\n\")\n\n    else:\n        valid_assets = \", \".join(asset.value for asset in VideoAssets)\n        raise ValueError(\n            f\"Invalid asset. It should be one of the following: {valid_assets}.\"\n        )\n\n    return filename\n</code></pre> <p>               Bases: <code>Enum</code></p> <p>Each member of this enum represents a video asset. The value associated with each member is the filename of the video.</p> Enum Member Video Filename Video URL <code>VEHICLES</code> <code>vehicles.mp4</code> Link <code>MILK_BOTTLING_PLANT</code> <code>milk-bottling-plant.mp4</code> Link <code>VEHICLES_2</code> <code>vehicles-2.mp4</code> Link <code>GROCERY_STORE</code> <code>grocery-store.mp4</code> Link <code>SUBWAY</code> <code>subway.mp4</code> Link <code>MARKET_SQUARE</code> <code>market-square.mp4</code> Link <code>PEOPLE_WALKING</code> <code>people-walking.mp4</code> Link <code>BEACH</code> <code>beach-1.mp4</code> Link <code>BASKETBALL</code> <code>basketball-1.mp4</code> Link Source code in <code>supervision/assets/list.py</code> <pre><code>class VideoAssets(Enum):\n    \"\"\"\n    Each member of this enum represents a video asset. The value associated with each\n    member is the filename of the video.\n\n    | Enum Member            | Video Filename             | Video URL                                                                             |\n    |------------------------|----------------------------|---------------------------------------------------------------------------------------|\n    | `VEHICLES`             | `vehicles.mp4`             | [Link](https://media.roboflow.com/supervision/video-examples/vehicles.mp4)            |\n    | `MILK_BOTTLING_PLANT`  | `milk-bottling-plant.mp4`  | [Link](https://media.roboflow.com/supervision/video-examples/milk-bottling-plant.mp4) |\n    | `VEHICLES_2`           | `vehicles-2.mp4`           | [Link](https://media.roboflow.com/supervision/video-examples/vehicles-2.mp4)          |\n    | `GROCERY_STORE`        | `grocery-store.mp4`        | [Link](https://media.roboflow.com/supervision/video-examples/grocery-store.mp4)       |\n    | `SUBWAY`               | `subway.mp4`               | [Link](https://media.roboflow.com/supervision/video-examples/subway.mp4)              |\n    | `MARKET_SQUARE`        | `market-square.mp4`        | [Link](https://media.roboflow.com/supervision/video-examples/market-square.mp4)       |\n    | `PEOPLE_WALKING`       | `people-walking.mp4`       | [Link](https://media.roboflow.com/supervision/video-examples/people-walking.mp4)      |\n    | `BEACH`                | `beach-1.mp4`              | [Link](https://media.roboflow.com/supervision/video-examples/beach-1.mp4)             |\n    | `BASKETBALL`           | `basketball-1.mp4`         | [Link](https://media.roboflow.com/supervision/video-examples/basketball-1.mp4)        |\n    \"\"\"  # noqa: E501 // docs\n\n    VEHICLES = \"vehicles.mp4\"\n    MILK_BOTTLING_PLANT = \"milk-bottling-plant.mp4\"\n    VEHICLES_2 = \"vehicles-2.mp4\"\n    GROCERY_STORE = \"grocery-store.mp4\"\n    SUBWAY = \"subway.mp4\"\n    MARKET_SQUARE = \"market-square.mp4\"\n    PEOPLE_WALKING = \"people-walking.mp4\"\n    BEACH = \"beach-1.mp4\"\n    BASKETBALL = \"basketball-1.mp4\"\n\n    @classmethod\n    def list(cls):\n        return list(map(lambda c: c.value, cls))\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#0220-jul-12-2024","title":"0.22.0 Jul 12, 2024","text":"<ul> <li>Added #1326: <code>sv.DetectionsDataset</code> and <code>sv.ClassificationDataset</code> allowing to load the images into memory only when necessary (lazy loading).</li> </ul> <p>Deprecated</p> <p>Constructing <code>DetectionDataset</code> with parameter <code>images</code> as <code>Dict[str, np.ndarray]</code> is deprecated and will be removed in <code>supervision-0.26.0</code>. Please pass a list of paths <code>List[str]</code> instead.</p> <p>Deprecated</p> <p>The <code>DetectionDataset.images</code> property is deprecated and will be removed in <code>supervision-0.26.0</code>. Please loop over images with <code>for path, image, annotation in dataset:</code>, as that does not require loading all images into memory.</p> <pre><code>import roboflow\nfrom roboflow import Roboflow\nimport supervision as sv\n\nroboflow.login()\nrf = Roboflow()\n\nproject = rf.workspace(&lt;WORKSPACE_ID&gt;).project(&lt;PROJECT_ID&gt;)\ndataset = project.version(&lt;PROJECT_VERSION&gt;).download(\"coco\")\n\nds_train = sv.DetectionDataset.from_coco(\n    images_directory_path=f\"{dataset.location}/train\",\n    annotations_path=f\"{dataset.location}/train/_annotations.coco.json\",\n)\n\npath, image, annotation = ds_train[0]\n    # loads image on demand\n\nfor path, image, annotation in ds_train:\n    # loads image on demand\n</code></pre> <ul> <li> <p>Added #1296: <code>sv.Detections.from_lmm</code> now supports parsing results from the Florence 2 model, extending the capability to handle outputs from this Large Multimodal Model (LMM). This includes detailed object detection, OCR with region proposals, segmentation, and more. Find out more in our Colab notebook.</p> </li> <li> <p>Added #1232 to support keypoint detection with Mediapipe. Both legacy and modern pipelines are supported. See <code>sv.KeyPoints.from_mediapipe</code> for more.</p> </li> <li> <p>Added #1316: <code>sv.KeyPoints.from_mediapipe</code> extended to support FaceMesh from Mediapipe. This enhancement allows for processing both face landmarks from <code>FaceLandmarker</code>, and legacy results from <code>FaceMesh</code>.</p> </li> <li> <p>Added #1310: <code>sv.KeyPoints.from_detectron2</code> is a new <code>KeyPoints</code> method, adding support for extracting keypoints from the popular Detectron 2 platform.</p> </li> <li> <p>Added #1300: <code>sv.Detections.from_detectron2</code> now supports segmentation models detectron2. The resulting masks can be used with <code>sv.MaskAnnotator</code> for displaying annotations.</p> </li> </ul> <pre><code>import supervision as sv\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nimport cv2\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\npredictor = DefaultPredictor(cfg)\n\nresult = predictor(image)\ndetections = sv.Detections.from_detectron2(result)\n\nmask_annotator = sv.MaskAnnotator()\nannotated_frame = mask_annotator.annotate(scene=image.copy(), detections=detections)\n</code></pre> <ul> <li>Added #1277: if you provide a font that supports symbols of a language, <code>sv.RichLabelAnnotator</code> will draw them on your images.</li> <li>Various other annotators have been revised to ensure proper in-place functionality when used with <code>numpy</code> arrays. Additionally, we fixed a bug where <code>sv.ColorAnnotator</code> was filling boxes with solid color when used in-place.</li> </ul> <pre><code>import cv2\nimport supervision as sv\nimport\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n\nmodel = get_model(model_id=\"yolov8n-640\")\nresults = model.infer(image)[0]\ndetections = sv.Detections.from_inference(results)\n\nrich_label_annotator = sv.RichLabelAnnotator(font_path=&lt;TTF_FONT_PATH&gt;)\nannotated_image = rich_label_annotator.annotate(scene=image.copy(), detections=detections)\n</code></pre> <ul> <li>Added #1227: Added support for loading Oriented Bounding Boxes dataset in YOLO format.</li> </ul> <pre><code>import supervision as sv\n\ntrain_ds = sv.DetectionDataset.from_yolo(\n    images_directory_path=\"/content/dataset/train/images\",\n    annotations_directory_path=\"/content/dataset/train/labels\",\n    data_yaml_path=\"/content/dataset/data.yaml\",\n    is_obb=True\n)\n\n_, image, detections in train_ds[0]\n\nobb_annotator = OrientedBoxAnnotator()\nannotated_image = obb_annotator.annotate(scene=image.copy(), detections=detections)\n</code></pre> <ul> <li>Fixed #1312: Fixed <code>CropAnnotator</code>.</li> </ul> <p>Removed</p> <p><code>BoxAnnotator</code> was removed, however <code>BoundingBoxAnnotator</code> has been renamed to <code>BoxAnnotator</code>. Use a combination of <code>BoxAnnotator</code> and <code>LabelAnnotator</code> to simulate old <code>BoundingBox</code> behavior.</p> <p>Deprecated</p> <p>The name <code>BoundingBoxAnnotator</code> has been deprecated and will be removed in <code>supervision-0.26.0</code>. It has been renamed to <code>BoxAnnotator</code>.</p> <ul> <li> <p>Added #975 \ud83d\udcdd New Cookbooks: serialize detections into json and csv.</p> </li> <li> <p>Added #1290: Mostly an internal change, our file utility function now support both <code>str</code> and <code>pathlib</code> paths.</p> </li> <li> <p>Added #1340: Two new methods for converting between bounding box formats - <code>xywh_to_xyxy</code> and <code>xcycwh_to_xyxy</code></p> </li> </ul> <p>Removed</p> <p><code>from_roboflow</code> method has been removed due to deprecation. Use from_inference instead.</p> <p>Removed</p> <p><code>Color.white()</code> has been removed due to deprecation. Use <code>color.WHITE</code> instead.</p> <p>Removed</p> <p><code>Color.black()</code> has been removed due to deprecation. Use <code>color.BLACK</code> instead.</p> <p>Removed</p> <p><code>Color.red()</code> has been removed due to deprecation. Use <code>color.RED</code> instead.</p> <p>Removed</p> <p><code>Color.green()</code> has been removed due to deprecation. Use <code>color.GREEN</code> instead.</p> <p>Removed</p> <p><code>Color.blue()</code> has been removed due to deprecation. Use <code>color.BLUE</code> instead.</p> <p>Removed</p> <p><code>ColorPalette.default()</code> has been removed due to deprecation. Use ColorPalette.DEFAULT instead.</p> <p>Removed</p> <p><code>FPSMonitor.__call__</code> has been removed due to deprecation. Use the attribute FPSMonitor.fps instead.</p>"},{"location":"changelog/#0210-jun-5-2024","title":"0.21.0 Jun 5, 2024","text":"<ul> <li> <p>Added #500: <code>sv.Detections.with_nmm</code> to perform non-maximum merging on the current set of object detections.</p> </li> <li> <p>Added #1221: <code>sv.Detections.from_lmm</code> allowing to parse Large Multimodal Model (LMM) text result into <code>sv.Detections</code> object. For now <code>from_lmm</code> supports only PaliGemma result parsing.</p> </li> </ul> <pre><code>import supervision as sv\n\npaligemma_result = \"&lt;loc0256&gt;&lt;loc0256&gt;&lt;loc0768&gt;&lt;loc0768&gt; cat\"\ndetections = sv.Detections.from_lmm(\n    sv.LMM.PALIGEMMA,\n    paligemma_result,\n    resolution_wh=(1000, 1000),\n    classes=['cat', 'dog']\n)\ndetections.xyxy\n# array([[250., 250., 750., 750.]])\n\ndetections.class_id\n# array([0])\n</code></pre> <ul> <li>Added #1236: <code>sv.VertexLabelAnnotator</code> allowing to annotate every vertex of a keypoint skeleton with custom text and color.</li> </ul> <pre><code>import supervision as sv\n\nimage = ...\nkey_points = sv.KeyPoints(...)\n\nedge_annotator = sv.EdgeAnnotator(\n    color=sv.Color.GREEN,\n    thickness=5\n)\nannotated_frame = edge_annotator.annotate(\n    scene=image.copy(),\n    key_points=key_points\n)\n</code></pre> <ul> <li> <p>Added #1147: <code>sv.KeyPoints.from_inference</code> allowing to create <code>sv.KeyPoints</code> from Inference result.</p> </li> <li> <p>Added #1138: <code>sv.KeyPoints.from_yolo_nas</code> allowing to create <code>sv.KeyPoints</code> from YOLO-NAS result.</p> </li> <li> <p>Added #1163: <code>sv.mask_to_rle</code> and <code>sv.rle_to_mask</code> allowing for easy conversion between mask and rle formats.</p> </li> <li> <p>Changed #1236: <code>sv.InferenceSlicer</code> allowing to select overlap filtering strategy (<code>NONE</code>, <code>NON_MAX_SUPPRESSION</code> and <code>NON_MAX_MERGE</code>).</p> </li> <li> <p>Changed #1178: <code>sv.InferenceSlicer</code> adding instance segmentation model support.</p> </li> </ul> <pre><code>import cv2\nimport numpy as np\nimport supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8x-seg-640\")\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n\ndef callback(image_slice: np.ndarray) -&gt; sv.Detections:\n    results = model.infer(image_slice)[0]\n    return sv.Detections.from_inference(results)\n\nslicer = sv.InferenceSlicer(callback = callback)\ndetections = slicer(image)\n\nmask_annotator = sv.MaskAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = mask_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n</code></pre> <ul> <li> <p>Changed #1228: <code>sv.LineZone</code> making it 10-20 times faster, depending on the use case.</p> </li> <li> <p>Changed #1163: <code>sv.DetectionDataset.from_coco</code> and <code>sv.DetectionDataset.as_coco</code> adding support for run-length encoding (RLE) mask format.</p> </li> </ul>"},{"location":"changelog/#0200-april-24-2024","title":"0.20.0 April 24, 2024","text":"<ul> <li> <p>Added #1128: <code>sv.KeyPoints</code> to provide initial support for pose estimation and broader keypoint detection models.</p> </li> <li> <p>Added #1128: <code>sv.EdgeAnnotator</code> and <code>sv.VertexAnnotator</code> to enable rendering of results from keypoint detection models.</p> </li> </ul> <pre><code>import cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nmodel = YOLO('yolov8l-pose')\n\nresult = model(image, verbose=False)[0]\nkeypoints = sv.KeyPoints.from_ultralytics(result)\n\nedge_annotators = sv.EdgeAnnotator(color=sv.Color.GREEN, thickness=5)\nannotated_image = edge_annotators.annotate(image.copy(), keypoints)\n</code></pre> <ul> <li> <p>Changed #1037: <code>sv.LabelAnnotator</code> by adding an additional <code>corner_radius</code> argument that allows for rounding the corners of the bounding box.</p> </li> <li> <p>Changed #1109: <code>sv.PolygonZone</code> such that the <code>frame_resolution_wh</code> argument is no longer required to initialize <code>sv.PolygonZone</code>.</p> </li> </ul> <p>Deprecated</p> <p>The <code>frame_resolution_wh</code> parameter in <code>sv.PolygonZone</code> is deprecated and will be removed in <code>supervision-0.24.0</code>.</p> <ul> <li> <p>Changed #1084: <code>sv.get_polygon_center</code> to calculate a more accurate polygon centroid.</p> </li> <li> <p>Changed #1069: <code>sv.Detections.from_transformers</code> by adding support for Transformers segmentation models and extract class names values.</p> </li> </ul> <pre><code>import torch\nimport supervision as sv\nfrom PIL import Image\nfrom transformers import DetrImageProcessor, DetrForSegmentation\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\nmodel = DetrForSegmentation.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n\nimage = Image.open(&lt;SOURCE_IMAGE_PATH&gt;)\ninputs = processor(images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nwidth, height = image.size\ntarget_size = torch.tensor([[height, width]])\nresults = processor.post_process_segmentation(\n    outputs=outputs, target_sizes=target_size)[0]\ndetections = sv.Detections.from_transformers(results, id2label=model.config.id2label)\n\nmask_annotator = sv.MaskAnnotator()\nlabel_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n\nannotated_image = mask_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n</code></pre> <ul> <li>Fixed #787: <code>sv.ByteTrack.update_with_detections</code> which was removing segmentation masks while tracking. Now, <code>ByteTrack</code> can be used alongside segmentation models.</li> </ul>"},{"location":"changelog/#0190-march-15-2024","title":"0.19.0 March 15, 2024","text":"<ul> <li>Added #818: <code>sv.CSVSink</code> allowing for the straightforward saving of image, video, or stream inference results in a <code>.csv</code> file.</li> </ul> <pre><code>import supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(&lt;SOURCE_MODEL_PATH&gt;)\ncsv_sink = sv.CSVSink(&lt;RESULT_CSV_FILE_PATH&gt;)\nframes_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;)\n\nwith csv_sink:\n    for frame in frames_generator:\n        result = model(frame)[0]\n        detections = sv.Detections.from_ultralytics(result)\n        csv_sink.append(detections, custom_data={&lt;CUSTOM_LABEL&gt;:&lt;CUSTOM_DATA&gt;})\n</code></pre> <ul> <li>Added #819: <code>sv.JSONSink</code> allowing for the straightforward saving of image, video, or stream inference results in a <code>.json</code> file.</li> </ul> <pre><code>import supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(&lt;SOURCE_MODEL_PATH&gt;)\njson_sink = sv.JSONSink(&lt;RESULT_JSON_FILE_PATH&gt;)\nframes_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;)\n\nwith json_sink:\n    for frame in frames_generator:\n        result = model(frame)[0]\n        detections = sv.Detections.from_ultralytics(result)\n        json_sink.append(detections, custom_data={&lt;CUSTOM_LABEL&gt;:&lt;CUSTOM_DATA&gt;})\n</code></pre> <ul> <li> <p>Added #847: <code>sv.mask_iou_batch</code> allowing to compute Intersection over Union (IoU) of two sets of masks.</p> </li> <li> <p>Added #847: <code>sv.mask_non_max_suppression</code> allowing to perform Non-Maximum Suppression (NMS) on segmentation predictions.</p> </li> <li> <p>Added #888: <code>sv.CropAnnotator</code> allowing users to annotate the scene with scaled-up crops of detections.</p> </li> </ul> <pre><code>import cv2\nimport supervision as sv\nfrom inference import get_model\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nmodel = get_model(model_id=\"yolov8n-640\")\n\nresult = model.infer(image)[0]\ndetections = sv.Detections.from_inference(result)\n\ncrop_annotator = sv.CropAnnotator()\nannotated_frame = crop_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <ul> <li> <p>Changed #827: <code>sv.ByteTrack.reset</code> allowing users to clear trackers state, enabling the processing of multiple video files in sequence.</p> </li> <li> <p>Changed #802: <code>sv.LineZoneAnnotator</code> allowing to hide in/out count using <code>display_in_count</code> and <code>display_out_count</code> properties.</p> </li> <li> <p>Changed #787: <code>sv.ByteTrack</code> input arguments and docstrings updated to improve readability and ease of use.</p> </li> </ul> <p>Deprecated</p> <p>The <code>track_buffer</code>, <code>track_thresh</code>, and <code>match_thresh</code> parameters in <code>sv.ByterTrack</code> are deprecated and will be removed in <code>supervision-0.23.0</code>. Use <code>lost_track_buffer,</code> <code>track_activation_threshold</code>, and <code>minimum_matching_threshold</code> instead.</p> <ul> <li>Changed #910: <code>sv.PolygonZone</code> to now accept a list of specific box anchors that must be in zone for a detection to be counted.</li> </ul> <p>Deprecated</p> <p>The <code>triggering_position</code> parameter in <code>sv.PolygonZone</code> is deprecated and will be removed in <code>supervision-0.23.0</code>. Use <code>triggering_anchors</code> instead.</p> <ul> <li> <p>Changed #875: annotators adding support for Pillow images. All supervision Annotators can now accept an image as either a numpy array or a Pillow Image. They automatically detect its type, draw annotations, and return the output in the same format as the input.</p> </li> <li> <p>Fixed #944: <code>sv.DetectionsSmoother</code> removing <code>tracking_id</code> from <code>sv.Detections</code>.</p> </li> </ul>"},{"location":"changelog/#0180-january-25-2024","title":"0.18.0 January 25, 2024","text":"<ul> <li>Added #720: <code>sv.PercentageBarAnnotator</code> allowing to annotate images and videos with percentage values representing confidence or other custom property.</li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; percentage_bar_annotator = sv.PercentageBarAnnotator()\n&gt;&gt;&gt; annotated_frame = percentage_bar_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <ul> <li> <p>Added #702: <code>sv.RoundBoxAnnotator</code> allowing to annotate images and videos with rounded corners bounding boxes.</p> </li> <li> <p>Added #770: <code>sv.OrientedBoxAnnotator</code> allowing to annotate images and videos with OBB (Oriented Bounding Boxes).</p> </li> </ul> <pre><code>import cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nmodel = YOLO(\"yolov8n-obb.pt\")\n\nresult = model(image)[0]\ndetections = sv.Detections.from_ultralytics(result)\n\noriented_box_annotator = sv.OrientedBoxAnnotator()\nannotated_frame = oriented_box_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <ul> <li> <p>Added #696: <code>sv.DetectionsSmoother</code> allowing for smoothing detections over multiple frames in video tracking.</p> </li> <li> <p>Added #769: <code>sv.ColorPalette.from_matplotlib</code> allowing users to create a <code>sv.ColorPalette</code> instance from a Matplotlib color palette.</p> </li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; sv.ColorPalette.from_matplotlib('viridis', 5)\nColorPalette(colors=[Color(r=68, g=1, b=84), Color(r=59, g=82, b=139), ...])\n</code></pre> <ul> <li> <p>Changed #770: <code>sv.Detections.from_ultralytics</code> adding support for OBB (Oriented Bounding Boxes).</p> </li> <li> <p>Changed #735: <code>sv.LineZone</code> to now accept a list of specific box anchors that must cross the line for a detection to be counted. This update marks a significant improvement from the previous requirement, where all four box corners were necessary. Users can now specify a single anchor, such as <code>sv.Position.BOTTOM_CENTER</code>, or any other combination of anchors defined as <code>List[sv.Position]</code>.</p> </li> <li> <p>Changed #756: <code>sv.Color</code>'s and <code>sv.ColorPalette</code>'s method of accessing predefined colors, transitioning from a function-based approach (<code>sv.Color.red()</code>) to a more intuitive and conventional property-based method (<code>sv.Color.RED</code>).</p> </li> </ul> <p>Deprecated</p> <p><code>sv.ColorPalette.default()</code> is deprecated and will be removed in <code>supervision-0.22.0</code>. Use <code>sv.ColorPalette.DEFAULT</code> instead.</p> <ul> <li> <p>Changed #769: <code>sv.ColorPalette.DEFAULT</code> value, giving users a more extensive set of annotation colors.</p> </li> <li> <p>Changed #677: <code>sv.Detections.from_roboflow</code> to <code>sv.Detections.from_inference</code> streamlining its functionality to be compatible with both the both inference pip package and the Robloflow hosted API.</p> </li> </ul> <p>Deprecated</p> <p><code>Detections.from_roboflow()</code> is deprecated and will be removed in <code>supervision-0.22.0</code>. Use <code>Detections.from_inference</code> instead.</p> <ul> <li>Fixed #735: <code>sv.LineZone</code> functionality to accurately update the counter when an object crosses a line from any direction, including from the side. This enhancement enables more precise tracking and analytics, such as calculating individual in/out counts for each lane on the road.</li> </ul>"},{"location":"changelog/#0170-december-06-2023","title":"0.17.0 December 06, 2023","text":"<ul> <li> <p>Added #633: <code>sv.PixelateAnnotator</code> allowing to pixelate objects on images and videos.</p> </li> <li> <p>Added #652: <code>sv.TriangleAnnotator</code> allowing to annotate images and videos with triangle markers.</p> </li> <li> <p>Added #602: <code>sv.PolygonAnnotator</code> allowing to annotate images and videos with segmentation mask outline.</p> </li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; polygon_annotator = sv.PolygonAnnotator()\n&gt;&gt;&gt; annotated_frame = polygon_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <ul> <li>Added #476: <code>sv.assets</code> allowing download of video files that you can use in your demos.</li> </ul> <pre><code>&gt;&gt;&gt; from supervision.assets import download_assets, VideoAssets\n&gt;&gt;&gt; download_assets(VideoAssets.VEHICLES)\n\"vehicles.mp4\"\n</code></pre> <ul> <li> <p>Added #605: <code>Position.CENTER_OF_MASS</code> allowing to place labels in center of mass of segmentation masks.</p> </li> <li> <p>Added #651: <code>sv.scale_boxes</code> allowing to scale <code>sv.Detections.xyxy</code> values.</p> </li> <li> <p>Added #637: <code>sv.calculate_dynamic_text_scale</code> and <code>sv.calculate_dynamic_line_thickness</code> allowing text scale and line thickness to match image resolution.</p> </li> <li> <p>Added #620: <code>sv.Color.as_hex</code> allowing to extract color value in HEX format.</p> </li> <li> <p>Added #572: <code>sv.Classifications.from_timm</code> allowing to load classification result from timm models.</p> </li> <li> <p>Added #478: <code>sv.Classifications.from_clip</code> allowing to load classification result from clip model.</p> </li> <li> <p>Added #571: <code>sv.Detections.from_azure_analyze_image</code> allowing to load detection results from Azure Image Analysis.</p> </li> <li> <p>Changed #646: <code>sv.BoxMaskAnnotator</code> renaming it to <code>sv.ColorAnnotator</code>.</p> </li> <li> <p>Changed #606: <code>sv.MaskAnnotator</code> to make it 5x faster.</p> </li> <li> <p>Fixed #584: <code>sv.DetectionDataset.from_yolo</code> to ignore empty lines in annotation files.</p> </li> <li> <p>Fixed #555: <code>sv.BlurAnnotator</code> to trim negative coordinates before bluring detections.</p> </li> <li> <p>Fixed #511: <code>sv.TraceAnnotator</code> to respect trace position.</p> </li> </ul>"},{"location":"changelog/#0160-october-19-2023","title":"0.16.0 October 19, 2023","text":"<ul> <li> <p>Added #422: <code>sv.BoxMaskAnnotator</code> allowing to annotate images and videos with mox masks.</p> </li> <li> <p>Added #433: <code>sv.HaloAnnotator</code> allowing to annotate images and videos with halo effect.</p> </li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; halo_annotator = sv.HaloAnnotator()\n&gt;&gt;&gt; annotated_frame = halo_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <ul> <li> <p>Added #466: <code>sv.HeatMapAnnotator</code> allowing to annotate videos with heat maps.</p> </li> <li> <p>Added #492: <code>sv.DotAnnotator</code> allowing to annotate images and videos with dots.</p> </li> <li> <p>Added #449: <code>sv.draw_image</code> allowing to draw an image onto a given scene with specified opacity and dimensions.</p> </li> <li> <p>Added #280: <code>sv.FPSMonitor</code> for monitoring frames per second (FPS) to benchmark latency.</p> </li> <li> <p>Added #454: \ud83e\udd17 Hugging Face Annotators space.</p> </li> <li> <p>Changed #482: <code>sv.LineZone.trigger</code> now return <code>Tuple[np.ndarray, np.ndarray]</code>. The first array indicates which detections have crossed the line from outside to inside. The second array indicates which detections have crossed the line from inside to outside.</p> </li> <li> <p>Changed #465: Annotator argument name from <code>color_map: str</code> to <code>color_lookup: ColorLookup</code> enum to increase type safety.</p> </li> <li> <p>Changed #426: <code>sv.MaskAnnotator</code> allowing 2x faster annotation.</p> </li> <li> <p>Fixed #477: Poetry env definition allowing proper local installation.</p> </li> <li> <p>Fixed #430: <code>sv.ByteTrack</code> to return <code>np.array([], dtype=int)</code> when <code>svDetections</code> is empty.</p> </li> </ul> <p>Deprecated</p> <p><code>sv.Detections.from_yolov8</code> and <code>sv.Classifications.from_yolov8</code> as those are now replaced by <code>sv.Detections.from_ultralytics</code> and <code>sv.Classifications.from_ultralytics</code>.</p>"},{"location":"changelog/#0150-october-5-2023","title":"0.15.0 October 5, 2023","text":"<ul> <li> <p>Added #170: <code>sv.BoundingBoxAnnotator</code> allowing to annotate images and videos with bounding boxes.</p> </li> <li> <p>Added #170: <code>sv.BoxCornerAnnotator</code> allowing to annotate images and videos with just bounding box corners.</p> </li> <li> <p>Added #170: <code>sv.MaskAnnotator</code> allowing to annotate images and videos with segmentation masks.</p> </li> <li> <p>Added #170: <code>sv.EllipseAnnotator</code> allowing to annotate images and videos with ellipses (sports game style).</p> </li> <li> <p>Added #386: <code>sv.CircleAnnotator</code> allowing to annotate images and videos with circles.</p> </li> <li> <p>Added #354: <code>sv.TraceAnnotator</code> allowing to draw path of moving objects on videos.</p> </li> <li> <p>Added #405: <code>sv.BlurAnnotator</code> allowing to blur objects on images and videos.</p> </li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; image = ...\n&gt;&gt;&gt; detections = sv.Detections(...)\n\n&gt;&gt;&gt; bounding_box_annotator = sv.BoundingBoxAnnotator()\n&gt;&gt;&gt; annotated_frame = bounding_box_annotator.annotate(\n...     scene=image.copy(),\n...     detections=detections\n... )\n</code></pre> <ul> <li> <p>Added #354: Supervision usage example. You can now learn how to perform traffic flow analysis with Supervision.</p> </li> <li> <p>Changed #399: <code>sv.Detections.from_roboflow</code> now does not require <code>class_list</code> to be specified. The <code>class_id</code> value can be extracted directly from the inference response.</p> </li> <li> <p>Changed #381: <code>sv.VideoSink</code> now allows to customize the output codec.</p> </li> <li> <p>Changed #361: <code>sv.InferenceSlicer</code> can now operate in multithreading mode.</p> </li> <li> <p>Fixed #348: <code>sv.Detections.from_deepsparse</code> to allow processing empty deepsparse result object.</p> </li> </ul>"},{"location":"changelog/#0140-august-31-2023","title":"0.14.0 August 31, 2023","text":"<ul> <li>Added #282: support for SAHI inference technique with <code>sv.InferenceSlicer</code>.</li> </ul> <pre><code>&gt;&gt;&gt; import cv2\n&gt;&gt;&gt; import supervision as sv\n&gt;&gt;&gt; from ultralytics import YOLO\n\n&gt;&gt;&gt; image = cv2.imread(SOURCE_IMAGE_PATH)\n&gt;&gt;&gt; model = YOLO(...)\n\n&gt;&gt;&gt; def callback(image_slice: np.ndarray) -&gt; sv.Detections:\n...     result = model(image_slice)[0]\n...     return sv.Detections.from_ultralytics(result)\n\n&gt;&gt;&gt; slicer = sv.InferenceSlicer(callback = callback)\n\n&gt;&gt;&gt; detections = slicer(image)\n</code></pre> <ul> <li> <p>Added #297: <code>Detections.from_deepsparse</code> to enable seamless integration with DeepSparse framework.</p> </li> <li> <p>Added #281: <code>sv.Classifications.from_ultralytics</code> to enable seamless integration with Ultralytics framework. This will enable you to use supervision with all models that Ultralytics supports.</p> </li> </ul> <p>Deprecated</p> <p>sv.Detections.from_yolov8 and sv.Classifications.from_yolov8 are now deprecated and will be removed with <code>supervision-0.16.0</code> release.</p> <ul> <li> <p>Added #341: First supervision usage example script showing how to detect and track objects on video using YOLOv8 + Supervision.</p> </li> <li> <p>Changed #296: <code>sv.ClassificationDataset</code> and <code>sv.DetectionDataset</code> now use image path (not image name) as dataset keys.</p> </li> <li> <p>Fixed #300: <code>Detections.from_roboflow</code> to filter out polygons with less than 3 points.</p> </li> </ul>"},{"location":"changelog/#0130-august-8-2023","title":"0.13.0 August 8, 2023","text":"<ul> <li>Added #236: support for mean average precision (mAP) for object detection models with <code>sv.MeanAveragePrecision</code>.</li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n&gt;&gt;&gt; from ultralytics import YOLO\n\n&gt;&gt;&gt; dataset = sv.DetectionDataset.from_yolo(...)\n\n&gt;&gt;&gt; model = YOLO(...)\n&gt;&gt;&gt; def callback(image: np.ndarray) -&gt; sv.Detections:\n...     result = model(image)[0]\n...     return sv.Detections.from_yolov8(result)\n\n&gt;&gt;&gt; mean_average_precision = sv.MeanAveragePrecision.benchmark(\n...     dataset = dataset,\n...     callback = callback\n... )\n\n&gt;&gt;&gt; mean_average_precision.map50_95\n0.433\n</code></pre> <ul> <li> <p>Added #256: support for ByteTrack for object tracking with <code>sv.ByteTrack</code>.</p> </li> <li> <p>Added #222: <code>sv.Detections.from_ultralytics</code> to enable seamless integration with Ultralytics framework. This will enable you to use <code>supervision</code> with all models that Ultralytics supports.</p> </li> </ul> <p>Deprecated</p> <p><code>sv.Detections.from_yolov8</code> is now deprecated and will be removed with <code>supervision-0.15.0</code> release.</p> <ul> <li> <p>Added #191: <code>sv.Detections.from_paddledet</code> to enable seamless integration with PaddleDetection framework.</p> </li> <li> <p>Added #245: support for loading PASCAL VOC segmentation datasets with <code>sv.DetectionDataset.</code>.</p> </li> </ul>"},{"location":"changelog/#0120-july-24-2023","title":"0.12.0 July 24, 2023","text":"<p>Python 3.7. Support Terminated</p> <p>With the <code>supervision-0.12.0</code> release, we are terminating official support for Python 3.7.</p> <ul> <li>Added #177: initial support for object detection model benchmarking with <code>sv.ConfusionMatrix</code>.</li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n&gt;&gt;&gt; from ultralytics import YOLO\n\n&gt;&gt;&gt; dataset = sv.DetectionDataset.from_yolo(...)\n\n&gt;&gt;&gt; model = YOLO(...)\n&gt;&gt;&gt; def callback(image: np.ndarray) -&gt; sv.Detections:\n...     result = model(image)[0]\n...     return sv.Detections.from_yolov8(result)\n\n&gt;&gt;&gt; confusion_matrix = sv.ConfusionMatrix.benchmark(\n...     dataset = dataset,\n...     callback = callback\n... )\n\n&gt;&gt;&gt; confusion_matrix.matrix\narray([\n    [0., 0., 0., 0.],\n    [0., 1., 0., 1.],\n    [0., 1., 1., 0.],\n    [1., 1., 0., 0.]\n])\n</code></pre> <ul> <li> <p>Added #173: <code>Detections.from_mmdetection</code> to enable seamless integration with MMDetection framework.</p> </li> <li> <p>Added #130: ability to install package in <code>headless</code> or <code>desktop</code> mode.</p> </li> <li> <p>Changed #180: packing method from <code>setup.py</code> to <code>pyproject.toml</code>.</p> </li> <li> <p>Fixed #188: <code>sv.DetectionDataset.from_cooc</code> can't be loaded when there are images without annotations.</p> </li> <li> <p>Fixed #226: <code>sv.DetectionDataset.from_yolo</code> can't load background instances.</p> </li> </ul>"},{"location":"changelog/#0111-june-29-2023","title":"0.11.1 June 29, 2023","text":"<ul> <li>Fix #165: <code>as_folder_structure</code> fails to save <code>sv.ClassificationDataset</code> when it is result of inference.</li> </ul>"},{"location":"changelog/#0110-june-28-2023","title":"0.11.0 June 28, 2023","text":"<ul> <li>Added #150: ability to load and save <code>sv.DetectionDataset</code> in COCO format using <code>as_coco</code> and <code>from_coco</code> methods.</li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; ds = sv.DetectionDataset.from_coco(\n...     images_directory_path='...',\n...     annotations_path='...'\n... )\n\n&gt;&gt;&gt; ds.as_coco(\n...     images_directory_path='...',\n...     annotations_path='...'\n... )\n</code></pre> <ul> <li>Added #158: ability to merge multiple <code>sv.DetectionDataset</code> together using <code>merge</code> method.</li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; ds_1 = sv.DetectionDataset(...)\n&gt;&gt;&gt; len(ds_1)\n100\n&gt;&gt;&gt; ds_1.classes\n['dog', 'person']\n\n&gt;&gt;&gt; ds_2 = sv.DetectionDataset(...)\n&gt;&gt;&gt; len(ds_2)\n200\n&gt;&gt;&gt; ds_2.classes\n['cat']\n\n&gt;&gt;&gt; ds_merged = sv.DetectionDataset.merge([ds_1, ds_2])\n&gt;&gt;&gt; len(ds_merged)\n300\n&gt;&gt;&gt; ds_merged.classes\n['cat', 'dog', 'person']\n</code></pre> <ul> <li> <p>Added #162: additional <code>start</code> and <code>end</code> arguments to <code>sv.get_video_frames_generator</code> allowing to generate frames only for a selected part of the video.</p> </li> <li> <p>Fix #157: incorrect loading of YOLO dataset class names from <code>data.yaml</code>.</p> </li> </ul>"},{"location":"changelog/#0100-june-14-2023","title":"0.10.0 June 14, 2023","text":"<ul> <li>Added #125: ability to load and save <code>sv.ClassificationDataset</code> in a folder structure format.</li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; cs = sv.ClassificationDataset.from_folder_structure(\n...     root_directory_path='...'\n... )\n\n&gt;&gt;&gt; cs.as_folder_structure(\n...     root_directory_path='...'\n... )\n</code></pre> <ul> <li> <p>Added #125: support for <code>sv.ClassificationDataset.split</code> allowing to divide <code>sv.ClassificationDataset</code> into two parts.</p> </li> <li> <p>Added #110: ability to extract masks from Roboflow API results using <code>sv.Detections.from_roboflow</code>.</p> </li> <li> <p>Added commit hash: Supervision Quickstart notebook where you can learn more about Detection, Dataset and Video APIs.</p> </li> <li> <p>Changed #135: <code>sv.get_video_frames_generator</code> documentation to better describe actual behavior.</p> </li> </ul>"},{"location":"changelog/#090-june-7-2023","title":"0.9.0 June 7, 2023","text":"<ul> <li>Added #118: ability to select <code>sv.Detections</code> by index, list of indexes or slice. Here is an example illustrating the new selection methods.</li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; detections = sv.Detections(...)\n&gt;&gt;&gt; len(detections[0])\n1\n&gt;&gt;&gt; len(detections[[0, 1]])\n2\n&gt;&gt;&gt; len(detections[0:2])\n2\n</code></pre> <ul> <li> <p>Added #101: ability to extract masks from YOLOv8 result using <code>sv.Detections.from_yolov8</code>. Here is an example illustrating how to extract boolean masks from the result of the YOLOv8 model inference.</p> </li> <li> <p>Added #122: ability to crop image using <code>sv.crop</code>. Here is an example showing how to get a separate crop for each detection in <code>sv.Detections</code>.</p> </li> <li> <p>Added #120: ability to conveniently save multiple images into directory using <code>sv.ImageSink</code>. Here is an example showing how to save every tenth video frame as a separate image.</p> </li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; with sv.ImageSink(target_dir_path='target/directory/path') as sink:\n...     for image in sv.get_video_frames_generator(source_path='source_video.mp4', stride=10):\n...         sink.save_image(image=image)\n</code></pre> <ul> <li>Fixed #106: inconvenient handling of <code>sv.PolygonZone</code> coordinates. Now <code>sv.PolygonZone</code> accepts coordinates in the form of <code>[[x1, y1], [x2, y2], ...]</code> that can be both integers and floats.</li> </ul>"},{"location":"changelog/#080-may-17-2023","title":"0.8.0 May 17, 2023","text":"<ul> <li>Added #100: support for dataset inheritance. The current <code>Dataset</code> got renamed to <code>DetectionDataset</code>. Now <code>DetectionDataset</code> inherits from <code>BaseDataset</code>. This change was made to enforce the future consistency of APIs of different types of computer vision datasets.</li> <li>Added #100: ability to save datasets in YOLO format using <code>DetectionDataset.as_yolo</code>.</li> </ul> <pre><code>&gt;&gt;&gt; import roboflow\n&gt;&gt;&gt; from roboflow import Roboflow\n&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; roboflow.login()\n\n&gt;&gt;&gt; rf = Roboflow()\n\n&gt;&gt;&gt; project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n&gt;&gt;&gt; dataset = project.version(PROJECT_VERSION).download(\"yolov5\")\n\n&gt;&gt;&gt; ds = sv.DetectionDataset.from_yolo(\n...     images_directory_path=f\"{dataset.location}/train/images\",\n...     annotations_directory_path=f\"{dataset.location}/train/labels\",\n...     data_yaml_path=f\"{dataset.location}/data.yaml\"\n... )\n\n&gt;&gt;&gt; ds.classes\n['dog', 'person']\n</code></pre> <ul> <li>Added #102: support for <code>DetectionDataset.split</code> allowing to divide <code>DetectionDataset</code> into two parts.</li> </ul> <pre><code>&gt;&gt;&gt; import supervision as sv\n\n&gt;&gt;&gt; ds = sv.DetectionDataset(...)\n&gt;&gt;&gt; train_ds, test_ds = ds.split(split_ratio=0.7, random_state=42, shuffle=True)\n\n&gt;&gt;&gt; len(train_ds), len(test_ds)\n(700, 300)\n</code></pre> <ul> <li>Changed #100: default value of <code>approximation_percentage</code> parameter from <code>0.75</code> to <code>0.0</code> in <code>DetectionDataset.as_yolo</code> and <code>DetectionDataset.as_pascal_voc</code>.</li> </ul>"},{"location":"changelog/#070-may-11-2023","title":"0.7.0 May 11, 2023","text":"<ul> <li>Added #91: <code>Detections.from_yolo_nas</code> to enable seamless integration with YOLO-NAS model.</li> <li>Added #86: ability to load datasets in YOLO format using <code>Dataset.from_yolo</code>.</li> <li>Added #84: <code>Detections.merge</code> to merge multiple <code>Detections</code> objects together.</li> <li>Fixed #81: <code>LineZoneAnnotator.annotate</code> does not return annotated frame.</li> <li>Changed #44: <code>LineZoneAnnotator.annotate</code> to allow for custom text for the in and out tags.</li> </ul>"},{"location":"changelog/#060-april-19-2023","title":"0.6.0 April 19, 2023","text":"<ul> <li>Added #71: initial <code>Dataset</code> support and ability to save <code>Detections</code> in Pascal VOC XML format.</li> <li>Added #71: new <code>mask_to_polygons</code>, <code>filter_polygons_by_area</code>, <code>polygon_to_xyxy</code> and <code>approximate_polygon</code> utilities.</li> <li>Added #72: ability to load Pascal VOC XML object detections dataset as <code>Dataset</code>.</li> <li>Changed #70: order of <code>Detections</code> attributes to make it consistent with order of objects in <code>__iter__</code> tuple.</li> <li>Changed #71: <code>generate_2d_mask</code> to <code>polygon_to_mask</code>.</li> </ul>"},{"location":"changelog/#052-april-13-2023","title":"0.5.2 April 13, 2023","text":"<ul> <li>Fixed #63: <code>LineZone.trigger</code> function expects 4 values instead of 5.</li> </ul>"},{"location":"changelog/#051-april-12-2023","title":"0.5.1 April 12, 2023","text":"<ul> <li>Fixed <code>Detections.__getitem__</code> method did not return mask for selected item.</li> <li>Fixed <code>Detections.area</code> crashed for mask detections.</li> </ul>"},{"location":"changelog/#050-april-10-2023","title":"0.5.0 April 10, 2023","text":"<ul> <li>Added #58: <code>Detections.mask</code> to enable segmentation support.</li> <li>Added #58: <code>MaskAnnotator</code> to allow easy <code>Detections.mask</code> annotation.</li> <li>Added #58: <code>Detections.from_sam</code> to enable native Segment Anything Model (SAM) support.</li> <li>Changed #58: <code>Detections.area</code> behaviour to work not only with boxes but also with masks.</li> </ul>"},{"location":"changelog/#040-april-5-2023","title":"0.4.0 April 5, 2023","text":"<ul> <li>Added #46: <code>Detections.empty</code> to allow easy creation of empty <code>Detections</code> objects.</li> <li>Added #56: <code>Detections.from_roboflow</code> to allow easy creation of <code>Detections</code> objects from Roboflow API inference results.</li> <li>Added #56: <code>plot_images_grid</code> to allow easy plotting of multiple images on single plot.</li> <li>Added #56: initial support for Pascal VOC XML format with <code>detections_to_voc_xml</code> method.</li> <li>Changed #56: <code>show_frame_in_notebook</code> refactored and renamed to <code>plot_image</code>.</li> </ul>"},{"location":"changelog/#032-march-23-2023","title":"0.3.2 March 23, 2023","text":"<ul> <li>Changed #50: Allow <code>Detections.class_id</code> to be <code>None</code>.</li> </ul>"},{"location":"changelog/#031-march-6-2023","title":"0.3.1 March 6, 2023","text":"<ul> <li>Fixed #41: <code>PolygonZone</code> throws an exception when the object touches the bottom edge of the image.</li> <li>Fixed #42: <code>Detections.wth_nms</code> method throws an exception when <code>Detections</code> is empty.</li> <li>Changed #36: <code>Detections.wth_nms</code> support class agnostic and non-class agnostic case.</li> </ul>"},{"location":"changelog/#030-march-6-2023","title":"0.3.0 March 6, 2023","text":"<ul> <li>Changed: Allow <code>Detections.confidence</code> to be <code>None</code>.</li> <li>Added: <code>Detections.from_transformers</code> and <code>Detections.from_detectron2</code> to enable seamless integration with Transformers and Detectron2 models.</li> <li>Added: <code>Detections.area</code> to dynamically calculate bounding box area.</li> <li>Added: <code>Detections.wth_nms</code> to filter out double detections with NMS. Initial - only class agnostic - implementation.</li> </ul>"},{"location":"changelog/#020-february-2-2023","title":"0.2.0 February 2, 2023","text":"<ul> <li>Added: Advanced <code>Detections</code> filtering with pandas-like API.</li> <li>Added: <code>Detections.from_yolov5</code> and <code>Detections.from_yolov8</code> to enable seamless integration with YOLOv5 and YOLOv8 models.</li> </ul>"},{"location":"changelog/#010-january-19-2023","title":"0.1.0 January 19, 2023","text":"<p>Say hello to Supervision \ud83d\udc4b</p>"},{"location":"code_of_conduct/","title":"Code of conduct","text":"<pre><code># Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, caste, color, religion, or sexual\nidentity and orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n* Demonstrating empathy and kindness toward other people\n* Being respectful of differing opinions, viewpoints, and experiences\n* Giving and gracefully accepting constructive feedback\n* Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n* Focusing on what is best not just for us as individuals, but for the overall\n  community\n\nExamples of unacceptable behavior include:\n\n* The use of sexualized language or imagery, and sexual attention or advances of\n  any kind\n* Trolling, insulting or derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or email address,\n  without their explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\ncommunity-reports@roboflow.com.\n\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series of\nactions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or permanent\nban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior, harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within the\ncommunity.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.1, available at\n[https://www.contributor-covenant.org/version/2/1/code_of_conduct.html][v2.1].\n\nCommunity Impact Guidelines were inspired by\n[Mozilla's code of conduct enforcement ladder][Mozilla CoC].\n\nFor answers to common questions about this code of conduct, see the FAQ at\n[https://www.contributor-covenant.org/faq][FAQ]. Translations are available at\n[https://www.contributor-covenant.org/translations][translations].\n\n[homepage]: https://www.contributor-covenant.org\n[v2.1]: https://www.contributor-covenant.org/version/2/1/code_of_conduct.html\n[Mozilla CoC]: https://github.com/mozilla/diversity\n[FAQ]: https://www.contributor-covenant.org/faq\n[translations]: https://www.contributor-covenant.org/translations\n</code></pre>"},{"location":"contributing/","title":"Contributing to Supervision \ud83d\udee0\ufe0f","text":"<p>Thank you for your interest in contributing to Supervision!</p> <p>We are actively improving this library to reduce the amount of work you need to do to solve common computer vision problems.</p>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please read and adhere to our Code of Conduct. This document outlines the expected behavior for all participants in our project.</p>"},{"location":"contributing/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Contribution Guidelines</li> <li>Contributing Features</li> <li>How to Contribute Changes</li> <li>Installation for Contributors</li> <li>Code Style and Quality</li> <li>Pre-commit tool</li> <li>Docstrings</li> <li>Type checking</li> <li>Documentation</li> <li>Cookbooks</li> <li>Tests</li> <li>License</li> </ul>"},{"location":"contributing/#contribution-guidelines","title":"Contribution Guidelines","text":"<p>We welcome contributions to:</p> <ol> <li>Add a new feature to the library (guidance below).</li> <li>Improve our documentation and add examples to make it clear how to leverage the supervision library.</li> <li>Report bugs and issues in the project.</li> <li>Submit a request for a new feature.</li> <li>Improve our test coverage.</li> </ol>"},{"location":"contributing/#contributing-features","title":"Contributing Features \u2728","text":"<p>Supervision is designed to provide generic utilities to solve problems. Thus, we focus on contributions that can have an impact on a wide range of projects.</p> <p>For example, counting objects that cross a line anywhere on an image is a common problem in computer vision, but counting objects that cross a line 75% of the way through is less useful.</p> <p>Before you contribute a new feature, consider submitting an Issue to discuss the feature so the community can weigh in and assist.</p>"},{"location":"contributing/#how-to-contribute-changes","title":"How to Contribute Changes","text":"<p>First, fork this repository to your own GitHub account. Click \"fork\" in the top corner of the <code>supervision</code> repository to get started:</p> <p></p> <p></p> <p>Then, run <code>git clone</code> to download the project code to your computer.</p> <p>You should also set up <code>roboflow/supervision</code> as an \"upstream\" remote (that is, tell git that the reference Supervision repository was the source of your fork of it):</p> <pre><code>git remote add upstream https://github.com/roboflow/supervision.git\ngit fetch upstream\n</code></pre> <p>Move to a new branch using the <code>git checkout</code> command:</p> <pre><code>git checkout -b &lt;scope&gt;/&lt;your_branch_name&gt; upstream/develop\n</code></pre> <p>The name you choose for your branch should describe the change you want to make and start with an appropriate prefix:</p> <ul> <li><code>feat/</code>: for new features (e.g., <code>feat/line-counter</code>)</li> <li><code>fix/</code>: for bug fixes (e.g., <code>fix/memory-leak</code>)</li> <li><code>docs/</code>: for documentation changes (e.g., <code>docs/update-readme</code>)</li> <li><code>chore/</code>: for routine tasks, maintenance, or tooling changes (e.g., <code>chore/update-dependencies</code>)</li> <li><code>test/</code>: for adding or modifying tests (e.g., <code>test/add-unit-tests</code>)</li> <li><code>refactor/</code>: for code refactoring (e.g., <code>refactor/simplify-algorithm</code>)</li> </ul> <p>Make any changes you want to the project code, then run the following commands to commit your changes:</p> <pre><code>git add -A\ngit commit -m \"feat: add line counter functionality\"\ngit push -u origin &lt;your_branch_name&gt;\n</code></pre> <p>Use conventional commit messages to clearly describe your changes. The format is:</p> <p>[optional scope]:  <p>Common types include: - feat: A new feature - fix: A bug fix - docs: Documentation only changes - style: Changes that do not affect the meaning of the code (white-space, formatting, etc) - refactor: A code change that neither fixes a bug nor adds a feature - perf: A code change that improves performance - test: Adding missing tests or correcting existing tests - chore: Changes to the build process or auxiliary tools and libraries</p> <p>Then, go back to your fork of the <code>supervision</code> repository, click \"Pull Requests\", and click \"New Pull Request\".</p> <p></p> <p>Make sure the <code>base</code> branch is <code>develop</code> before submitting your PR.</p> <p>On the next page, review your changes then click \"Create pull request\":</p> <p></p> <p>Next, write a description for your pull request, and click \"Create pull request\" again to submit it for review:</p> <p></p> <p>When creating new functions, please ensure you have the following:</p> <ol> <li>Docstrings for the function and all parameters.</li> <li>Unit tests for the function.</li> <li>Examples in the documentation for the function.</li> <li>Created an entry in our docs to autogenerate the documentation for the function.</li> <li>Please share a Google Colab with minimal code to test new feature or reproduce PR whenever it is possible. Please ensure that Google Colab can be accessed without any issue.</li> </ol> <p>When you submit your Pull Request, you will be asked to sign a Contributor License Agreement (CLA) by the <code>cla-assistant</code> GitHub bot. We can only respond to PRs from contributors who have signed the project CLA.</p> <p>All pull requests will be reviewed by the maintainers of the project. We will provide feedback and ask for changes if necessary.</p> <p>PRs must pass all tests and linting requirements before they can be merged.</p>"},{"location":"contributing/#installation-for-contributors","title":"Installation for Contributors","text":"<p>Before starting your work on the project, set up your development environment:</p> <ol> <li> <p>Clone your fork of the project:    <pre><code>git clone https://github.com/YOUR_USERNAME/supervision.git\ncd supervision\n</code></pre>    Replace <code>YOUR_USERNAME</code> with your GitHub username.</p> </li> <li> <p>Create and activate a virtual environment:    <pre><code>python3 -m venv .venv\nsource .venv/bin/activate\n</code></pre></p> </li> <li> <p>Install Poetry:</p> </li> </ol> <p>Using pip:    <pre><code>pip install -U pip setuptools\npip install poetry\n</code></pre></p> <p>Or using pipx (recommended for global installation):    <pre><code>pipx install poetry\n</code></pre></p> <ol> <li> <p>Install project dependencies:    <pre><code>poetry install\n</code></pre></p> </li> <li> <p>Run pytest to verify the setup:    <pre><code>poetry run pytest\n</code></pre></p> </li> </ol>"},{"location":"contributing/#code-style-and-quality","title":"\ud83c\udfa8 Code Style and Quality","text":""},{"location":"contributing/#pre-commit-tool","title":"Pre-commit tool","text":"<p>This project uses the pre-commit tool to maintain code quality and consistency. Before submitting a pull request or making any commits, it is important to run the pre-commit tool to ensure that your changes meet the project's guidelines.</p> <p>Furthermore, we have integrated a pre-commit GitHub Action into our workflow. This means that with every pull request opened, the pre-commit checks will be automatically enforced, streamlining the code review process and ensuring that all contributions adhere to our quality standards.</p> <p>To run the pre-commit tool, follow these steps:</p> <ol> <li> <p>Install pre-commit by running the following command: <code>poetry install</code>. It will not only install pre-commit but also install all the deps and dev-deps of project</p> </li> <li> <p>Once pre-commit is installed, navigate to the project's root directory.</p> </li> <li> <p>Run the command <code>pre-commit run --all-files</code>. This will execute the pre-commit hooks configured for this project against the modified files. If any issues are found, the pre-commit tool will provide feedback on how to resolve them. Make the necessary changes and re-run the pre-commit command until all issues are resolved.</p> </li> <li> <p>You can also install pre-commit as a git hook by executing <code>pre-commit install</code>. Every time you do a <code>git commit</code> pre-commit run automatically for you.</p> </li> </ol>"},{"location":"contributing/#docstrings","title":"Docstrings","text":"<p>All new functions and classes in <code>supervision</code> should include docstrings. This is a prerequisite for any new functions and classes to be added to the library.</p> <p><code>supervision</code> adheres to the Google Python docstring style. Please refer to the style guide while writing docstrings for your contribution.</p>"},{"location":"contributing/#type-checking","title":"Type checking","text":"<p>So far, there is no type checking with mypy. See issue.</p>"},{"location":"contributing/#documentation","title":"\ud83d\udcdd Documentation","text":"<p>The <code>supervision</code> documentation is stored in a folder called <code>docs</code>. The project documentation is built using <code>mkdocs</code>.</p> <p>To run the documentation, install the project requirements with <code>poetry install --with dev</code>. Then, run <code>mkdocs serve</code> to start the documentation server.</p> <p>You can learn more about mkdocs on the mkdocs website.</p>"},{"location":"contributing/#cookbooks","title":"\ud83e\uddd1\u200d\ud83c\udf73 Cookbooks","text":"<p>We are always looking for new examples and cookbooks to add to the <code>supervision</code> documentation. If you have a use case that you think would be helpful to others, please submit a PR with your example. Here are some guidelines for submitting a new example:</p> <ul> <li>Create a new notebook in the <code>docs/notebooks</code> folder.</li> <li>Add a link to the new notebook in <code>docs/theme/cookbooks.html</code>. Make sure to add the path to the new notebook, as well as a title, labels, author and supervision version.</li> <li>Use the Count Objects Crossing the Line example as a template for your new example.</li> <li>Freeze the version of <code>supervision</code> you are using.</li> <li>Place an appropriate Open in Colab button at the top of the notebook. You can find an example of such a button in the aforementioned <code>Count Objects Crossing the Line</code> cookbook.</li> <li>Notebook should be self-contained. If you rely on external data ( videos, images, etc.) or libraries, include download and installation commands in the notebook.</li> <li>Annotate the code with appropriate comments, including links to the documentation describing each of the tools you have used.</li> </ul>"},{"location":"contributing/#tests","title":"\ud83e\uddea Tests","text":"<p><code>pytests</code> is used to run our tests.</p>"},{"location":"contributing/#license","title":"\ud83d\udcc4 License","text":"<p>By contributing, you agree that your contributions will be licensed under an MIT license.</p>"},{"location":"deprecated/","title":"Deprecated","text":"<p>These features are phased out due to better alternatives or potential issues in future versions. Deprecated functionalities are supported for five subsequent releases, providing time for users to transition to updated methods.</p> <ul> <li>The <code>track_buffer</code>, <code>track_thresh</code>, and <code>match_thresh</code> parameters in <code>ByterTrack</code> are deprecated and will be removed in <code>supervision-0.23.0</code>. Use <code>lost_track_buffer,</code> <code>track_activation_threshold</code>, and <code>minimum_matching_threshold</code> instead.</li> <li>The <code>triggering_position</code> parameter in <code>sv.PolygonZone</code> will be removed in <code>supervision-0.23.0</code>. Use <code>triggering_anchors</code> instead.</li> <li>The <code>frame_resolution_wh</code> parameter in <code>sv.PolygonZone</code> will be removed in <code>supervision-0.24.0</code>.</li> <li>Constructing <code>DetectionDataset</code> and <code>ClassificationDataset</code> with parameter <code>images</code> as <code>Dict[str, np.ndarray]</code> will be removed in <code>supervision-0.26.0</code>. Please pass a list of paths <code>List[str]</code> instead.</li> <li>The <code>DetectionDataset.images</code> property will be removed in <code>supervision-0.26.0</code>. Please loop over images with <code>for path, image, annotation in dataset:</code>, as that does not require loading all images into memory.</li> <li><code>BoundingBoxAnnotator</code> has been renamed to <code>BoxAnnotator</code> after the old implementation of <code>BoxAnnotator</code> has been removed. <code>BoundingBoxAnnotator</code> will be removed in <code>supervision-0.26.0</code>.</li> </ul>"},{"location":"deprecated/#removed","title":"Removed","text":"<ul> <li><code>Detections.from_froboflow</code> is removed as of <code>supervision-0.22.0</code>. Use <code>Detections.from_inference</code> instead.</li> <li>The method <code>Color.white()</code> was removed as of <code>supervision-0.22.0</code>. Use the constant <code>Color.WHITE</code> instead.</li> <li>The method <code>Color.black()</code> was removed as of <code>supervision-0.22.0</code>. Use the constant <code>Color.BLACK</code> instead.</li> <li>The method <code>Color.red()</code> was removed as of <code>supervision-0.22.0</code>. Use the constant <code>Color.RED</code> instead.</li> <li>The method <code>Color.green()</code> was removed as of <code>supervision-0.22.0</code>. Use the constant <code>Color.GREEN</code> instead.</li> <li>The method <code>Color.blue()</code> was removed as of <code>supervision-0.22.0</code>. Use the constant <code>Color.BLUE</code> instead.</li> <li>The method <code>ColorPalette.default()</code> was removed as of <code>supervision-0.22.0</code>. Use the constant <code>ColorPalette.DEFAULT</code> instead.</li> <li><code>BoxAnnotator</code> was removed as of <code>supervision-0.22.0</code>, however <code>BoundingBoxAnnotator</code> was immediately renamed to <code>BoxAnnotator</code>. Use <code>BoxAnnotator</code> and <code>LabelAnnotator</code> instead of the old <code>BoxAnnotator</code>.</li> <li>The method <code>FPSMonitor.__call__</code> was removed as of <code>supervision-0.22.0</code>. Use the attribute <code>FPSMonitor.fps</code> instead.</li> </ul>"},{"location":"license/","title":"License","text":"<pre><code>MIT License\n\nCopyright (c) 2022 Roboflow\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre>"},{"location":"trackers/","title":"ByteTrack","text":"<p>Initialize the ByteTrack object.</p> <p>Parameters:</p> Name Type Description Default <code>track_activation_threshold</code> <code>float</code> <p>Detection confidence threshold for track activation. Increasing track_activation_threshold improves accuracy and stability but might miss true detections. Decreasing it increases completeness but risks introducing noise and instability.</p> <code>0.25</code> <code>lost_track_buffer</code> <code>int</code> <p>Number of frames to buffer when a track is lost. Increasing lost_track_buffer enhances occlusion handling, significantly reducing the likelihood of track fragmentation or disappearance caused by brief detection gaps.</p> <code>30</code> <code>minimum_matching_threshold</code> <code>float</code> <p>Threshold for matching tracks with detections. Increasing minimum_matching_threshold improves accuracy but risks fragmentation. Decreasing it improves completeness but risks false positives and drift.</p> <code>0.8</code> <code>frame_rate</code> <code>int</code> <p>The frame rate of the video.</p> <code>30</code> <code>minimum_consecutive_frames</code> <code>int</code> <p>Number of consecutive frames that an object must be tracked before it is considered a 'valid' track. Increasing minimum_consecutive_frames prevents the creation of accidental tracks from false detection or double detection, but risks missing shorter tracks.</p> <code>1</code> Source code in <code>supervision/tracker/byte_tracker/core.py</code> <pre><code>class ByteTrack:\n    \"\"\"\n    Initialize the ByteTrack object.\n\n    &lt;video controls&gt;\n        &lt;source src=\"https://media.roboflow.com/supervision/video-examples/how-to/track-objects/annotate-video-with-traces.mp4\" type=\"video/mp4\"&gt;\n    &lt;/video&gt;\n\n    Parameters:\n        track_activation_threshold (float, optional): Detection confidence threshold\n            for track activation. Increasing track_activation_threshold improves accuracy\n            and stability but might miss true detections. Decreasing it increases\n            completeness but risks introducing noise and instability.\n        lost_track_buffer (int, optional): Number of frames to buffer when a track is lost.\n            Increasing lost_track_buffer enhances occlusion handling, significantly\n            reducing the likelihood of track fragmentation or disappearance caused\n            by brief detection gaps.\n        minimum_matching_threshold (float, optional): Threshold for matching tracks with detections.\n            Increasing minimum_matching_threshold improves accuracy but risks fragmentation.\n            Decreasing it improves completeness but risks false positives and drift.\n        frame_rate (int, optional): The frame rate of the video.\n        minimum_consecutive_frames (int, optional): Number of consecutive frames that an object must\n            be tracked before it is considered a 'valid' track.\n            Increasing minimum_consecutive_frames prevents the creation of accidental tracks from\n            false detection or double detection, but risks missing shorter tracks.\n    \"\"\"  # noqa: E501 // docs\n\n    @deprecated_parameter(\n        old_parameter=\"track_buffer\",\n        new_parameter=\"lost_track_buffer\",\n        map_function=lambda x: x,\n        warning_message=\"`{old_parameter}` in `{function_name}` is deprecated and will \"\n        \"be remove in `supervision-0.23.0`. Use '{new_parameter}' \"\n        \"instead.\",\n    )\n    @deprecated_parameter(\n        old_parameter=\"track_thresh\",\n        new_parameter=\"track_activation_threshold\",\n        map_function=lambda x: x,\n        warning_message=\"`{old_parameter}` in `{function_name}` is deprecated and will \"\n        \"be remove in `supervision-0.23.0`. Use '{new_parameter}' \"\n        \"instead.\",\n    )\n    @deprecated_parameter(\n        old_parameter=\"match_thresh\",\n        new_parameter=\"minimum_matching_threshold\",\n        map_function=lambda x: x,\n        warning_message=\"`{old_parameter}` in `{function_name}` is deprecated and will \"\n        \"be remove in `supervision-0.23.0`. Use '{new_parameter}' \"\n        \"instead.\",\n    )\n    def __init__(\n        self,\n        track_activation_threshold: float = 0.25,\n        lost_track_buffer: int = 30,\n        minimum_matching_threshold: float = 0.8,\n        frame_rate: int = 30,\n        minimum_consecutive_frames: int = 1,\n    ):\n        self.track_activation_threshold = track_activation_threshold\n        self.minimum_matching_threshold = minimum_matching_threshold\n\n        self.frame_id = 0\n        self.det_thresh = self.track_activation_threshold + 0.1\n        self.max_time_lost = int(frame_rate / 30.0 * lost_track_buffer)\n        self.minimum_consecutive_frames = minimum_consecutive_frames\n        self.kalman_filter = KalmanFilter()\n\n        self.tracked_tracks: List[STrack] = []\n        self.lost_tracks: List[STrack] = []\n        self.removed_tracks: List[STrack] = []\n\n    def update_with_detections(self, detections: Detections) -&gt; Detections:\n        \"\"\"\n        Updates the tracker with the provided detections and returns the updated\n        detection results.\n\n        Args:\n            detections (Detections): The detections to pass through the tracker.\n\n        Example:\n            ```python\n            import supervision as sv\n            from ultralytics import YOLO\n\n            model = YOLO(&lt;MODEL_PATH&gt;)\n            tracker = sv.ByteTrack()\n\n            box_annotator = sv.BoxAnnotator()\n            label_annotator = sv.LabelAnnotator()\n\n            def callback(frame: np.ndarray, index: int) -&gt; np.ndarray:\n                results = model(frame)[0]\n                detections = sv.Detections.from_ultralytics(results)\n                detections = tracker.update_with_detections(detections)\n\n                labels = [f\"#{tracker_id}\" for tracker_id in detections.tracker_id]\n\n                annotated_frame = box_annotator.annotate(\n                    scene=frame.copy(), detections=detections)\n                annotated_frame = label_annotator.annotate(\n                    scene=annotated_frame, detections=detections, labels=labels)\n                return annotated_frame\n\n            sv.process_video(\n                source_path=&lt;SOURCE_VIDEO_PATH&gt;,\n                target_path=&lt;TARGET_VIDEO_PATH&gt;,\n                callback=callback\n            )\n            ```\n        \"\"\"\n\n        tensors = detections2boxes(detections=detections)\n        tracks = self.update_with_tensors(tensors=tensors)\n\n        if len(tracks) &gt; 0:\n            detection_bounding_boxes = np.asarray([det[:4] for det in tensors])\n            track_bounding_boxes = np.asarray([track.tlbr for track in tracks])\n\n            ious = box_iou_batch(detection_bounding_boxes, track_bounding_boxes)\n\n            iou_costs = 1 - ious\n\n            matches, _, _ = matching.linear_assignment(iou_costs, 0.5)\n            detections.tracker_id = np.full(len(detections), -1, dtype=int)\n            for i_detection, i_track in matches:\n                detections.tracker_id[i_detection] = int(\n                    tracks[i_track].external_track_id\n                )\n\n            return detections[detections.tracker_id != -1]\n\n        else:\n            detections = Detections.empty()\n            detections.tracker_id = np.array([], dtype=int)\n\n            return detections\n\n    def reset(self):\n        \"\"\"\n        Resets the internal state of the ByteTrack tracker.\n\n        This method clears the tracking data, including tracked, lost,\n        and removed tracks, as well as resetting the frame counter. It's\n        particularly useful when processing multiple videos sequentially,\n        ensuring the tracker starts with a clean state for each new video.\n        \"\"\"\n        self.frame_id = 0\n        self.tracked_tracks: List[STrack] = []\n        self.lost_tracks: List[STrack] = []\n        self.removed_tracks: List[STrack] = []\n        BaseTrack.reset_counter()\n        STrack.reset_external_counter()\n\n    def update_with_tensors(self, tensors: np.ndarray) -&gt; List[STrack]:\n        \"\"\"\n        Updates the tracker with the provided tensors and returns the updated tracks.\n\n        Parameters:\n            tensors: The new tensors to update with.\n\n        Returns:\n            List[STrack]: Updated tracks.\n        \"\"\"\n        self.frame_id += 1\n        activated_starcks = []\n        refind_stracks = []\n        lost_stracks = []\n        removed_stracks = []\n\n        class_ids = tensors[:, 5]\n        scores = tensors[:, 4]\n        bboxes = tensors[:, :4]\n\n        remain_inds = scores &gt; self.track_activation_threshold\n        inds_low = scores &gt; 0.1\n        inds_high = scores &lt; self.track_activation_threshold\n\n        inds_second = np.logical_and(inds_low, inds_high)\n        dets_second = bboxes[inds_second]\n        dets = bboxes[remain_inds]\n        scores_keep = scores[remain_inds]\n        scores_second = scores[inds_second]\n\n        class_ids_keep = class_ids[remain_inds]\n        class_ids_second = class_ids[inds_second]\n\n        if len(dets) &gt; 0:\n            \"\"\"Detections\"\"\"\n            detections = [\n                STrack(STrack.tlbr_to_tlwh(tlbr), s, c, self.minimum_consecutive_frames)\n                for (tlbr, s, c) in zip(dets, scores_keep, class_ids_keep)\n            ]\n        else:\n            detections = []\n\n        \"\"\" Add newly detected tracklets to tracked_stracks\"\"\"\n        unconfirmed = []\n        tracked_stracks = []  # type: list[STrack]\n\n        for track in self.tracked_tracks:\n            if not track.is_activated:\n                unconfirmed.append(track)\n            else:\n                tracked_stracks.append(track)\n\n        \"\"\" Step 2: First association, with high score detection boxes\"\"\"\n        strack_pool = joint_tracks(tracked_stracks, self.lost_tracks)\n        # Predict the current location with KF\n        STrack.multi_predict(strack_pool)\n        dists = matching.iou_distance(strack_pool, detections)\n\n        dists = matching.fuse_score(dists, detections)\n        matches, u_track, u_detection = matching.linear_assignment(\n            dists, thresh=self.minimum_matching_threshold\n        )\n\n        for itracked, idet in matches:\n            track = strack_pool[itracked]\n            det = detections[idet]\n            if track.state == TrackState.Tracked:\n                track.update(detections[idet], self.frame_id)\n                activated_starcks.append(track)\n            else:\n                track.re_activate(det, self.frame_id, new_id=False)\n                refind_stracks.append(track)\n\n        \"\"\" Step 3: Second association, with low score detection boxes\"\"\"\n        # association the untrack to the low score detections\n        if len(dets_second) &gt; 0:\n            \"\"\"Detections\"\"\"\n            detections_second = [\n                STrack(STrack.tlbr_to_tlwh(tlbr), s, c, self.minimum_consecutive_frames)\n                for (tlbr, s, c) in zip(dets_second, scores_second, class_ids_second)\n            ]\n        else:\n            detections_second = []\n        r_tracked_stracks = [\n            strack_pool[i]\n            for i in u_track\n            if strack_pool[i].state == TrackState.Tracked\n        ]\n        dists = matching.iou_distance(r_tracked_stracks, detections_second)\n        matches, u_track, u_detection_second = matching.linear_assignment(\n            dists, thresh=0.5\n        )\n        for itracked, idet in matches:\n            track = r_tracked_stracks[itracked]\n            det = detections_second[idet]\n            if track.state == TrackState.Tracked:\n                track.update(det, self.frame_id)\n                activated_starcks.append(track)\n            else:\n                track.re_activate(det, self.frame_id, new_id=False)\n                refind_stracks.append(track)\n\n        for it in u_track:\n            track = r_tracked_stracks[it]\n            if not track.state == TrackState.Lost:\n                track.mark_lost()\n                lost_stracks.append(track)\n\n        \"\"\"Deal with unconfirmed tracks, usually tracks with only one beginning frame\"\"\"\n        detections = [detections[i] for i in u_detection]\n        dists = matching.iou_distance(unconfirmed, detections)\n\n        dists = matching.fuse_score(dists, detections)\n        matches, u_unconfirmed, u_detection = matching.linear_assignment(\n            dists, thresh=0.7\n        )\n        for itracked, idet in matches:\n            unconfirmed[itracked].update(detections[idet], self.frame_id)\n            activated_starcks.append(unconfirmed[itracked])\n        for it in u_unconfirmed:\n            track = unconfirmed[it]\n            track.mark_removed()\n            removed_stracks.append(track)\n\n        \"\"\" Step 4: Init new stracks\"\"\"\n        for inew in u_detection:\n            track = detections[inew]\n            if track.score &lt; self.det_thresh:\n                continue\n            track.activate(self.kalman_filter, self.frame_id)\n            activated_starcks.append(track)\n        \"\"\" Step 5: Update state\"\"\"\n        for track in self.lost_tracks:\n            if self.frame_id - track.end_frame &gt; self.max_time_lost:\n                track.mark_removed()\n                removed_stracks.append(track)\n\n        self.tracked_tracks = [\n            t for t in self.tracked_tracks if t.state == TrackState.Tracked\n        ]\n        self.tracked_tracks = joint_tracks(self.tracked_tracks, activated_starcks)\n        self.tracked_tracks = joint_tracks(self.tracked_tracks, refind_stracks)\n        self.lost_tracks = sub_tracks(self.lost_tracks, self.tracked_tracks)\n        self.lost_tracks.extend(lost_stracks)\n        self.lost_tracks = sub_tracks(self.lost_tracks, self.removed_tracks)\n        self.removed_tracks = removed_stracks\n        self.tracked_tracks, self.lost_tracks = remove_duplicate_tracks(\n            self.tracked_tracks, self.lost_tracks\n        )\n        output_stracks = [track for track in self.tracked_tracks if track.is_activated]\n\n        return output_stracks\n</code></pre>"},{"location":"trackers/#supervision.tracker.byte_tracker.core.ByteTrack-functions","title":"Functions","text":""},{"location":"trackers/#supervision.tracker.byte_tracker.core.ByteTrack.reset","title":"<code>reset()</code>","text":"<p>Resets the internal state of the ByteTrack tracker.</p> <p>This method clears the tracking data, including tracked, lost, and removed tracks, as well as resetting the frame counter. It's particularly useful when processing multiple videos sequentially, ensuring the tracker starts with a clean state for each new video.</p> Source code in <code>supervision/tracker/byte_tracker/core.py</code> <pre><code>def reset(self):\n    \"\"\"\n    Resets the internal state of the ByteTrack tracker.\n\n    This method clears the tracking data, including tracked, lost,\n    and removed tracks, as well as resetting the frame counter. It's\n    particularly useful when processing multiple videos sequentially,\n    ensuring the tracker starts with a clean state for each new video.\n    \"\"\"\n    self.frame_id = 0\n    self.tracked_tracks: List[STrack] = []\n    self.lost_tracks: List[STrack] = []\n    self.removed_tracks: List[STrack] = []\n    BaseTrack.reset_counter()\n    STrack.reset_external_counter()\n</code></pre>"},{"location":"trackers/#supervision.tracker.byte_tracker.core.ByteTrack.update_with_detections","title":"<code>update_with_detections(detections)</code>","text":"<p>Updates the tracker with the provided detections and returns the updated detection results.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p>The detections to pass through the tracker.</p> required Example <pre><code>import supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(&lt;MODEL_PATH&gt;)\ntracker = sv.ByteTrack()\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\ndef callback(frame: np.ndarray, index: int) -&gt; np.ndarray:\n    results = model(frame)[0]\n    detections = sv.Detections.from_ultralytics(results)\n    detections = tracker.update_with_detections(detections)\n\n    labels = [f\"#{tracker_id}\" for tracker_id in detections.tracker_id]\n\n    annotated_frame = box_annotator.annotate(\n        scene=frame.copy(), detections=detections)\n    annotated_frame = label_annotator.annotate(\n        scene=annotated_frame, detections=detections, labels=labels)\n    return annotated_frame\n\nsv.process_video(\n    source_path=&lt;SOURCE_VIDEO_PATH&gt;,\n    target_path=&lt;TARGET_VIDEO_PATH&gt;,\n    callback=callback\n)\n</code></pre> Source code in <code>supervision/tracker/byte_tracker/core.py</code> <pre><code>def update_with_detections(self, detections: Detections) -&gt; Detections:\n    \"\"\"\n    Updates the tracker with the provided detections and returns the updated\n    detection results.\n\n    Args:\n        detections (Detections): The detections to pass through the tracker.\n\n    Example:\n        ```python\n        import supervision as sv\n        from ultralytics import YOLO\n\n        model = YOLO(&lt;MODEL_PATH&gt;)\n        tracker = sv.ByteTrack()\n\n        box_annotator = sv.BoxAnnotator()\n        label_annotator = sv.LabelAnnotator()\n\n        def callback(frame: np.ndarray, index: int) -&gt; np.ndarray:\n            results = model(frame)[0]\n            detections = sv.Detections.from_ultralytics(results)\n            detections = tracker.update_with_detections(detections)\n\n            labels = [f\"#{tracker_id}\" for tracker_id in detections.tracker_id]\n\n            annotated_frame = box_annotator.annotate(\n                scene=frame.copy(), detections=detections)\n            annotated_frame = label_annotator.annotate(\n                scene=annotated_frame, detections=detections, labels=labels)\n            return annotated_frame\n\n        sv.process_video(\n            source_path=&lt;SOURCE_VIDEO_PATH&gt;,\n            target_path=&lt;TARGET_VIDEO_PATH&gt;,\n            callback=callback\n        )\n        ```\n    \"\"\"\n\n    tensors = detections2boxes(detections=detections)\n    tracks = self.update_with_tensors(tensors=tensors)\n\n    if len(tracks) &gt; 0:\n        detection_bounding_boxes = np.asarray([det[:4] for det in tensors])\n        track_bounding_boxes = np.asarray([track.tlbr for track in tracks])\n\n        ious = box_iou_batch(detection_bounding_boxes, track_bounding_boxes)\n\n        iou_costs = 1 - ious\n\n        matches, _, _ = matching.linear_assignment(iou_costs, 0.5)\n        detections.tracker_id = np.full(len(detections), -1, dtype=int)\n        for i_detection, i_track in matches:\n            detections.tracker_id[i_detection] = int(\n                tracks[i_track].external_track_id\n            )\n\n        return detections[detections.tracker_id != -1]\n\n    else:\n        detections = Detections.empty()\n        detections.tracker_id = np.array([], dtype=int)\n\n        return detections\n</code></pre>"},{"location":"trackers/#supervision.tracker.byte_tracker.core.ByteTrack.update_with_tensors","title":"<code>update_with_tensors(tensors)</code>","text":"<p>Updates the tracker with the provided tensors and returns the updated tracks.</p> <p>Parameters:</p> Name Type Description Default <code>tensors</code> <code>ndarray</code> <p>The new tensors to update with.</p> required <p>Returns:</p> Type Description <code>List[STrack]</code> <p>List[STrack]: Updated tracks.</p> Source code in <code>supervision/tracker/byte_tracker/core.py</code> <pre><code>def update_with_tensors(self, tensors: np.ndarray) -&gt; List[STrack]:\n    \"\"\"\n    Updates the tracker with the provided tensors and returns the updated tracks.\n\n    Parameters:\n        tensors: The new tensors to update with.\n\n    Returns:\n        List[STrack]: Updated tracks.\n    \"\"\"\n    self.frame_id += 1\n    activated_starcks = []\n    refind_stracks = []\n    lost_stracks = []\n    removed_stracks = []\n\n    class_ids = tensors[:, 5]\n    scores = tensors[:, 4]\n    bboxes = tensors[:, :4]\n\n    remain_inds = scores &gt; self.track_activation_threshold\n    inds_low = scores &gt; 0.1\n    inds_high = scores &lt; self.track_activation_threshold\n\n    inds_second = np.logical_and(inds_low, inds_high)\n    dets_second = bboxes[inds_second]\n    dets = bboxes[remain_inds]\n    scores_keep = scores[remain_inds]\n    scores_second = scores[inds_second]\n\n    class_ids_keep = class_ids[remain_inds]\n    class_ids_second = class_ids[inds_second]\n\n    if len(dets) &gt; 0:\n        \"\"\"Detections\"\"\"\n        detections = [\n            STrack(STrack.tlbr_to_tlwh(tlbr), s, c, self.minimum_consecutive_frames)\n            for (tlbr, s, c) in zip(dets, scores_keep, class_ids_keep)\n        ]\n    else:\n        detections = []\n\n    \"\"\" Add newly detected tracklets to tracked_stracks\"\"\"\n    unconfirmed = []\n    tracked_stracks = []  # type: list[STrack]\n\n    for track in self.tracked_tracks:\n        if not track.is_activated:\n            unconfirmed.append(track)\n        else:\n            tracked_stracks.append(track)\n\n    \"\"\" Step 2: First association, with high score detection boxes\"\"\"\n    strack_pool = joint_tracks(tracked_stracks, self.lost_tracks)\n    # Predict the current location with KF\n    STrack.multi_predict(strack_pool)\n    dists = matching.iou_distance(strack_pool, detections)\n\n    dists = matching.fuse_score(dists, detections)\n    matches, u_track, u_detection = matching.linear_assignment(\n        dists, thresh=self.minimum_matching_threshold\n    )\n\n    for itracked, idet in matches:\n        track = strack_pool[itracked]\n        det = detections[idet]\n        if track.state == TrackState.Tracked:\n            track.update(detections[idet], self.frame_id)\n            activated_starcks.append(track)\n        else:\n            track.re_activate(det, self.frame_id, new_id=False)\n            refind_stracks.append(track)\n\n    \"\"\" Step 3: Second association, with low score detection boxes\"\"\"\n    # association the untrack to the low score detections\n    if len(dets_second) &gt; 0:\n        \"\"\"Detections\"\"\"\n        detections_second = [\n            STrack(STrack.tlbr_to_tlwh(tlbr), s, c, self.minimum_consecutive_frames)\n            for (tlbr, s, c) in zip(dets_second, scores_second, class_ids_second)\n        ]\n    else:\n        detections_second = []\n    r_tracked_stracks = [\n        strack_pool[i]\n        for i in u_track\n        if strack_pool[i].state == TrackState.Tracked\n    ]\n    dists = matching.iou_distance(r_tracked_stracks, detections_second)\n    matches, u_track, u_detection_second = matching.linear_assignment(\n        dists, thresh=0.5\n    )\n    for itracked, idet in matches:\n        track = r_tracked_stracks[itracked]\n        det = detections_second[idet]\n        if track.state == TrackState.Tracked:\n            track.update(det, self.frame_id)\n            activated_starcks.append(track)\n        else:\n            track.re_activate(det, self.frame_id, new_id=False)\n            refind_stracks.append(track)\n\n    for it in u_track:\n        track = r_tracked_stracks[it]\n        if not track.state == TrackState.Lost:\n            track.mark_lost()\n            lost_stracks.append(track)\n\n    \"\"\"Deal with unconfirmed tracks, usually tracks with only one beginning frame\"\"\"\n    detections = [detections[i] for i in u_detection]\n    dists = matching.iou_distance(unconfirmed, detections)\n\n    dists = matching.fuse_score(dists, detections)\n    matches, u_unconfirmed, u_detection = matching.linear_assignment(\n        dists, thresh=0.7\n    )\n    for itracked, idet in matches:\n        unconfirmed[itracked].update(detections[idet], self.frame_id)\n        activated_starcks.append(unconfirmed[itracked])\n    for it in u_unconfirmed:\n        track = unconfirmed[it]\n        track.mark_removed()\n        removed_stracks.append(track)\n\n    \"\"\" Step 4: Init new stracks\"\"\"\n    for inew in u_detection:\n        track = detections[inew]\n        if track.score &lt; self.det_thresh:\n            continue\n        track.activate(self.kalman_filter, self.frame_id)\n        activated_starcks.append(track)\n    \"\"\" Step 5: Update state\"\"\"\n    for track in self.lost_tracks:\n        if self.frame_id - track.end_frame &gt; self.max_time_lost:\n            track.mark_removed()\n            removed_stracks.append(track)\n\n    self.tracked_tracks = [\n        t for t in self.tracked_tracks if t.state == TrackState.Tracked\n    ]\n    self.tracked_tracks = joint_tracks(self.tracked_tracks, activated_starcks)\n    self.tracked_tracks = joint_tracks(self.tracked_tracks, refind_stracks)\n    self.lost_tracks = sub_tracks(self.lost_tracks, self.tracked_tracks)\n    self.lost_tracks.extend(lost_stracks)\n    self.lost_tracks = sub_tracks(self.lost_tracks, self.removed_tracks)\n    self.removed_tracks = removed_stracks\n    self.tracked_tracks, self.lost_tracks = remove_duplicate_tracks(\n        self.tracked_tracks, self.lost_tracks\n    )\n    output_stracks = [track for track in self.tracked_tracks if track.is_activated]\n\n    return output_stracks\n</code></pre>"},{"location":"classification/core/","title":"Classifications","text":"Source code in <code>supervision/classification/core.py</code> <pre><code>@dataclass\nclass Classifications:\n    class_id: np.ndarray\n    confidence: Optional[np.ndarray] = None\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"\n        Validate the classification inputs.\n        \"\"\"\n        n = len(self.class_id)\n\n        _validate_class_ids(self.class_id, n)\n        _validate_confidence(self.confidence, n)\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the number of classifications.\n        \"\"\"\n        return len(self.class_id)\n\n    @classmethod\n    def from_clip(cls, clip_results) -&gt; Classifications:\n        \"\"\"\n        Creates a Classifications instance from a\n        [clip](https://github.com/openai/clip) inference result.\n\n        Args:\n            clip_results (np.ndarray): The inference result from clip model.\n\n        Returns:\n            Classifications: A new Classifications object.\n\n        Example:\n            ```python\n            from PIL import Image\n            import clip\n            import supervision as sv\n\n            model, preprocess = clip.load('ViT-B/32')\n\n            image = cv2.imread(SOURCE_IMAGE_PATH)\n            image = preprocess(image).unsqueeze(0)\n\n            text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"])\n            output, _ = model(image, text)\n            classifications = sv.Classifications.from_clip(output)\n            ```\n        \"\"\"\n\n        confidence = clip_results.softmax(dim=-1).cpu().detach().numpy()[0]\n\n        if len(confidence) == 0:\n            return cls(class_id=np.array([]), confidence=np.array([]))\n\n        class_ids = np.arange(len(confidence))\n        return cls(class_id=class_ids, confidence=confidence)\n\n    @classmethod\n    def from_ultralytics(cls, ultralytics_results) -&gt; Classifications:\n        \"\"\"\n        Creates a Classifications instance from a\n        [ultralytics](https://github.com/ultralytics/ultralytics) inference result.\n\n        Args:\n            ultralytics_results (ultralytics.engine.results.Results):\n                The inference result from ultralytics model.\n\n        Returns:\n            Classifications: A new Classifications object.\n\n        Example:\n            ```python\n            import cv2\n            from ultralytics import YOLO\n            import supervision as sv\n\n            image = cv2.imread(SOURCE_IMAGE_PATH)\n            model = YOLO('yolov8n-cls.pt')\n\n            output = model(image)[0]\n            classifications = sv.Classifications.from_ultralytics(output)\n            ```\n        \"\"\"\n        confidence = ultralytics_results.probs.data.cpu().numpy()\n        return cls(class_id=np.arange(confidence.shape[0]), confidence=confidence)\n\n    @classmethod\n    def from_timm(cls, timm_results) -&gt; Classifications:\n        \"\"\"\n        Creates a Classifications instance from a\n        [timm](https://huggingface.co/docs/hub/timm) inference result.\n\n        Args:\n            timm_results (torch.Tensor): The inference result from timm model.\n\n        Returns:\n            Classifications: A new Classifications object.\n\n        Example:\n            ```python\n            from PIL import Image\n            import timm\n            from timm.data import resolve_data_config, create_transform\n            import supervision as sv\n\n            model = timm.create_model(\n                model_name='hf-hub:nateraw/resnet50-oxford-iiit-pet',\n                pretrained=True\n            ).eval()\n\n            config = resolve_data_config({}, model=model)\n            transform = create_transform(**config)\n\n            image = Image.open(SOURCE_IMAGE_PATH).convert('RGB')\n            x = transform(image).unsqueeze(0)\n\n            output = model(x)\n\n            classifications = sv.Classifications.from_timm(output)\n            ```\n        \"\"\"\n        confidence = timm_results.cpu().detach().numpy()[0]\n\n        if len(confidence) == 0:\n            return cls(class_id=np.array([]), confidence=np.array([]))\n\n        class_id = np.arange(len(confidence))\n        return cls(class_id=class_id, confidence=confidence)\n\n    def get_top_k(self, k: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Retrieve the top k class IDs and confidences,\n            ordered in descending order by confidence.\n\n        Args:\n            k (int): The number of top class IDs and confidences to retrieve.\n\n        Returns:\n            Tuple[np.ndarray, np.ndarray]: A tuple containing\n                the top k class IDs and confidences.\n\n        Example:\n            ```python\n            import supervision as sv\n\n            classifications = sv.Classifications(...)\n\n            classifications.get_top_k(1)\n\n            (array([1]), array([0.9]))\n            ```\n        \"\"\"\n        if self.confidence is None:\n            raise ValueError(\"top_k could not be calculated, confidence is None\")\n\n        order = np.argsort(self.confidence)[::-1]\n        top_k_order = order[:k]\n        top_k_class_id = self.class_id[top_k_order]\n        top_k_confidence = self.confidence[top_k_order]\n\n        return top_k_class_id, top_k_confidence\n</code></pre>"},{"location":"classification/core/#supervision.classification.core.Classifications-functions","title":"Functions","text":""},{"location":"classification/core/#supervision.classification.core.Classifications.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of classifications.</p> Source code in <code>supervision/classification/core.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the number of classifications.\n    \"\"\"\n    return len(self.class_id)\n</code></pre>"},{"location":"classification/core/#supervision.classification.core.Classifications.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate the classification inputs.</p> Source code in <code>supervision/classification/core.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"\n    Validate the classification inputs.\n    \"\"\"\n    n = len(self.class_id)\n\n    _validate_class_ids(self.class_id, n)\n    _validate_confidence(self.confidence, n)\n</code></pre>"},{"location":"classification/core/#supervision.classification.core.Classifications.from_clip","title":"<code>from_clip(clip_results)</code>  <code>classmethod</code>","text":"<p>Creates a Classifications instance from a clip inference result.</p> <p>Parameters:</p> Name Type Description Default <code>clip_results</code> <code>ndarray</code> <p>The inference result from clip model.</p> required <p>Returns:</p> Name Type Description <code>Classifications</code> <code>Classifications</code> <p>A new Classifications object.</p> Example <pre><code>from PIL import Image\nimport clip\nimport supervision as sv\n\nmodel, preprocess = clip.load('ViT-B/32')\n\nimage = cv2.imread(SOURCE_IMAGE_PATH)\nimage = preprocess(image).unsqueeze(0)\n\ntext = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"])\noutput, _ = model(image, text)\nclassifications = sv.Classifications.from_clip(output)\n</code></pre> Source code in <code>supervision/classification/core.py</code> <pre><code>@classmethod\ndef from_clip(cls, clip_results) -&gt; Classifications:\n    \"\"\"\n    Creates a Classifications instance from a\n    [clip](https://github.com/openai/clip) inference result.\n\n    Args:\n        clip_results (np.ndarray): The inference result from clip model.\n\n    Returns:\n        Classifications: A new Classifications object.\n\n    Example:\n        ```python\n        from PIL import Image\n        import clip\n        import supervision as sv\n\n        model, preprocess = clip.load('ViT-B/32')\n\n        image = cv2.imread(SOURCE_IMAGE_PATH)\n        image = preprocess(image).unsqueeze(0)\n\n        text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"])\n        output, _ = model(image, text)\n        classifications = sv.Classifications.from_clip(output)\n        ```\n    \"\"\"\n\n    confidence = clip_results.softmax(dim=-1).cpu().detach().numpy()[0]\n\n    if len(confidence) == 0:\n        return cls(class_id=np.array([]), confidence=np.array([]))\n\n    class_ids = np.arange(len(confidence))\n    return cls(class_id=class_ids, confidence=confidence)\n</code></pre>"},{"location":"classification/core/#supervision.classification.core.Classifications.from_timm","title":"<code>from_timm(timm_results)</code>  <code>classmethod</code>","text":"<p>Creates a Classifications instance from a timm inference result.</p> <p>Parameters:</p> Name Type Description Default <code>timm_results</code> <code>Tensor</code> <p>The inference result from timm model.</p> required <p>Returns:</p> Name Type Description <code>Classifications</code> <code>Classifications</code> <p>A new Classifications object.</p> Example <pre><code>from PIL import Image\nimport timm\nfrom timm.data import resolve_data_config, create_transform\nimport supervision as sv\n\nmodel = timm.create_model(\n    model_name='hf-hub:nateraw/resnet50-oxford-iiit-pet',\n    pretrained=True\n).eval()\n\nconfig = resolve_data_config({}, model=model)\ntransform = create_transform(**config)\n\nimage = Image.open(SOURCE_IMAGE_PATH).convert('RGB')\nx = transform(image).unsqueeze(0)\n\noutput = model(x)\n\nclassifications = sv.Classifications.from_timm(output)\n</code></pre> Source code in <code>supervision/classification/core.py</code> <pre><code>@classmethod\ndef from_timm(cls, timm_results) -&gt; Classifications:\n    \"\"\"\n    Creates a Classifications instance from a\n    [timm](https://huggingface.co/docs/hub/timm) inference result.\n\n    Args:\n        timm_results (torch.Tensor): The inference result from timm model.\n\n    Returns:\n        Classifications: A new Classifications object.\n\n    Example:\n        ```python\n        from PIL import Image\n        import timm\n        from timm.data import resolve_data_config, create_transform\n        import supervision as sv\n\n        model = timm.create_model(\n            model_name='hf-hub:nateraw/resnet50-oxford-iiit-pet',\n            pretrained=True\n        ).eval()\n\n        config = resolve_data_config({}, model=model)\n        transform = create_transform(**config)\n\n        image = Image.open(SOURCE_IMAGE_PATH).convert('RGB')\n        x = transform(image).unsqueeze(0)\n\n        output = model(x)\n\n        classifications = sv.Classifications.from_timm(output)\n        ```\n    \"\"\"\n    confidence = timm_results.cpu().detach().numpy()[0]\n\n    if len(confidence) == 0:\n        return cls(class_id=np.array([]), confidence=np.array([]))\n\n    class_id = np.arange(len(confidence))\n    return cls(class_id=class_id, confidence=confidence)\n</code></pre>"},{"location":"classification/core/#supervision.classification.core.Classifications.from_ultralytics","title":"<code>from_ultralytics(ultralytics_results)</code>  <code>classmethod</code>","text":"<p>Creates a Classifications instance from a ultralytics inference result.</p> <p>Parameters:</p> Name Type Description Default <code>ultralytics_results</code> <code>Results</code> <p>The inference result from ultralytics model.</p> required <p>Returns:</p> Name Type Description <code>Classifications</code> <code>Classifications</code> <p>A new Classifications object.</p> Example <pre><code>import cv2\nfrom ultralytics import YOLO\nimport supervision as sv\n\nimage = cv2.imread(SOURCE_IMAGE_PATH)\nmodel = YOLO('yolov8n-cls.pt')\n\noutput = model(image)[0]\nclassifications = sv.Classifications.from_ultralytics(output)\n</code></pre> Source code in <code>supervision/classification/core.py</code> <pre><code>@classmethod\ndef from_ultralytics(cls, ultralytics_results) -&gt; Classifications:\n    \"\"\"\n    Creates a Classifications instance from a\n    [ultralytics](https://github.com/ultralytics/ultralytics) inference result.\n\n    Args:\n        ultralytics_results (ultralytics.engine.results.Results):\n            The inference result from ultralytics model.\n\n    Returns:\n        Classifications: A new Classifications object.\n\n    Example:\n        ```python\n        import cv2\n        from ultralytics import YOLO\n        import supervision as sv\n\n        image = cv2.imread(SOURCE_IMAGE_PATH)\n        model = YOLO('yolov8n-cls.pt')\n\n        output = model(image)[0]\n        classifications = sv.Classifications.from_ultralytics(output)\n        ```\n    \"\"\"\n    confidence = ultralytics_results.probs.data.cpu().numpy()\n    return cls(class_id=np.arange(confidence.shape[0]), confidence=confidence)\n</code></pre>"},{"location":"classification/core/#supervision.classification.core.Classifications.get_top_k","title":"<code>get_top_k(k)</code>","text":"<p>Retrieve the top k class IDs and confidences,     ordered in descending order by confidence.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>The number of top class IDs and confidences to retrieve.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: A tuple containing the top k class IDs and confidences.</p> Example <pre><code>import supervision as sv\n\nclassifications = sv.Classifications(...)\n\nclassifications.get_top_k(1)\n\n(array([1]), array([0.9]))\n</code></pre> Source code in <code>supervision/classification/core.py</code> <pre><code>def get_top_k(self, k: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Retrieve the top k class IDs and confidences,\n        ordered in descending order by confidence.\n\n    Args:\n        k (int): The number of top class IDs and confidences to retrieve.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: A tuple containing\n            the top k class IDs and confidences.\n\n    Example:\n        ```python\n        import supervision as sv\n\n        classifications = sv.Classifications(...)\n\n        classifications.get_top_k(1)\n\n        (array([1]), array([0.9]))\n        ```\n    \"\"\"\n    if self.confidence is None:\n        raise ValueError(\"top_k could not be calculated, confidence is None\")\n\n    order = np.argsort(self.confidence)[::-1]\n    top_k_order = order[:k]\n    top_k_class_id = self.class_id[top_k_order]\n    top_k_confidence = self.confidence[top_k_order]\n\n    return top_k_class_id, top_k_confidence\n</code></pre>"},{"location":"datasets/core/","title":"Datasets","text":"<p>Warning</p> <p>Dataset API is still fluid and may change. If you use Dataset API in your project until further notice, freeze the <code>supervision</code> version in your <code>requirements.txt</code> or <code>setup.py</code>.</p> DetectionDataset <p>               Bases: <code>BaseDataset</code></p> <p>Contains information about a detection dataset. Handles lazy image loading and annotation retrieval, dataset splitting, conversions into multiple formats.</p> <p>Attributes:</p> Name Type Description <code>classes</code> <code>List[str]</code> <p>List containing dataset class names.</p> <code>images</code> <code>Union[List[str], Dict[str, ndarray]]</code> <p>Accepts a list of image paths, or dictionaries of loaded cv2 images with paths as keys. If you pass a list of paths, the dataset will lazily load images on demand, which is much more memory-efficient.</p> <code>annotations</code> <code>Dict[str, Detections]</code> <p>Dictionary mapping image path to annotations. The dictionary keys match match the keys in <code>images</code> or entries in the list of image paths.</p> Source code in <code>supervision/dataset/core.py</code> <pre><code>class DetectionDataset(BaseDataset):\n    \"\"\"\n    Contains information about a detection dataset. Handles lazy image loading\n    and annotation retrieval, dataset splitting, conversions into multiple\n    formats.\n\n    Attributes:\n        classes (List[str]): List containing dataset class names.\n        images (Union[List[str], Dict[str, np.ndarray]]):\n            Accepts a list of image paths, or dictionaries of loaded cv2 images\n            with paths as keys. If you pass a list of paths, the dataset will\n            lazily load images on demand, which is much more memory-efficient.\n        annotations (Dict[str, Detections]): Dictionary mapping\n            image path to annotations. The dictionary keys match\n            match the keys in `images` or entries in the list of\n            image paths.\n    \"\"\"\n\n    def __init__(\n        self,\n        classes: List[str],\n        images: Union[List[str], Dict[str, np.ndarray]],\n        annotations: Dict[str, Detections],\n    ) -&gt; None:\n        self.classes = classes\n\n        if set(images) != set(annotations):\n            raise ValueError(\n                \"The keys of the images and annotations dictionaries must match.\"\n            )\n        self.annotations = annotations\n\n        # Eliminate duplicates while preserving order\n        self.image_paths = list(dict.fromkeys(images))\n\n        self._images_in_memory: Dict[str, np.ndarray] = {}\n        if isinstance(images, dict):\n            self._images_in_memory = images\n            warn_deprecated(\n                \"Passing a `Dict[str, np.ndarray]` into `DetectionDataset` is \"\n                \"deprecated and will be removed in `supervision-0.26.0`. Use \"\n                \"a list of paths `List[str]` instead.\"\n            )\n            # TODO: when supervision-0.26.0 is released, and Dict[str, np.ndarray]\n            #       for images is no longer supported, also simplify the rest of\n            #       the code. E.g. list(images) is no longer needed, and merge can\n            #       be simplified.\n\n    @property\n    @deprecated(\n        \"`DetectionDataset.images` property is deprecated and will be removed in \"\n        \"`supervision-0.26.0`. Iterate with `for path, image, annotation in dataset:` \"\n        \"instead.\"\n    )\n    def images(self) -&gt; Dict[str, np.ndarray]:\n        \"\"\"\n        Load all images to memory and return them as a dictionary.\n\n        !!! warning\n\n            Only use this when you need all images at once.\n            It is much more memory-efficient to initialize dataset with\n            image paths and use `for path, image, annotation in dataset:`.\n        \"\"\"\n        if self._images_in_memory:\n            return self._images_in_memory\n\n        images = {image_path: cv2.imread(image_path) for image_path in self.image_paths}\n        return images\n\n    def _get_image(self, image_path: str) -&gt; np.ndarray:\n        \"\"\"Assumes that image is in dataset\"\"\"\n        if self._images_in_memory:\n            return self._images_in_memory[image_path]\n        return cv2.imread(image_path)\n\n    def __len__(self) -&gt; int:\n        return len(self._images_in_memory) or len(self.image_paths)\n\n    def __getitem__(self, i: int) -&gt; Tuple[str, np.ndarray, Detections]:\n        \"\"\"\n        Returns:\n            Tuple[str, np.ndarray, Detections]: The image path, image data,\n                and its corresponding annotation at index i.\n        \"\"\"\n        image_path = self.image_paths[i]\n        image = self._get_image(image_path)\n        annotation = self.annotations[image_path]\n        return image_path, image, annotation\n\n    def __iter__(self) -&gt; Iterator[Tuple[str, np.ndarray, Detections]]:\n        \"\"\"\n        Iterate over the images and annotations in the dataset.\n\n        Yields:\n            Iterator[Tuple[str, np.ndarray, Detections]]:\n                An iterator that yields tuples containing the image path,\n                the image data, and its corresponding annotation.\n        \"\"\"\n        for i in range(len(self)):\n            image_path, image, annotation = self[i]\n            yield image_path, image, annotation\n\n    def __eq__(self, other) -&gt; bool:\n        if not isinstance(other, DetectionDataset):\n            return False\n\n        if set(self.classes) != set(other.classes):\n            return False\n\n        if self.image_paths != other.image_paths:\n            return False\n\n        if self._images_in_memory or other._images_in_memory:\n            if not np.array_equal(\n                list(self._images_in_memory.values()),\n                list(other._images_in_memory.values()),\n            ):\n                return False\n\n        if self.annotations != other.annotations:\n            return False\n\n        return True\n\n    def split(\n        self, split_ratio=0.8, random_state=None, shuffle: bool = True\n    ) -&gt; Tuple[DetectionDataset, DetectionDataset]:\n        \"\"\"\n        Splits the dataset into two parts (training and testing)\n            using the provided split_ratio.\n\n        Args:\n            split_ratio (float, optional): The ratio of the training\n                set to the entire dataset.\n            random_state (int, optional): The seed for the random number generator.\n                This is used for reproducibility.\n            shuffle (bool, optional): Whether to shuffle the data before splitting.\n\n        Returns:\n            Tuple[DetectionDataset, DetectionDataset]: A tuple containing\n                the training and testing datasets.\n\n        Examples:\n            ```python\n            import supervision as sv\n\n            ds = sv.DetectionDataset(...)\n            train_ds, test_ds = ds.split(split_ratio=0.7, random_state=42, shuffle=True)\n            len(train_ds), len(test_ds)\n            # (700, 300)\n            ```\n        \"\"\"\n\n        train_paths, test_paths = train_test_split(\n            data=self.image_paths,\n            train_ratio=split_ratio,\n            random_state=random_state,\n            shuffle=shuffle,\n        )\n\n        train_input: Union[List[str], Dict[str, np.ndarray]]\n        test_input: Union[List[str], Dict[str, np.ndarray]]\n        if self._images_in_memory:\n            train_input = {path: self._images_in_memory[path] for path in train_paths}\n            test_input = {path: self._images_in_memory[path] for path in test_paths}\n        else:\n            train_input = train_paths\n            test_input = test_paths\n        train_annotations = {path: self.annotations[path] for path in train_paths}\n        test_annotations = {path: self.annotations[path] for path in test_paths}\n\n        train_dataset = DetectionDataset(\n            classes=self.classes,\n            images=train_input,\n            annotations=train_annotations,\n        )\n        test_dataset = DetectionDataset(\n            classes=self.classes,\n            images=test_input,\n            annotations=test_annotations,\n        )\n        return train_dataset, test_dataset\n\n    @classmethod\n    def merge(cls, dataset_list: List[DetectionDataset]) -&gt; DetectionDataset:\n        \"\"\"\n        Merge a list of `DetectionDataset` objects into a single\n            `DetectionDataset` object.\n\n        This method takes a list of `DetectionDataset` objects and combines\n        their respective fields (`classes`, `images`,\n        `annotations`) into a single `DetectionDataset` object.\n\n        Args:\n            dataset_list (List[DetectionDataset]): A list of `DetectionDataset`\n                objects to merge.\n\n        Returns:\n            (DetectionDataset): A single `DetectionDataset` object containing\n            the merged data from the input list.\n\n        Examples:\n            ```python\n            import supervision as sv\n\n            ds_1 = sv.DetectionDataset(...)\n            len(ds_1)\n            # 100\n            ds_1.classes\n            # ['dog', 'person']\n\n            ds_2 = sv.DetectionDataset(...)\n            len(ds_2)\n            # 200\n            ds_2.classes\n            # ['cat']\n\n            ds_merged = sv.DetectionDataset.merge([ds_1, ds_2])\n            len(ds_merged)\n            # 300\n            ds_merged.classes\n            # ['cat', 'dog', 'person']\n            ```\n        \"\"\"\n\n        def is_in_memory(dataset: DetectionDataset) -&gt; bool:\n            return len(dataset._images_in_memory) &gt; 0 or len(dataset.image_paths) == 0\n\n        def is_lazy(dataset: DetectionDataset) -&gt; bool:\n            return len(dataset._images_in_memory) == 0\n\n        all_in_memory = all([is_in_memory(dataset) for dataset in dataset_list])\n        all_lazy = all([is_lazy(dataset) for dataset in dataset_list])\n        if not all_in_memory and not all_lazy:\n            raise ValueError(\n                \"Merging lazy and in-memory DetectionDatasets is not supported.\"\n            )\n\n        images_in_memory = {}\n        for dataset in dataset_list:\n            images_in_memory.update(dataset._images_in_memory)\n\n        image_paths = list(\n            chain.from_iterable(dataset.image_paths for dataset in dataset_list)\n        )\n        image_paths_unique = list(dict.fromkeys(image_paths))\n        if len(image_paths) != len(image_paths_unique):\n            duplicates = find_duplicates(image_paths)\n            raise ValueError(\n                f\"Image paths {duplicates} are not unique across datasets.\"\n            )\n        image_paths = image_paths_unique\n\n        classes = merge_class_lists(\n            class_lists=[dataset.classes for dataset in dataset_list]\n        )\n\n        annotations = {}\n        for dataset in dataset_list:\n            annotations.update(dataset.annotations)\n        for dataset in dataset_list:\n            class_index_mapping = build_class_index_mapping(\n                source_classes=dataset.classes, target_classes=classes\n            )\n            for image_path in dataset.image_paths:\n                annotations[image_path] = map_detections_class_id(\n                    source_to_target_mapping=class_index_mapping,\n                    detections=annotations[image_path],\n                )\n\n        return cls(\n            classes=classes,\n            images=images_in_memory or image_paths,\n            annotations=annotations,\n        )\n\n    def as_pascal_voc(\n        self,\n        images_directory_path: Optional[str] = None,\n        annotations_directory_path: Optional[str] = None,\n        min_image_area_percentage: float = 0.0,\n        max_image_area_percentage: float = 1.0,\n        approximation_percentage: float = 0.0,\n    ) -&gt; None:\n        \"\"\"\n        Exports the dataset to PASCAL VOC format. This method saves the images\n        and their corresponding annotations in PASCAL VOC format.\n\n        Args:\n            images_directory_path (Optional[str]): The path to the directory\n                where the images should be saved.\n                If not provided, images will not be saved.\n            annotations_directory_path (Optional[str]): The path to\n                the directory where the annotations in PASCAL VOC format should be\n                saved. If not provided, annotations will not be saved.\n            min_image_area_percentage (float): The minimum percentage of\n                detection area relative to\n                the image area for a detection to be included.\n                Argument is used only for segmentation datasets.\n            max_image_area_percentage (float): The maximum percentage\n                of detection area relative to\n                the image area for a detection to be included.\n                Argument is used only for segmentation datasets.\n            approximation_percentage (float): The percentage of\n                polygon points to be removed from the input polygon,\n                in the range [0, 1). Argument is used only for segmentation datasets.\n        \"\"\"\n        if images_directory_path:\n            save_dataset_images(\n                dataset=self,\n                images_directory_path=images_directory_path,\n            )\n        if annotations_directory_path:\n            Path(annotations_directory_path).mkdir(parents=True, exist_ok=True)\n            for image_path, image, annotations in self:\n                annotation_name = Path(image_path).stem\n                annotations_path = os.path.join(\n                    annotations_directory_path, f\"{annotation_name}.xml\"\n                )\n                image_name = Path(image_path).name\n                pascal_voc_xml = detections_to_pascal_voc(\n                    detections=annotations,\n                    classes=self.classes,\n                    filename=image_name,\n                    image_shape=image.shape,  # type: ignore\n                    min_image_area_percentage=min_image_area_percentage,\n                    max_image_area_percentage=max_image_area_percentage,\n                    approximation_percentage=approximation_percentage,\n                )\n\n                with open(annotations_path, \"w\") as f:\n                    f.write(pascal_voc_xml)\n\n    @classmethod\n    def from_pascal_voc(\n        cls,\n        images_directory_path: str,\n        annotations_directory_path: str,\n        force_masks: bool = False,\n    ) -&gt; DetectionDataset:\n        \"\"\"\n        Creates a Dataset instance from PASCAL VOC formatted data.\n\n        Args:\n            images_directory_path (str): Path to the directory containing the images.\n            annotations_directory_path (str): Path to the directory\n                containing the PASCAL VOC XML annotations.\n            force_masks (bool, optional): If True, forces masks to\n                be loaded for all annotations, regardless of whether they are present.\n\n        Returns:\n            DetectionDataset: A DetectionDataset instance containing\n                the loaded images and annotations.\n\n        Examples:\n            ```python\n            import roboflow\n            from roboflow import Roboflow\n            import supervision as sv\n\n            roboflow.login()\n\n            rf = Roboflow()\n\n            project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n            dataset = project.version(PROJECT_VERSION).download(\"voc\")\n\n            ds = sv.DetectionDataset.from_pascal_voc(\n                images_directory_path=f\"{dataset.location}/train/images\",\n                annotations_directory_path=f\"{dataset.location}/train/labels\"\n            )\n\n            ds.classes\n            # ['dog', 'person']\n            ```\n        \"\"\"\n\n        classes, image_paths, annotations = load_pascal_voc_annotations(\n            images_directory_path=images_directory_path,\n            annotations_directory_path=annotations_directory_path,\n            force_masks=force_masks,\n        )\n\n        return DetectionDataset(\n            classes=classes, images=image_paths, annotations=annotations\n        )\n\n    @classmethod\n    def from_yolo(\n        cls,\n        images_directory_path: str,\n        annotations_directory_path: str,\n        data_yaml_path: str,\n        force_masks: bool = False,\n        is_obb: bool = False,\n    ) -&gt; DetectionDataset:\n        \"\"\"\n        Creates a Dataset instance from YOLO formatted data.\n\n        Args:\n            images_directory_path (str): The path to the\n                directory containing the images.\n            annotations_directory_path (str): The path to the directory\n                containing the YOLO annotation files.\n            data_yaml_path (str): The path to the data\n                YAML file containing class information.\n            force_masks (bool, optional): If True, forces\n                masks to be loaded for all annotations,\n                regardless of whether they are present.\n            is_obb (bool, optional): If True, loads the annotations in OBB format.\n                OBB annotations are defined as `[class_id, x, y, x, y, x, y, x, y]`,\n                where pairs of [x, y] are box corners.\n\n        Returns:\n            DetectionDataset: A DetectionDataset instance\n                containing the loaded images and annotations.\n\n        Examples:\n            ```python\n            import roboflow\n            from roboflow import Roboflow\n            import supervision as sv\n\n            roboflow.login()\n            rf = Roboflow()\n\n            project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n            dataset = project.version(PROJECT_VERSION).download(\"yolov5\")\n\n            ds = sv.DetectionDataset.from_yolo(\n                images_directory_path=f\"{dataset.location}/train/images\",\n                annotations_directory_path=f\"{dataset.location}/train/labels\",\n                data_yaml_path=f\"{dataset.location}/data.yaml\"\n            )\n\n            ds.classes\n            # ['dog', 'person']\n            ```\n        \"\"\"\n        classes, image_paths, annotations = load_yolo_annotations(\n            images_directory_path=images_directory_path,\n            annotations_directory_path=annotations_directory_path,\n            data_yaml_path=data_yaml_path,\n            force_masks=force_masks,\n            is_obb=is_obb,\n        )\n        return DetectionDataset(\n            classes=classes, images=image_paths, annotations=annotations\n        )\n\n    def as_yolo(\n        self,\n        images_directory_path: Optional[str] = None,\n        annotations_directory_path: Optional[str] = None,\n        data_yaml_path: Optional[str] = None,\n        min_image_area_percentage: float = 0.0,\n        max_image_area_percentage: float = 1.0,\n        approximation_percentage: float = 0.0,\n    ) -&gt; None:\n        \"\"\"\n        Exports the dataset to YOLO format. This method saves the\n        images and their corresponding annotations in YOLO format.\n\n        Args:\n            images_directory_path (Optional[str]): The path to the\n                directory where the images should be saved.\n                If not provided, images will not be saved.\n            annotations_directory_path (Optional[str]): The path to the\n                directory where the annotations in\n                YOLO format should be saved. If not provided,\n                annotations will not be saved.\n            data_yaml_path (Optional[str]): The path where the data.yaml\n                file should be saved.\n                If not provided, the file will not be saved.\n            min_image_area_percentage (float): The minimum percentage of\n                detection area relative to\n                the image area for a detection to be included.\n                Argument is used only for segmentation datasets.\n            max_image_area_percentage (float): The maximum percentage\n                of detection area relative to\n                the image area for a detection to be included.\n                Argument is used only for segmentation datasets.\n            approximation_percentage (float): The percentage of polygon points to\n                be removed from the input polygon, in the range [0, 1).\n                This is useful for simplifying the annotations.\n                Argument is used only for segmentation datasets.\n        \"\"\"\n        if images_directory_path is not None:\n            save_dataset_images(\n                dataset=self, images_directory_path=images_directory_path\n            )\n        if annotations_directory_path is not None:\n            save_yolo_annotations(\n                dataset=self,\n                annotations_directory_path=annotations_directory_path,\n                min_image_area_percentage=min_image_area_percentage,\n                max_image_area_percentage=max_image_area_percentage,\n                approximation_percentage=approximation_percentage,\n            )\n        if data_yaml_path is not None:\n            save_data_yaml(data_yaml_path=data_yaml_path, classes=self.classes)\n\n    @classmethod\n    def from_coco(\n        cls,\n        images_directory_path: str,\n        annotations_path: str,\n        force_masks: bool = False,\n    ) -&gt; DetectionDataset:\n        \"\"\"\n        Creates a Dataset instance from COCO formatted data.\n\n        Args:\n            images_directory_path (str): The path to the\n                directory containing the images.\n            annotations_path (str): The path to the json annotation files.\n            force_masks (bool, optional): If True,\n                forces masks to be loaded for all annotations,\n                regardless of whether they are present.\n\n        Returns:\n            DetectionDataset: A DetectionDataset instance containing\n                the loaded images and annotations.\n\n        Examples:\n            ```python\n            import roboflow\n            from roboflow import Roboflow\n            import supervision as sv\n\n            roboflow.login()\n            rf = Roboflow()\n\n            project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n            dataset = project.version(PROJECT_VERSION).download(\"coco\")\n\n            ds = sv.DetectionDataset.from_coco(\n                images_directory_path=f\"{dataset.location}/train\",\n                annotations_path=f\"{dataset.location}/train/_annotations.coco.json\",\n            )\n\n            ds.classes\n            # ['dog', 'person']\n            ```\n        \"\"\"\n        classes, images, annotations = load_coco_annotations(\n            images_directory_path=images_directory_path,\n            annotations_path=annotations_path,\n            force_masks=force_masks,\n        )\n        return DetectionDataset(classes=classes, images=images, annotations=annotations)\n\n    def as_coco(\n        self,\n        images_directory_path: Optional[str] = None,\n        annotations_path: Optional[str] = None,\n        min_image_area_percentage: float = 0.0,\n        max_image_area_percentage: float = 1.0,\n        approximation_percentage: float = 0.0,\n    ) -&gt; None:\n        \"\"\"\n        Exports the dataset to COCO format. This method saves the\n        images and their corresponding annotations in COCO format.\n\n        !!! tip\n\n            The format of the mask is determined automatically based on its structure:\n\n            - If a mask contains multiple disconnected components or holes, it will be\n            saved using the Run-Length Encoding (RLE) format for efficient storage and\n            processing.\n            - If a mask consists of a single, contiguous region without any holes, it\n            will be encoded as a polygon, preserving the outline of the object.\n\n            This automatic selection ensures that the masks are stored in the most\n            appropriate and space-efficient format, complying with COCO dataset\n            standards.\n\n        Args:\n            images_directory_path (Optional[str]): The path to the directory\n                where the images should be saved.\n                If not provided, images will not be saved.\n            annotations_path (Optional[str]): The path to COCO annotation file.\n            min_image_area_percentage (float): The minimum percentage of\n                detection area relative to\n                the image area for a detection to be included.\n                Argument is used only for segmentation datasets.\n            max_image_area_percentage (float): The maximum percentage of\n                detection area relative to\n                the image area for a detection to be included.\n                Argument is used only for segmentation datasets.\n            approximation_percentage (float): The percentage of polygon points\n                to be removed from the input polygon,\n                in the range [0, 1). This is useful for simplifying the annotations.\n                Argument is used only for segmentation datasets.\n        \"\"\"\n        if images_directory_path is not None:\n            save_dataset_images(\n                dataset=self, images_directory_path=images_directory_path\n            )\n        if annotations_path is not None:\n            save_coco_annotations(\n                dataset=self,\n                annotation_path=annotations_path,\n                min_image_area_percentage=min_image_area_percentage,\n                max_image_area_percentage=max_image_area_percentage,\n                approximation_percentage=approximation_percentage,\n            )\n</code></pre> ClassificationDataset <p>               Bases: <code>BaseDataset</code></p> <p>Contains information about a classification dataset, handles lazy image loading, dataset splitting.</p> <p>Attributes:</p> Name Type Description <code>classes</code> <code>List[str]</code> <p>List containing dataset class names.</p> <code>images</code> <code>Union[List[str], Dict[str, ndarray]]</code> <p>List of image paths or dictionary mapping image name to image data.</p> <code>annotations</code> <code>Dict[str, Classifications]</code> <p>Dictionary mapping image name to annotations.</p> Source code in <code>supervision/dataset/core.py</code> <pre><code>@dataclass\nclass ClassificationDataset(BaseDataset):\n    \"\"\"\n    Contains information about a classification dataset, handles lazy image\n    loading, dataset splitting.\n\n    Attributes:\n        classes (List[str]): List containing dataset class names.\n        images (Union[List[str], Dict[str, np.ndarray]]):\n            List of image paths or dictionary mapping image name to image data.\n        annotations (Dict[str, Classifications]): Dictionary mapping\n            image name to annotations.\n    \"\"\"\n\n    def __init__(\n        self,\n        classes: List[str],\n        images: Union[List[str], Dict[str, np.ndarray]],\n        annotations: Dict[str, Classifications],\n    ) -&gt; None:\n        self.classes = classes\n\n        if set(images) != set(annotations):\n            raise ValueError(\n                \"The keys of the images and annotations dictionaries must match.\"\n            )\n        self.annotations = annotations\n\n        # Eliminate duplicates while preserving order\n        self.image_paths = list(dict.fromkeys(images))\n\n        self._images_in_memory: Dict[str, np.ndarray] = {}\n        if isinstance(images, dict):\n            self._images_in_memory = images\n            warn_deprecated(\n                \"Passing a `Dict[str, np.ndarray]` into `ClassificationDataset` is \"\n                \"deprecated and will be removed in a future release. Use \"\n                \"a list of paths `List[str]` instead.\"\n            )\n\n    @property\n    @deprecated(\n        \"`DetectionDataset.images` property is deprecated and will be removed in \"\n        \"`supervision-0.26.0`. Iterate with `for path, image, annotation in dataset:` \"\n        \"instead.\"\n    )\n    def images(self) -&gt; Dict[str, np.ndarray]:\n        \"\"\"\n        Load all images to memory and return them as a dictionary.\n\n        !!! warning\n\n            Only use this when you need all images at once.\n            It is much more memory-efficient to initialize dataset with\n            image paths and use `for path, image, annotation in dataset:`.\n        \"\"\"\n        if self._images_in_memory:\n            return self._images_in_memory\n\n        images = {image_path: cv2.imread(image_path) for image_path in self.image_paths}\n        return images\n\n    def _get_image(self, image_path: str) -&gt; np.ndarray:\n        \"\"\"Assumes that image is in dataset\"\"\"\n        if self._images_in_memory:\n            return self._images_in_memory[image_path]\n        return cv2.imread(image_path)\n\n    def __len__(self) -&gt; int:\n        return len(self._images_in_memory) or len(self.image_paths)\n\n    def __getitem__(self, i: int) -&gt; Tuple[str, np.ndarray, Classifications]:\n        \"\"\"\n        Returns:\n            Tuple[str, np.ndarray, Classifications]: The image path, image data,\n                and its corresponding annotation at index i.\n        \"\"\"\n        image_path = self.image_paths[i]\n        image = self._get_image(image_path)\n        annotation = self.annotations[image_path]\n        return image_path, image, annotation\n\n    def __iter__(self) -&gt; Iterator[Tuple[str, np.ndarray, Classifications]]:\n        \"\"\"\n        Iterate over the images and annotations in the dataset.\n\n        Yields:\n            Iterator[Tuple[str, np.ndarray, Detections]]:\n                An iterator that yields tuples containing the image path,\n                the image data, and its corresponding annotation.\n        \"\"\"\n        for i in range(len(self)):\n            image_path, image, annotation = self[i]\n            yield image_path, image, annotation\n\n    def __eq__(self, other) -&gt; bool:\n        if not isinstance(other, ClassificationDataset):\n            return False\n\n        if set(self.classes) != set(other.classes):\n            return False\n\n        if self.image_paths != other.image_paths:\n            return False\n\n        if self._images_in_memory or other._images_in_memory:\n            if not np.array_equal(\n                list(self._images_in_memory.values()),\n                list(other._images_in_memory.values()),\n            ):\n                return False\n\n        if self.annotations != other.annotations:\n            return False\n\n        return True\n\n    def split(\n        self, split_ratio=0.8, random_state=None, shuffle: bool = True\n    ) -&gt; Tuple[ClassificationDataset, ClassificationDataset]:\n        \"\"\"\n        Splits the dataset into two parts (training and testing)\n            using the provided split_ratio.\n\n        Args:\n            split_ratio (float, optional): The ratio of the training\n                set to the entire dataset.\n            random_state (int, optional): The seed for the\n                random number generator. This is used for reproducibility.\n            shuffle (bool, optional): Whether to shuffle the data before splitting.\n\n        Returns:\n            Tuple[ClassificationDataset, ClassificationDataset]: A tuple containing\n            the training and testing datasets.\n\n        Examples:\n            ```python\n            import supervision as sv\n\n            cd = sv.ClassificationDataset(...)\n            train_cd,test_cd = cd.split(split_ratio=0.7, random_state=42,shuffle=True)\n            len(train_cd), len(test_cd)\n            # (700, 300)\n            ```\n        \"\"\"\n        train_paths, test_paths = train_test_split(\n            data=self.image_paths,\n            train_ratio=split_ratio,\n            random_state=random_state,\n            shuffle=shuffle,\n        )\n\n        train_input: Union[List[str], Dict[str, np.ndarray]]\n        test_input: Union[List[str], Dict[str, np.ndarray]]\n        if self._images_in_memory:\n            train_input = {path: self._images_in_memory[path] for path in train_paths}\n            test_input = {path: self._images_in_memory[path] for path in test_paths}\n        else:\n            train_input = train_paths\n            test_input = test_paths\n        train_annotations = {path: self.annotations[path] for path in train_paths}\n        test_annotations = {path: self.annotations[path] for path in test_paths}\n\n        train_dataset = ClassificationDataset(\n            classes=self.classes,\n            images=train_input,\n            annotations=train_annotations,\n        )\n        test_dataset = ClassificationDataset(\n            classes=self.classes,\n            images=test_input,\n            annotations=test_annotations,\n        )\n\n        return train_dataset, test_dataset\n\n    def as_folder_structure(self, root_directory_path: str) -&gt; None:\n        \"\"\"\n        Saves the dataset as a multi-class folder structure.\n\n        Args:\n            root_directory_path (str): The path to the directory\n                where the dataset will be saved.\n        \"\"\"\n        os.makedirs(root_directory_path, exist_ok=True)\n\n        for class_name in self.classes:\n            os.makedirs(os.path.join(root_directory_path, class_name), exist_ok=True)\n\n        for image_save_path, image, annotation in self:\n            image_name = Path(image_save_path).name\n            class_id = (\n                annotation.class_id[0]\n                if annotation.confidence is None\n                else annotation.get_top_k(1)[0][0]\n            )\n            class_name = self.classes[class_id]\n            image_save_path = os.path.join(root_directory_path, class_name, image_name)\n            cv2.imwrite(image_save_path, image)\n\n    @classmethod\n    def from_folder_structure(cls, root_directory_path: str) -&gt; ClassificationDataset:\n        \"\"\"\n        Load data from a multiclass folder structure into a ClassificationDataset.\n\n        Args:\n            root_directory_path (str): The path to the dataset directory.\n\n        Returns:\n            ClassificationDataset: The dataset.\n\n        Examples:\n            ```python\n            import roboflow\n            from roboflow import Roboflow\n            import supervision as sv\n\n            roboflow.login()\n            rf = Roboflow()\n\n            project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n            dataset = project.version(PROJECT_VERSION).download(\"folder\")\n\n            cd = sv.ClassificationDataset.from_folder_structure(\n                root_directory_path=f\"{dataset.location}/train\"\n            )\n            ```\n        \"\"\"\n        classes = os.listdir(root_directory_path)\n        classes = sorted(set(classes))\n\n        image_paths = []\n        annotations = {}\n\n        for class_name in classes:\n            class_id = classes.index(class_name)\n\n            for image in os.listdir(os.path.join(root_directory_path, class_name)):\n                image_path = str(os.path.join(root_directory_path, class_name, image))\n                image_paths.append(image_path)\n                annotations[image_path] = Classifications(\n                    class_id=np.array([class_id]),\n                )\n\n        return cls(\n            classes=classes,\n            images=image_paths,\n            annotations=annotations,\n        )\n</code></pre>"},{"location":"datasets/core/#supervision.dataset.core.DetectionDataset-attributes","title":"Attributes","text":""},{"location":"datasets/core/#supervision.dataset.core.DetectionDataset.images","title":"<code>images: Dict[str, np.ndarray]</code>  <code>property</code>","text":"<p>Load all images to memory and return them as a dictionary.</p> <p>Warning</p> <p>Only use this when you need all images at once. It is much more memory-efficient to initialize dataset with image paths and use <code>for path, image, annotation in dataset:</code>.</p>"},{"location":"datasets/core/#supervision.dataset.core.DetectionDataset-functions","title":"Functions","text":""},{"location":"datasets/core/#supervision.dataset.core.DetectionDataset.__getitem__","title":"<code>__getitem__(i)</code>","text":"<p>Returns:</p> Type Description <code>Tuple[str, ndarray, Detections]</code> <p>Tuple[str, np.ndarray, Detections]: The image path, image data, and its corresponding annotation at index i.</p> Source code in <code>supervision/dataset/core.py</code> <pre><code>def __getitem__(self, i: int) -&gt; Tuple[str, np.ndarray, Detections]:\n    \"\"\"\n    Returns:\n        Tuple[str, np.ndarray, Detections]: The image path, image data,\n            and its corresponding annotation at index i.\n    \"\"\"\n    image_path = self.image_paths[i]\n    image = self._get_image(image_path)\n    annotation = self.annotations[image_path]\n    return image_path, image, annotation\n</code></pre>"},{"location":"datasets/core/#supervision.dataset.core.DetectionDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over the images and annotations in the dataset.</p> <p>Yields:</p> Type Description <code>str</code> <p>Iterator[Tuple[str, np.ndarray, Detections]]: An iterator that yields tuples containing the image path, the image data, and its corresponding annotation.</p> Source code in <code>supervision/dataset/core.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Tuple[str, np.ndarray, Detections]]:\n    \"\"\"\n    Iterate over the images and annotations in the dataset.\n\n    Yields:\n        Iterator[Tuple[str, np.ndarray, Detections]]:\n            An iterator that yields tuples containing the image path,\n            the image data, and its corresponding annotation.\n    \"\"\"\n    for i in range(len(self)):\n        image_path, image, annotation = self[i]\n        yield image_path, image, annotation\n</code></pre>"},{"location":"datasets/core/#supervision.dataset.core.DetectionDataset.as_coco","title":"<code>as_coco(images_directory_path=None, annotations_path=None, min_image_area_percentage=0.0, max_image_area_percentage=1.0, approximation_percentage=0.0)</code>","text":"<p>Exports the dataset to COCO format. This method saves the images and their corresponding annotations in COCO format.</p> <p>Tip</p> <p>The format of the mask is determined automatically based on its structure:</p> <ul> <li>If a mask contains multiple disconnected components or holes, it will be saved using the Run-Length Encoding (RLE) format for efficient storage and processing.</li> <li>If a mask consists of a single, contiguous region without any holes, it will be encoded as a polygon, preserving the outline of the object.</li> </ul> <p>This automatic selection ensures that the masks are stored in the most appropriate and space-efficient format, complying with COCO dataset standards.</p> <p>Parameters:</p> Name Type Description Default <code>images_directory_path</code> <code>Optional[str]</code> <p>The path to the directory where the images should be saved. If not provided, images will not be saved.</p> <code>None</code> <code>annotations_path</code> <code>Optional[str]</code> <p>The path to COCO annotation file.</p> <code>None</code> <code>min_image_area_percentage</code> <code>float</code> <p>The minimum percentage of detection area relative to the image area for a detection to be included. Argument is used only for segmentation datasets.</p> <code>0.0</code> <code>max_image_area_percentage</code> <code>float</code> <p>The maximum percentage of detection area relative to the image area for a detection to be included. Argument is used only for segmentation datasets.</p> <code>1.0</code> <code>approximation_percentage</code> <code>float</code> <p>The percentage of polygon points to be removed from the input polygon, in the range [0, 1). This is useful for simplifying the annotations. Argument is used only for segmentation datasets.</p> <code>0.0</code> Source code in <code>supervision/dataset/core.py</code> <pre><code>def as_coco(\n    self,\n    images_directory_path: Optional[str] = None,\n    annotations_path: Optional[str] = None,\n    min_image_area_percentage: float = 0.0,\n    max_image_area_percentage: float = 1.0,\n    approximation_percentage: float = 0.0,\n) -&gt; None:\n    \"\"\"\n    Exports the dataset to COCO format. This method saves the\n    images and their corresponding annotations in COCO format.\n\n    !!! tip\n\n        The format of the mask is determined automatically based on its structure:\n\n        - If a mask contains multiple disconnected components or holes, it will be\n        saved using the Run-Length Encoding (RLE) format for efficient storage and\n        processing.\n        - If a mask consists of a single, contiguous region without any holes, it\n        will be encoded as a polygon, preserving the outline of the object.\n\n        This automatic selection ensures that the masks are stored in the most\n        appropriate and space-efficient format, complying with COCO dataset\n        standards.\n\n    Args:\n        images_directory_path (Optional[str]): The path to the directory\n            where the images should be saved.\n            If not provided, images will not be saved.\n        annotations_path (Optional[str]): The path to COCO annotation file.\n        min_image_area_percentage (float): The minimum percentage of\n            detection area relative to\n            the image area for a detection to be included.\n            Argument is used only for segmentation datasets.\n        max_image_area_percentage (float): The maximum percentage of\n            detection area relative to\n            the image area for a detection to be included.\n            Argument is used only for segmentation datasets.\n        approximation_percentage (float): The percentage of polygon points\n            to be removed from the input polygon,\n            in the range [0, 1). This is useful for simplifying the annotations.\n            Argument is used only for segmentation datasets.\n    \"\"\"\n    if images_directory_path is not None:\n        save_dataset_images(\n            dataset=self, images_directory_path=images_directory_path\n        )\n    if annotations_path is not None:\n        save_coco_annotations(\n            dataset=self,\n            annotation_path=annotations_path,\n            min_image_area_percentage=min_image_area_percentage,\n            max_image_area_percentage=max_image_area_percentage,\n            approximation_percentage=approximation_percentage,\n        )\n</code></pre>"},{"location":"datasets/core/#supervision.dataset.core.DetectionDataset.as_pascal_voc","title":"<code>as_pascal_voc(images_directory_path=None, annotations_directory_path=None, min_image_area_percentage=0.0, max_image_area_percentage=1.0, approximation_percentage=0.0)</code>","text":"<p>Exports the dataset to PASCAL VOC format. This method saves the images and their corresponding annotations in PASCAL VOC format.</p> <p>Parameters:</p> Name Type Description Default <code>images_directory_path</code> <code>Optional[str]</code> <p>The path to the directory where the images should be saved. If not provided, images will not be saved.</p> <code>None</code> <code>annotations_directory_path</code> <code>Optional[str]</code> <p>The path to the directory where the annotations in PASCAL VOC format should be saved. If not provided, annotations will not be saved.</p> <code>None</code> <code>min_image_area_percentage</code> <code>float</code> <p>The minimum percentage of detection area relative to the image area for a detection to be included. Argument is used only for segmentation datasets.</p> <code>0.0</code> <code>max_image_area_percentage</code> <code>float</code> <p>The maximum percentage of detection area relative to the image area for a detection to be included. Argument is used only for segmentation datasets.</p> <code>1.0</code> <code>approximation_percentage</code> <code>float</code> <p>The percentage of polygon points to be removed from the input polygon, in the range [0, 1). Argument is used only for segmentation datasets.</p> <code>0.0</code> Source code in <code>supervision/dataset/core.py</code> <pre><code>def as_pascal_voc(\n    self,\n    images_directory_path: Optional[str] = None,\n    annotations_directory_path: Optional[str] = None,\n    min_image_area_percentage: float = 0.0,\n    max_image_area_percentage: float = 1.0,\n    approximation_percentage: float = 0.0,\n) -&gt; None:\n    \"\"\"\n    Exports the dataset to PASCAL VOC format. This method saves the images\n    and their corresponding annotations in PASCAL VOC format.\n\n    Args:\n        images_directory_path (Optional[str]): The path to the directory\n            where the images should be saved.\n            If not provided, images will not be saved.\n        annotations_directory_path (Optional[str]): The path to\n            the directory where the annotations in PASCAL VOC format should be\n            saved. If not provided, annotations will not be saved.\n        min_image_area_percentage (float): The minimum percentage of\n            detection area relative to\n            the image area for a detection to be included.\n            Argument is used only for segmentation datasets.\n        max_image_area_percentage (float): The maximum percentage\n            of detection area relative to\n            the image area for a detection to be included.\n            Argument is used only for segmentation datasets.\n        approximation_percentage (float): The percentage of\n            polygon points to be removed from the input polygon,\n            in the range [0, 1). Argument is used only for segmentation datasets.\n    \"\"\"\n    if images_directory_path:\n        save_dataset_images(\n            dataset=self,\n            images_directory_path=images_directory_path,\n        )\n    if annotations_directory_path:\n        Path(annotations_directory_path).mkdir(parents=True, exist_ok=True)\n        for image_path, image, annotations in self:\n            annotation_name = Path(image_path).stem\n            annotations_path = os.path.join(\n                annotations_directory_path, f\"{annotation_name}.xml\"\n            )\n            image_name = Path(image_path).name\n            pascal_voc_xml = detections_to_pascal_voc(\n                detections=annotations,\n                classes=self.classes,\n                filename=image_name,\n                image_shape=image.shape,  # type: ignore\n                min_image_area_percentage=min_image_area_percentage,\n                max_image_area_percentage=max_image_area_percentage,\n                approximation_percentage=approximation_percentage,\n            )\n\n            with open(annotations_path, \"w\") as f:\n                f.write(pascal_voc_xml)\n</code></pre>"},{"location":"datasets/core/#supervision.dataset.core.DetectionDataset.as_yolo","title":"<code>as_yolo(images_directory_path=None, annotations_directory_path=None, data_yaml_path=None, min_image_area_percentage=0.0, max_image_area_percentage=1.0, approximation_percentage=0.0)</code>","text":"<p>Exports the dataset to YOLO format. This method saves the images and their corresponding annotations in YOLO format.</p> <p>Parameters:</p> Name Type Description Default <code>images_directory_path</code> <code>Optional[str]</code> <p>The path to the directory where the images should be saved. If not provided, images will not be saved.</p> <code>None</code> <code>annotations_directory_path</code> <code>Optional[str]</code> <p>The path to the directory where the annotations in YOLO format should be saved. If not provided, annotations will not be saved.</p> <code>None</code> <code>data_yaml_path</code> <code>Optional[str]</code> <p>The path where the data.yaml file should be saved. If not provided, the file will not be saved.</p> <code>None</code> <code>min_image_area_percentage</code> <code>float</code> <p>The minimum percentage of detection area relative to the image area for a detection to be included. Argument is used only for segmentation datasets.</p> <code>0.0</code> <code>max_image_area_percentage</code> <code>float</code> <p>The maximum percentage of detection area relative to the image area for a detection to be included. Argument is used only for segmentation datasets.</p> <code>1.0</code> <code>approximation_percentage</code> <code>float</code> <p>The percentage of polygon points to be removed from the input polygon, in the range [0, 1). This is useful for simplifying the annotations. Argument is used only for segmentation datasets.</p> <code>0.0</code> Source code in <code>supervision/dataset/core.py</code> <pre><code>def as_yolo(\n    self,\n    images_directory_path: Optional[str] = None,\n    annotations_directory_path: Optional[str] = None,\n    data_yaml_path: Optional[str] = None,\n    min_image_area_percentage: float = 0.0,\n    max_image_area_percentage: float = 1.0,\n    approximation_percentage: float = 0.0,\n) -&gt; None:\n    \"\"\"\n    Exports the dataset to YOLO format. This method saves the\n    images and their corresponding annotations in YOLO format.\n\n    Args:\n        images_directory_path (Optional[str]): The path to the\n            directory where the images should be saved.\n            If not provided, images will not be saved.\n        annotations_directory_path (Optional[str]): The path to the\n            directory where the annotations in\n            YOLO format should be saved. If not provided,\n            annotations will not be saved.\n        data_yaml_path (Optional[str]): The path where the data.yaml\n            file should be saved.\n            If not provided, the file will not be saved.\n        min_image_area_percentage (float): The minimum percentage of\n            detection area relative to\n            the image area for a detection to be included.\n            Argument is used only for segmentation datasets.\n        max_image_area_percentage (float): The maximum percentage\n            of detection area relative to\n            the image area for a detection to be included.\n            Argument is used only for segmentation datasets.\n        approximation_percentage (float): The percentage of polygon points to\n            be removed from the input polygon, in the range [0, 1).\n            This is useful for simplifying the annotations.\n            Argument is used only for segmentation datasets.\n    \"\"\"\n    if images_directory_path is not None:\n        save_dataset_images(\n            dataset=self, images_directory_path=images_directory_path\n        )\n    if annotations_directory_path is not None:\n        save_yolo_annotations(\n            dataset=self,\n            annotations_directory_path=annotations_directory_path,\n            min_image_area_percentage=min_image_area_percentage,\n            max_image_area_percentage=max_image_area_percentage,\n            approximation_percentage=approximation_percentage,\n        )\n    if data_yaml_path is not None:\n        save_data_yaml(data_yaml_path=data_yaml_path, classes=self.classes)\n</code></pre>"},{"location":"datasets/core/#supervision.dataset.core.DetectionDataset.from_coco","title":"<code>from_coco(images_directory_path, annotations_path, force_masks=False)</code>  <code>classmethod</code>","text":"<p>Creates a Dataset instance from COCO formatted data.</p> <p>Parameters:</p> Name Type Description Default <code>images_directory_path</code> <code>str</code> <p>The path to the directory containing the images.</p> required <code>annotations_path</code> <code>str</code> <p>The path to the json annotation files.</p> required <code>force_masks</code> <code>bool</code> <p>If True, forces masks to be loaded for all annotations, regardless of whether they are present.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>DetectionDataset</code> <code>DetectionDataset</code> <p>A DetectionDataset instance containing the loaded images and annotations.</p> <p>Examples:</p> <pre><code>import roboflow\nfrom roboflow import Roboflow\nimport supervision as sv\n\nroboflow.login()\nrf = Roboflow()\n\nproject = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\ndataset = project.version(PROJECT_VERSION).download(\"coco\")\n\nds = sv.DetectionDataset.from_coco(\n    images_directory_path=f\"{dataset.location}/train\",\n    annotations_path=f\"{dataset.location}/train/_annotations.coco.json\",\n)\n\nds.classes\n# ['dog', 'person']\n</code></pre> Source code in <code>supervision/dataset/core.py</code> <pre><code>@classmethod\ndef from_coco(\n    cls,\n    images_directory_path: str,\n    annotations_path: str,\n    force_masks: bool = False,\n) -&gt; DetectionDataset:\n    \"\"\"\n    Creates a Dataset instance from COCO formatted data.\n\n    Args:\n        images_directory_path (str): The path to the\n            directory containing the images.\n        annotations_path (str): The path to the json annotation files.\n        force_masks (bool, optional): If True,\n            forces masks to be loaded for all annotations,\n            regardless of whether they are present.\n\n    Returns:\n        DetectionDataset: A DetectionDataset instance containing\n            the loaded images and annotations.\n\n    Examples:\n        ```python\n        import roboflow\n        from roboflow import Roboflow\n        import supervision as sv\n\n        roboflow.login()\n        rf = Roboflow()\n\n        project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n        dataset = project.version(PROJECT_VERSION).download(\"coco\")\n\n        ds = sv.DetectionDataset.from_coco(\n            images_directory_path=f\"{dataset.location}/train\",\n            annotations_path=f\"{dataset.location}/train/_annotations.coco.json\",\n        )\n\n        ds.classes\n        # ['dog', 'person']\n        ```\n    \"\"\"\n    classes, images, annotations = load_coco_annotations(\n        images_directory_path=images_directory_path,\n        annotations_path=annotations_path,\n        force_masks=force_masks,\n    )\n    return DetectionDataset(classes=classes, images=images, annotations=annotations)\n</code></pre>"},{"location":"datasets/core/#supervision.dataset.core.DetectionDataset.from_pascal_voc","title":"<code>from_pascal_voc(images_directory_path, annotations_directory_path, force_masks=False)</code>  <code>classmethod</code>","text":"<p>Creates a Dataset instance from PASCAL VOC formatted data.</p> <p>Parameters:</p> Name Type Description Default <code>images_directory_path</code> <code>str</code> <p>Path to the directory containing the images.</p> required <code>annotations_directory_path</code> <code>str</code> <p>Path to the directory containing the PASCAL VOC XML annotations.</p> required <code>force_masks</code> <code>bool</code> <p>If True, forces masks to be loaded for all annotations, regardless of whether they are present.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>DetectionDataset</code> <code>DetectionDataset</code> <p>A DetectionDataset instance containing the loaded images and annotations.</p> <p>Examples:</p> <pre><code>import roboflow\nfrom roboflow import Roboflow\nimport supervision as sv\n\nroboflow.login()\n\nrf = Roboflow()\n\nproject = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\ndataset = project.version(PROJECT_VERSION).download(\"voc\")\n\nds = sv.DetectionDataset.from_pascal_voc(\n    images_directory_path=f\"{dataset.location}/train/images\",\n    annotations_directory_path=f\"{dataset.location}/train/labels\"\n)\n\nds.classes\n# ['dog', 'person']\n</code></pre> Source code in <code>supervision/dataset/core.py</code> <pre><code>@classmethod\ndef from_pascal_voc(\n    cls,\n    images_directory_path: str,\n    annotations_directory_path: str,\n    force_masks: bool = False,\n) -&gt; DetectionDataset:\n    \"\"\"\n    Creates a Dataset instance from PASCAL VOC formatted data.\n\n    Args:\n        images_directory_path (str): Path to the directory containing the images.\n        annotations_directory_path (str): Path to the directory\n            containing the PASCAL VOC XML annotations.\n        force_masks (bool, optional): If True, forces masks to\n            be loaded for all annotations, regardless of whether they are present.\n\n    Returns:\n        DetectionDataset: A DetectionDataset instance containing\n            the loaded images and annotations.\n\n    Examples:\n        ```python\n        import roboflow\n        from roboflow import Roboflow\n        import supervision as sv\n\n        roboflow.login()\n\n        rf = Roboflow()\n\n        project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n        dataset = project.version(PROJECT_VERSION).download(\"voc\")\n\n        ds = sv.DetectionDataset.from_pascal_voc(\n            images_directory_path=f\"{dataset.location}/train/images\",\n            annotations_directory_path=f\"{dataset.location}/train/labels\"\n        )\n\n        ds.classes\n        # ['dog', 'person']\n        ```\n    \"\"\"\n\n    classes, image_paths, annotations = load_pascal_voc_annotations(\n        images_directory_path=images_directory_path,\n        annotations_directory_path=annotations_directory_path,\n        force_masks=force_masks,\n    )\n\n    return DetectionDataset(\n        classes=classes, images=image_paths, annotations=annotations\n    )\n</code></pre>"},{"location":"datasets/core/#supervision.dataset.core.DetectionDataset.from_yolo","title":"<code>from_yolo(images_directory_path, annotations_directory_path, data_yaml_path, force_masks=False, is_obb=False)</code>  <code>classmethod</code>","text":"<p>Creates a Dataset instance from YOLO formatted data.</p> <p>Parameters:</p> Name Type Description Default <code>images_directory_path</code> <code>str</code> <p>The path to the directory containing the images.</p> required <code>annotations_directory_path</code> <code>str</code> <p>The path to the directory containing the YOLO annotation files.</p> required <code>data_yaml_path</code> <code>str</code> <p>The path to the data YAML file containing class information.</p> required <code>force_masks</code> <code>bool</code> <p>If True, forces masks to be loaded for all annotations, regardless of whether they are present.</p> <code>False</code> <code>is_obb</code> <code>bool</code> <p>If True, loads the annotations in OBB format. OBB annotations are defined as <code>[class_id, x, y, x, y, x, y, x, y]</code>, where pairs of [x, y] are box corners.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>DetectionDataset</code> <code>DetectionDataset</code> <p>A DetectionDataset instance containing the loaded images and annotations.</p> <p>Examples:</p> <pre><code>import roboflow\nfrom roboflow import Roboflow\nimport supervision as sv\n\nroboflow.login()\nrf = Roboflow()\n\nproject = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\ndataset = project.version(PROJECT_VERSION).download(\"yolov5\")\n\nds = sv.DetectionDataset.from_yolo(\n    images_directory_path=f\"{dataset.location}/train/images\",\n    annotations_directory_path=f\"{dataset.location}/train/labels\",\n    data_yaml_path=f\"{dataset.location}/data.yaml\"\n)\n\nds.classes\n# ['dog', 'person']\n</code></pre> Source code in <code>supervision/dataset/core.py</code> <pre><code>@classmethod\ndef from_yolo(\n    cls,\n    images_directory_path: str,\n    annotations_directory_path: str,\n    data_yaml_path: str,\n    force_masks: bool = False,\n    is_obb: bool = False,\n) -&gt; DetectionDataset:\n    \"\"\"\n    Creates a Dataset instance from YOLO formatted data.\n\n    Args:\n        images_directory_path (str): The path to the\n            directory containing the images.\n        annotations_directory_path (str): The path to the directory\n            containing the YOLO annotation files.\n        data_yaml_path (str): The path to the data\n            YAML file containing class information.\n        force_masks (bool, optional): If True, forces\n            masks to be loaded for all annotations,\n            regardless of whether they are present.\n        is_obb (bool, optional): If True, loads the annotations in OBB format.\n            OBB annotations are defined as `[class_id, x, y, x, y, x, y, x, y]`,\n            where pairs of [x, y] are box corners.\n\n    Returns:\n        DetectionDataset: A DetectionDataset instance\n            containing the loaded images and annotations.\n\n    Examples:\n        ```python\n        import roboflow\n        from roboflow import Roboflow\n        import supervision as sv\n\n        roboflow.login()\n        rf = Roboflow()\n\n        project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n        dataset = project.version(PROJECT_VERSION).download(\"yolov5\")\n\n        ds = sv.DetectionDataset.from_yolo(\n            images_directory_path=f\"{dataset.location}/train/images\",\n            annotations_directory_path=f\"{dataset.location}/train/labels\",\n            data_yaml_path=f\"{dataset.location}/data.yaml\"\n        )\n\n        ds.classes\n        # ['dog', 'person']\n        ```\n    \"\"\"\n    classes, image_paths, annotations = load_yolo_annotations(\n        images_directory_path=images_directory_path,\n        annotations_directory_path=annotations_directory_path,\n        data_yaml_path=data_yaml_path,\n        force_masks=force_masks,\n        is_obb=is_obb,\n    )\n    return DetectionDataset(\n        classes=classes, images=image_paths, annotations=annotations\n    )\n</code></pre>"},{"location":"datasets/core/#supervision.dataset.core.DetectionDataset.merge","title":"<code>merge(dataset_list)</code>  <code>classmethod</code>","text":"<p>Merge a list of <code>DetectionDataset</code> objects into a single     <code>DetectionDataset</code> object.</p> <p>This method takes a list of <code>DetectionDataset</code> objects and combines their respective fields (<code>classes</code>, <code>images</code>, <code>annotations</code>) into a single <code>DetectionDataset</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_list</code> <code>List[DetectionDataset]</code> <p>A list of <code>DetectionDataset</code> objects to merge.</p> required <p>Returns:</p> Type Description <code>DetectionDataset</code> <p>A single <code>DetectionDataset</code> object containing</p> <code>DetectionDataset</code> <p>the merged data from the input list.</p> <p>Examples:</p> <pre><code>import supervision as sv\n\nds_1 = sv.DetectionDataset(...)\nlen(ds_1)\n# 100\nds_1.classes\n# ['dog', 'person']\n\nds_2 = sv.DetectionDataset(...)\nlen(ds_2)\n# 200\nds_2.classes\n# ['cat']\n\nds_merged = sv.DetectionDataset.merge([ds_1, ds_2])\nlen(ds_merged)\n# 300\nds_merged.classes\n# ['cat', 'dog', 'person']\n</code></pre> Source code in <code>supervision/dataset/core.py</code> <pre><code>@classmethod\ndef merge(cls, dataset_list: List[DetectionDataset]) -&gt; DetectionDataset:\n    \"\"\"\n    Merge a list of `DetectionDataset` objects into a single\n        `DetectionDataset` object.\n\n    This method takes a list of `DetectionDataset` objects and combines\n    their respective fields (`classes`, `images`,\n    `annotations`) into a single `DetectionDataset` object.\n\n    Args:\n        dataset_list (List[DetectionDataset]): A list of `DetectionDataset`\n            objects to merge.\n\n    Returns:\n        (DetectionDataset): A single `DetectionDataset` object containing\n        the merged data from the input list.\n\n    Examples:\n        ```python\n        import supervision as sv\n\n        ds_1 = sv.DetectionDataset(...)\n        len(ds_1)\n        # 100\n        ds_1.classes\n        # ['dog', 'person']\n\n        ds_2 = sv.DetectionDataset(...)\n        len(ds_2)\n        # 200\n        ds_2.classes\n        # ['cat']\n\n        ds_merged = sv.DetectionDataset.merge([ds_1, ds_2])\n        len(ds_merged)\n        # 300\n        ds_merged.classes\n        # ['cat', 'dog', 'person']\n        ```\n    \"\"\"\n\n    def is_in_memory(dataset: DetectionDataset) -&gt; bool:\n        return len(dataset._images_in_memory) &gt; 0 or len(dataset.image_paths) == 0\n\n    def is_lazy(dataset: DetectionDataset) -&gt; bool:\n        return len(dataset._images_in_memory) == 0\n\n    all_in_memory = all([is_in_memory(dataset) for dataset in dataset_list])\n    all_lazy = all([is_lazy(dataset) for dataset in dataset_list])\n    if not all_in_memory and not all_lazy:\n        raise ValueError(\n            \"Merging lazy and in-memory DetectionDatasets is not supported.\"\n        )\n\n    images_in_memory = {}\n    for dataset in dataset_list:\n        images_in_memory.update(dataset._images_in_memory)\n\n    image_paths = list(\n        chain.from_iterable(dataset.image_paths for dataset in dataset_list)\n    )\n    image_paths_unique = list(dict.fromkeys(image_paths))\n    if len(image_paths) != len(image_paths_unique):\n        duplicates = find_duplicates(image_paths)\n        raise ValueError(\n            f\"Image paths {duplicates} are not unique across datasets.\"\n        )\n    image_paths = image_paths_unique\n\n    classes = merge_class_lists(\n        class_lists=[dataset.classes for dataset in dataset_list]\n    )\n\n    annotations = {}\n    for dataset in dataset_list:\n        annotations.update(dataset.annotations)\n    for dataset in dataset_list:\n        class_index_mapping = build_class_index_mapping(\n            source_classes=dataset.classes, target_classes=classes\n        )\n        for image_path in dataset.image_paths:\n            annotations[image_path] = map_detections_class_id(\n                source_to_target_mapping=class_index_mapping,\n                detections=annotations[image_path],\n            )\n\n    return cls(\n        classes=classes,\n        images=images_in_memory or image_paths,\n        annotations=annotations,\n    )\n</code></pre>"},{"location":"datasets/core/#supervision.dataset.core.DetectionDataset.split","title":"<code>split(split_ratio=0.8, random_state=None, shuffle=True)</code>","text":"<p>Splits the dataset into two parts (training and testing)     using the provided split_ratio.</p> <p>Parameters:</p> Name Type Description Default <code>split_ratio</code> <code>float</code> <p>The ratio of the training set to the entire dataset.</p> <code>0.8</code> <code>random_state</code> <code>int</code> <p>The seed for the random number generator. This is used for reproducibility.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data before splitting.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[DetectionDataset, DetectionDataset]</code> <p>Tuple[DetectionDataset, DetectionDataset]: A tuple containing the training and testing datasets.</p> <p>Examples:</p> <pre><code>import supervision as sv\n\nds = sv.DetectionDataset(...)\ntrain_ds, test_ds = ds.split(split_ratio=0.7, random_state=42, shuffle=True)\nlen(train_ds), len(test_ds)\n# (700, 300)\n</code></pre> Source code in <code>supervision/dataset/core.py</code> <pre><code>def split(\n    self, split_ratio=0.8, random_state=None, shuffle: bool = True\n) -&gt; Tuple[DetectionDataset, DetectionDataset]:\n    \"\"\"\n    Splits the dataset into two parts (training and testing)\n        using the provided split_ratio.\n\n    Args:\n        split_ratio (float, optional): The ratio of the training\n            set to the entire dataset.\n        random_state (int, optional): The seed for the random number generator.\n            This is used for reproducibility.\n        shuffle (bool, optional): Whether to shuffle the data before splitting.\n\n    Returns:\n        Tuple[DetectionDataset, DetectionDataset]: A tuple containing\n            the training and testing datasets.\n\n    Examples:\n        ```python\n        import supervision as sv\n\n        ds = sv.DetectionDataset(...)\n        train_ds, test_ds = ds.split(split_ratio=0.7, random_state=42, shuffle=True)\n        len(train_ds), len(test_ds)\n        # (700, 300)\n        ```\n    \"\"\"\n\n    train_paths, test_paths = train_test_split(\n        data=self.image_paths,\n        train_ratio=split_ratio,\n        random_state=random_state,\n        shuffle=shuffle,\n    )\n\n    train_input: Union[List[str], Dict[str, np.ndarray]]\n    test_input: Union[List[str], Dict[str, np.ndarray]]\n    if self._images_in_memory:\n        train_input = {path: self._images_in_memory[path] for path in train_paths}\n        test_input = {path: self._images_in_memory[path] for path in test_paths}\n    else:\n        train_input = train_paths\n        test_input = test_paths\n    train_annotations = {path: self.annotations[path] for path in train_paths}\n    test_annotations = {path: self.annotations[path] for path in test_paths}\n\n    train_dataset = DetectionDataset(\n        classes=self.classes,\n        images=train_input,\n        annotations=train_annotations,\n    )\n    test_dataset = DetectionDataset(\n        classes=self.classes,\n        images=test_input,\n        annotations=test_annotations,\n    )\n    return train_dataset, test_dataset\n</code></pre>"},{"location":"datasets/core/#supervision.dataset.core.ClassificationDataset-attributes","title":"Attributes","text":""},{"location":"datasets/core/#supervision.dataset.core.ClassificationDataset.images","title":"<code>images: Dict[str, np.ndarray]</code>  <code>property</code>","text":"<p>Load all images to memory and return them as a dictionary.</p> <p>Warning</p> <p>Only use this when you need all images at once. It is much more memory-efficient to initialize dataset with image paths and use <code>for path, image, annotation in dataset:</code>.</p>"},{"location":"datasets/core/#supervision.dataset.core.ClassificationDataset-functions","title":"Functions","text":""},{"location":"datasets/core/#supervision.dataset.core.ClassificationDataset.__getitem__","title":"<code>__getitem__(i)</code>","text":"<p>Returns:</p> Type Description <code>Tuple[str, ndarray, Classifications]</code> <p>Tuple[str, np.ndarray, Classifications]: The image path, image data, and its corresponding annotation at index i.</p> Source code in <code>supervision/dataset/core.py</code> <pre><code>def __getitem__(self, i: int) -&gt; Tuple[str, np.ndarray, Classifications]:\n    \"\"\"\n    Returns:\n        Tuple[str, np.ndarray, Classifications]: The image path, image data,\n            and its corresponding annotation at index i.\n    \"\"\"\n    image_path = self.image_paths[i]\n    image = self._get_image(image_path)\n    annotation = self.annotations[image_path]\n    return image_path, image, annotation\n</code></pre>"},{"location":"datasets/core/#supervision.dataset.core.ClassificationDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over the images and annotations in the dataset.</p> <p>Yields:</p> Type Description <code>str</code> <p>Iterator[Tuple[str, np.ndarray, Detections]]: An iterator that yields tuples containing the image path, the image data, and its corresponding annotation.</p> Source code in <code>supervision/dataset/core.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Tuple[str, np.ndarray, Classifications]]:\n    \"\"\"\n    Iterate over the images and annotations in the dataset.\n\n    Yields:\n        Iterator[Tuple[str, np.ndarray, Detections]]:\n            An iterator that yields tuples containing the image path,\n            the image data, and its corresponding annotation.\n    \"\"\"\n    for i in range(len(self)):\n        image_path, image, annotation = self[i]\n        yield image_path, image, annotation\n</code></pre>"},{"location":"datasets/core/#supervision.dataset.core.ClassificationDataset.as_folder_structure","title":"<code>as_folder_structure(root_directory_path)</code>","text":"<p>Saves the dataset as a multi-class folder structure.</p> <p>Parameters:</p> Name Type Description Default <code>root_directory_path</code> <code>str</code> <p>The path to the directory where the dataset will be saved.</p> required Source code in <code>supervision/dataset/core.py</code> <pre><code>def as_folder_structure(self, root_directory_path: str) -&gt; None:\n    \"\"\"\n    Saves the dataset as a multi-class folder structure.\n\n    Args:\n        root_directory_path (str): The path to the directory\n            where the dataset will be saved.\n    \"\"\"\n    os.makedirs(root_directory_path, exist_ok=True)\n\n    for class_name in self.classes:\n        os.makedirs(os.path.join(root_directory_path, class_name), exist_ok=True)\n\n    for image_save_path, image, annotation in self:\n        image_name = Path(image_save_path).name\n        class_id = (\n            annotation.class_id[0]\n            if annotation.confidence is None\n            else annotation.get_top_k(1)[0][0]\n        )\n        class_name = self.classes[class_id]\n        image_save_path = os.path.join(root_directory_path, class_name, image_name)\n        cv2.imwrite(image_save_path, image)\n</code></pre>"},{"location":"datasets/core/#supervision.dataset.core.ClassificationDataset.from_folder_structure","title":"<code>from_folder_structure(root_directory_path)</code>  <code>classmethod</code>","text":"<p>Load data from a multiclass folder structure into a ClassificationDataset.</p> <p>Parameters:</p> Name Type Description Default <code>root_directory_path</code> <code>str</code> <p>The path to the dataset directory.</p> required <p>Returns:</p> Name Type Description <code>ClassificationDataset</code> <code>ClassificationDataset</code> <p>The dataset.</p> <p>Examples:</p> <pre><code>import roboflow\nfrom roboflow import Roboflow\nimport supervision as sv\n\nroboflow.login()\nrf = Roboflow()\n\nproject = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\ndataset = project.version(PROJECT_VERSION).download(\"folder\")\n\ncd = sv.ClassificationDataset.from_folder_structure(\n    root_directory_path=f\"{dataset.location}/train\"\n)\n</code></pre> Source code in <code>supervision/dataset/core.py</code> <pre><code>@classmethod\ndef from_folder_structure(cls, root_directory_path: str) -&gt; ClassificationDataset:\n    \"\"\"\n    Load data from a multiclass folder structure into a ClassificationDataset.\n\n    Args:\n        root_directory_path (str): The path to the dataset directory.\n\n    Returns:\n        ClassificationDataset: The dataset.\n\n    Examples:\n        ```python\n        import roboflow\n        from roboflow import Roboflow\n        import supervision as sv\n\n        roboflow.login()\n        rf = Roboflow()\n\n        project = rf.workspace(WORKSPACE_ID).project(PROJECT_ID)\n        dataset = project.version(PROJECT_VERSION).download(\"folder\")\n\n        cd = sv.ClassificationDataset.from_folder_structure(\n            root_directory_path=f\"{dataset.location}/train\"\n        )\n        ```\n    \"\"\"\n    classes = os.listdir(root_directory_path)\n    classes = sorted(set(classes))\n\n    image_paths = []\n    annotations = {}\n\n    for class_name in classes:\n        class_id = classes.index(class_name)\n\n        for image in os.listdir(os.path.join(root_directory_path, class_name)):\n            image_path = str(os.path.join(root_directory_path, class_name, image))\n            image_paths.append(image_path)\n            annotations[image_path] = Classifications(\n                class_id=np.array([class_id]),\n            )\n\n    return cls(\n        classes=classes,\n        images=image_paths,\n        annotations=annotations,\n    )\n</code></pre>"},{"location":"datasets/core/#supervision.dataset.core.ClassificationDataset.split","title":"<code>split(split_ratio=0.8, random_state=None, shuffle=True)</code>","text":"<p>Splits the dataset into two parts (training and testing)     using the provided split_ratio.</p> <p>Parameters:</p> Name Type Description Default <code>split_ratio</code> <code>float</code> <p>The ratio of the training set to the entire dataset.</p> <code>0.8</code> <code>random_state</code> <code>int</code> <p>The seed for the random number generator. This is used for reproducibility.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data before splitting.</p> <code>True</code> <p>Returns:</p> Type Description <code>ClassificationDataset</code> <p>Tuple[ClassificationDataset, ClassificationDataset]: A tuple containing</p> <code>ClassificationDataset</code> <p>the training and testing datasets.</p> <p>Examples:</p> <pre><code>import supervision as sv\n\ncd = sv.ClassificationDataset(...)\ntrain_cd,test_cd = cd.split(split_ratio=0.7, random_state=42,shuffle=True)\nlen(train_cd), len(test_cd)\n# (700, 300)\n</code></pre> Source code in <code>supervision/dataset/core.py</code> <pre><code>def split(\n    self, split_ratio=0.8, random_state=None, shuffle: bool = True\n) -&gt; Tuple[ClassificationDataset, ClassificationDataset]:\n    \"\"\"\n    Splits the dataset into two parts (training and testing)\n        using the provided split_ratio.\n\n    Args:\n        split_ratio (float, optional): The ratio of the training\n            set to the entire dataset.\n        random_state (int, optional): The seed for the\n            random number generator. This is used for reproducibility.\n        shuffle (bool, optional): Whether to shuffle the data before splitting.\n\n    Returns:\n        Tuple[ClassificationDataset, ClassificationDataset]: A tuple containing\n        the training and testing datasets.\n\n    Examples:\n        ```python\n        import supervision as sv\n\n        cd = sv.ClassificationDataset(...)\n        train_cd,test_cd = cd.split(split_ratio=0.7, random_state=42,shuffle=True)\n        len(train_cd), len(test_cd)\n        # (700, 300)\n        ```\n    \"\"\"\n    train_paths, test_paths = train_test_split(\n        data=self.image_paths,\n        train_ratio=split_ratio,\n        random_state=random_state,\n        shuffle=shuffle,\n    )\n\n    train_input: Union[List[str], Dict[str, np.ndarray]]\n    test_input: Union[List[str], Dict[str, np.ndarray]]\n    if self._images_in_memory:\n        train_input = {path: self._images_in_memory[path] for path in train_paths}\n        test_input = {path: self._images_in_memory[path] for path in test_paths}\n    else:\n        train_input = train_paths\n        test_input = test_paths\n    train_annotations = {path: self.annotations[path] for path in train_paths}\n    test_annotations = {path: self.annotations[path] for path in test_paths}\n\n    train_dataset = ClassificationDataset(\n        classes=self.classes,\n        images=train_input,\n        annotations=train_annotations,\n    )\n    test_dataset = ClassificationDataset(\n        classes=self.classes,\n        images=test_input,\n        annotations=test_annotations,\n    )\n\n    return train_dataset, test_dataset\n</code></pre>"},{"location":"datasets/utils/","title":"Datasets Utils","text":"rle_to_mask <p>Converts run-length encoding (RLE) to a binary mask.</p> <p>Parameters:</p> Name Type Description Default <code>rle</code> <code>Union[NDArray[int_], List[int]]</code> <p>The 1D RLE array, the format used in the COCO dataset (column-wise encoding, values of an array with even indices represent the number of pixels assigned as background, values of an array with odd indices represent the number of pixels assigned as foreground object).</p> required <code>resolution_wh</code> <code>Tuple[int, int]</code> <p>The width (w) and height (h) of the desired binary mask.</p> required <p>Returns:</p> Type Description <code>NDArray[bool_]</code> <p>The generated 2D Boolean mask of shape <code>(h, w)</code>, where the foreground object is marked with <code>True</code>'s and the rest is filled with <code>False</code>'s.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the sum of pixels encoded in RLE differs from the number of pixels in the expected mask (computed based on resolution_wh).</p> <p>Examples:</p> <pre><code>import supervision as sv\n\nsv.rle_to_mask([5, 2, 2, 2, 5], (4, 4))\n# array([\n#     [False, False, False, False],\n#     [False, True,  True,  False],\n#     [False, True,  True,  False],\n#     [False, False, False, False],\n# ])\n</code></pre> Source code in <code>supervision/dataset/utils.py</code> <pre><code>def rle_to_mask(\n    rle: Union[npt.NDArray[np.int_], List[int]], resolution_wh: Tuple[int, int]\n) -&gt; npt.NDArray[np.bool_]:\n    \"\"\"\n    Converts run-length encoding (RLE) to a binary mask.\n\n    Args:\n        rle (Union[npt.NDArray[np.int_], List[int]]): The 1D RLE array, the format\n            used in the COCO dataset (column-wise encoding, values of an array with\n            even indices represent the number of pixels assigned as background,\n            values of an array with odd indices represent the number of pixels\n            assigned as foreground object).\n        resolution_wh (Tuple[int, int]): The width (w) and height (h)\n            of the desired binary mask.\n\n    Returns:\n        The generated 2D Boolean mask of shape `(h, w)`, where the foreground object is\n            marked with `True`'s and the rest is filled with `False`'s.\n\n    Raises:\n        AssertionError: If the sum of pixels encoded in RLE differs from the\n            number of pixels in the expected mask (computed based on resolution_wh).\n\n    Examples:\n        ```python\n        import supervision as sv\n\n        sv.rle_to_mask([5, 2, 2, 2, 5], (4, 4))\n        # array([\n        #     [False, False, False, False],\n        #     [False, True,  True,  False],\n        #     [False, True,  True,  False],\n        #     [False, False, False, False],\n        # ])\n        ```\n    \"\"\"\n    if isinstance(rle, list):\n        rle = np.array(rle, dtype=int)\n\n    width, height = resolution_wh\n\n    assert width * height == np.sum(rle), (\n        \"the sum of the number of pixels in the RLE must be the same \"\n        \"as the number of pixels in the expected mask\"\n    )\n\n    zero_one_values = np.zeros(shape=(rle.size, 1), dtype=np.uint8)\n    zero_one_values[1::2] = 1\n\n    decoded_rle = np.repeat(zero_one_values, rle, axis=0)\n    decoded_rle = np.append(\n        decoded_rle, np.zeros(width * height - len(decoded_rle), dtype=np.uint8)\n    )\n    return decoded_rle.reshape((height, width), order=\"F\")\n</code></pre> mask_to_rle <p>Converts a binary mask into a run-length encoding (RLE).</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>NDArray[bool_]</code> <p>2D binary mask where <code>True</code> indicates foreground object and <code>False</code> indicates background.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>The run-length encoded mask. Values of a list with even indices represent the number of pixels assigned as background (<code>False</code>), values of a list with odd indices represent the number of pixels assigned as foreground object (<code>True</code>).</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If input mask is not 2D or is empty.</p> <p>Examples:</p> <pre><code>import numpy as np\nimport supervision as sv\n\nmask = np.array([\n    [True, True, True, True],\n    [True, True, True, True],\n    [True, True, True, True],\n    [True, True, True, True],\n])\nsv.mask_to_rle(mask)\n# [0, 16]\n\nmask = np.array([\n    [False, False, False, False],\n    [False, True,  True,  False],\n    [False, True,  True,  False],\n    [False, False, False, False],\n])\nsv.mask_to_rle(mask)\n# [5, 2, 2, 2, 5]\n</code></pre> <p></p> Source code in <code>supervision/dataset/utils.py</code> <pre><code>def mask_to_rle(mask: npt.NDArray[np.bool_]) -&gt; List[int]:\n    \"\"\"\n    Converts a binary mask into a run-length encoding (RLE).\n\n    Args:\n        mask (npt.NDArray[np.bool_]): 2D binary mask where `True` indicates foreground\n            object and `False` indicates background.\n\n    Returns:\n        The run-length encoded mask. Values of a list with even indices\n            represent the number of pixels assigned as background (`False`), values\n            of a list with odd indices represent the number of pixels assigned\n            as foreground object (`True`).\n\n    Raises:\n        AssertionError: If input mask is not 2D or is empty.\n\n    Examples:\n        ```python\n        import numpy as np\n        import supervision as sv\n\n        mask = np.array([\n            [True, True, True, True],\n            [True, True, True, True],\n            [True, True, True, True],\n            [True, True, True, True],\n        ])\n        sv.mask_to_rle(mask)\n        # [0, 16]\n\n        mask = np.array([\n            [False, False, False, False],\n            [False, True,  True,  False],\n            [False, True,  True,  False],\n            [False, False, False, False],\n        ])\n        sv.mask_to_rle(mask)\n        # [5, 2, 2, 2, 5]\n        ```\n\n    ![mask_to_rle](https://media.roboflow.com/supervision-docs/mask-to-rle.png){ align=center width=\"800\" }\n    \"\"\"  # noqa E501 // docs\n    assert mask.ndim == 2, \"Input mask must be 2D\"\n    assert mask.size != 0, \"Input mask cannot be empty\"\n\n    on_value_change_indices = np.where(\n        mask.ravel(order=\"F\") != np.roll(mask.ravel(order=\"F\"), 1)\n    )[0]\n\n    on_value_change_indices = np.append(on_value_change_indices, mask.size)\n    # need to add 0 at the beginning when the same value is in the first and\n    # last element of the flattened mask\n    if on_value_change_indices[0] != 0:\n        on_value_change_indices = np.insert(on_value_change_indices, 0, 0)\n\n    rle = np.diff(on_value_change_indices)\n\n    if mask[0][0] == 1:\n        rle = np.insert(rle, 0, 0)\n\n    return list(rle)\n</code></pre>"},{"location":"detection/annotators/","title":"Annotators","text":"BoxRoundBoxBoxCornerColorCircleDotTriangleEllipseHaloPercentageBarMaskPolygonLabelRichLabelCropBlurPixelateTraceHeatMapBackground Color <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nbox_annotator = sv.BoxAnnotator()\nannotated_frame = box_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nround_box_annotator = sv.RoundBoxAnnotator()\nannotated_frame = round_box_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\ncorner_annotator = sv.BoxCornerAnnotator()\nannotated_frame = corner_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\ncolor_annotator = sv.ColorAnnotator()\nannotated_frame = color_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\ncircle_annotator = sv.CircleAnnotator()\nannotated_frame = circle_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\ndot_annotator = sv.DotAnnotator()\nannotated_frame = dot_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\ntriangle_annotator = sv.TriangleAnnotator()\nannotated_frame = triangle_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nellipse_annotator = sv.EllipseAnnotator()\nannotated_frame = ellipse_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nhalo_annotator = sv.HaloAnnotator()\nannotated_frame = halo_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\npercentage_bar_annotator = sv.PercentageBarAnnotator()\nannotated_frame = percentage_bar_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nmask_annotator = sv.MaskAnnotator()\nannotated_frame = mask_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\npolygon_annotator = sv.PolygonAnnotator()\nannotated_frame = polygon_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nlabels = [\n    f\"{class_name} {confidence:.2f}\"\n    for class_name, confidence\n    in zip(detections['class_name'], detections.confidence)\n]\n\nlabel_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\nannotated_frame = label_annotator.annotate(\n    scene=image.copy(),\n    detections=detections,\n    labels=labels\n)\n</code></pre> <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nlabels = [\n    f\"{class_name} {confidence:.2f}\"\n    for class_name, confidence\n    in zip(detections['class_name'], detections.confidence)\n]\n\nrich_label_annotator = sv.RichLabelAnnotator(\n    font_path=\"&lt;TTF_FONT_PATH&gt;\",\n    text_position=sv.Position.CENTER\n)\nannotated_frame = rich_label_annotator.annotate(\n    scene=image.copy(),\n    detections=detections,\n    labels=labels\n)\n</code></pre> <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\ncrop_annotator = sv.CropAnnotator()\nannotated_frame = crop_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nblur_annotator = sv.BlurAnnotator()\nannotated_frame = blur_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\npixelate_annotator = sv.PixelateAnnotator()\nannotated_frame = pixelate_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <pre><code>import supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8x.pt')\n\ntrace_annotator = sv.TraceAnnotator()\n\nvideo_info = sv.VideoInfo.from_video_path(video_path='...')\nframes_generator = get_video_frames_generator(source_path='...')\ntracker = sv.ByteTrack()\n\nwith sv.VideoSink(target_path='...', video_info=video_info) as sink:\n    for frame in frames_generator:\n        result = model(frame)[0]\n        detections = sv.Detections.from_ultralytics(result)\n        detections = tracker.update_with_detections(detections)\n        annotated_frame = trace_annotator.annotate(\n            scene=frame.copy(),\n            detections=detections)\n        sink.write_frame(frame=annotated_frame)\n</code></pre> <pre><code>import supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8x.pt')\n\nheat_map_annotator = sv.HeatMapAnnotator()\n\nvideo_info = sv.VideoInfo.from_video_path(video_path='...')\nframes_generator = get_video_frames_generator(source_path='...')\n\nwith sv.VideoSink(target_path='...', video_info=video_info) as sink:\n    for frame in frames_generator:\n        result = model(frame)[0]\n        detections = sv.Detections.from_ultralytics(result)\n        annotated_frame = heat_map_annotator.annotate(\n            scene=frame.copy(),\n            detections=detections)\n        sink.write_frame(frame=annotated_frame)\n</code></pre> <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nbackground_overlay_annotator = sv.BackgroundOverlayAnnotator()\nannotated_frame = background_overlay_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> BoxAnnotator <p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing bounding boxes on an image using provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class BoxAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing bounding boxes on an image using provided detections.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n        thickness: int = 2,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            thickness (int): Thickness of the bounding box lines.\n            color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.thickness: int = thickness\n        self.color_lookup: ColorLookup = color_lookup\n\n    @ensure_cv2_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene with bounding boxes based on the provided detections.\n\n        Args:\n            scene (ImageType): The image where bounding boxes will be drawn. `ImageType`\n            is a flexible type, accepting either `numpy.ndarray` or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            detections = sv.Detections(...)\n\n            box_annotator = sv.BoxAnnotator()\n            annotated_frame = box_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n\n        ![bounding-box-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/bounding-box-annotator-example-purple.png)\n        \"\"\"\n        for detection_idx in range(len(detections)):\n            x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            cv2.rectangle(\n                img=scene,\n                pt1=(x1, y1),\n                pt2=(x2, y2),\n                color=color.as_bgr(),\n                thickness=self.thickness,\n            )\n        return scene\n</code></pre> RoundBoxAnnotator <p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing bounding boxes with round edges on an image using provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class RoundBoxAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing bounding boxes with round edges on an image\n    using provided detections.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n        thickness: int = 2,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n        roundness: float = 0.6,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            thickness (int): Thickness of the bounding box lines.\n            color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n            roundness (float): Percent of roundness for edges of bounding box.\n                Value must be float 0 &lt; roundness &lt;= 1.0\n                By default roundness percent is calculated based on smaller side\n                length (width or height).\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.thickness: int = thickness\n        self.color_lookup: ColorLookup = color_lookup\n        if not 0 &lt; roundness &lt;= 1.0:\n            raise ValueError(\"roundness attribute must be float between (0, 1.0]\")\n        self.roundness: float = roundness\n\n    @ensure_cv2_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene with bounding boxes with rounded edges\n        based on the provided detections.\n\n        Args:\n            scene (ImageType): The image where rounded bounding boxes will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            detections = sv.Detections(...)\n\n            round_box_annotator = sv.RoundBoxAnnotator()\n            annotated_frame = round_box_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n\n        ![round-box-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/round-box-annotator-example-purple.png)\n        \"\"\"\n\n        for detection_idx in range(len(detections)):\n            x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n\n            radius = (\n                int((x2 - x1) // 2 * self.roundness)\n                if abs(x1 - x2) &lt; abs(y1 - y2)\n                else int((y2 - y1) // 2 * self.roundness)\n            )\n\n            circle_coordinates = [\n                ((x1 + radius), (y1 + radius)),\n                ((x2 - radius), (y1 + radius)),\n                ((x2 - radius), (y2 - radius)),\n                ((x1 + radius), (y2 - radius)),\n            ]\n\n            line_coordinates = [\n                ((x1 + radius, y1), (x2 - radius, y1)),\n                ((x2, y1 + radius), (x2, y2 - radius)),\n                ((x1 + radius, y2), (x2 - radius, y2)),\n                ((x1, y1 + radius), (x1, y2 - radius)),\n            ]\n\n            start_angles = (180, 270, 0, 90)\n            end_angles = (270, 360, 90, 180)\n\n            for center_coordinates, line, start_angle, end_angle in zip(\n                circle_coordinates, line_coordinates, start_angles, end_angles\n            ):\n                cv2.ellipse(\n                    img=scene,\n                    center=center_coordinates,\n                    axes=(radius, radius),\n                    angle=0,\n                    startAngle=start_angle,\n                    endAngle=end_angle,\n                    color=color.as_bgr(),\n                    thickness=self.thickness,\n                )\n\n                cv2.line(\n                    img=scene,\n                    pt1=line[0],\n                    pt2=line[1],\n                    color=color.as_bgr(),\n                    thickness=self.thickness,\n                )\n\n        return scene\n</code></pre> BoxCornerAnnotator <p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing box corners on an image using provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class BoxCornerAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing box corners on an image using provided detections.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n        thickness: int = 4,\n        corner_length: int = 15,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            thickness (int): Thickness of the corner lines.\n            corner_length (int): Length of each corner line.\n            color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.thickness: int = thickness\n        self.corner_length: int = corner_length\n        self.color_lookup: ColorLookup = color_lookup\n\n    @ensure_cv2_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene with box corners based on the provided detections.\n\n        Args:\n            scene (ImageType): The image where box corners will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            detections = sv.Detections(...)\n\n            corner_annotator = sv.BoxCornerAnnotator()\n            annotated_frame = corner_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n\n        ![box-corner-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/box-corner-annotator-example-purple.png)\n        \"\"\"\n        for detection_idx in range(len(detections)):\n            x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            corners = [(x1, y1), (x2, y1), (x1, y2), (x2, y2)]\n\n            for x, y in corners:\n                x_end = x + self.corner_length if x == x1 else x - self.corner_length\n                cv2.line(\n                    scene, (x, y), (x_end, y), color.as_bgr(), thickness=self.thickness\n                )\n\n                y_end = y + self.corner_length if y == y1 else y - self.corner_length\n                cv2.line(\n                    scene, (x, y), (x, y_end), color.as_bgr(), thickness=self.thickness\n                )\n        return scene\n</code></pre> OrientedBoxAnnotator <p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing oriented bounding boxes on an image using provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class OrientedBoxAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing oriented bounding boxes on an image using provided detections.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n        thickness: int = 2,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            thickness (int): Thickness of the bounding box lines.\n            color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.thickness: int = thickness\n        self.color_lookup: ColorLookup = color_lookup\n\n    @ensure_cv2_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene with oriented bounding boxes based on the provided detections.\n\n        Args:\n            scene (ImageType): The image where bounding boxes will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import cv2\n            import supervision as sv\n            from ultralytics import YOLO\n\n            image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n            model = YOLO(\"yolov8n-obb.pt\")\n\n            result = model(image)[0]\n            detections = sv.Detections.from_ultralytics(result)\n\n            oriented_box_annotator = sv.OrientedBoxAnnotator()\n            annotated_frame = oriented_box_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n        \"\"\"  # noqa E501 // docs\n\n        if detections.data is None or ORIENTED_BOX_COORDINATES not in detections.data:\n            return scene\n\n        for detection_idx in range(len(detections)):\n            bbox = np.intp(detections.data.get(ORIENTED_BOX_COORDINATES)[detection_idx])\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n\n            cv2.drawContours(scene, [bbox], 0, color.as_bgr(), self.thickness)\n\n        return scene\n</code></pre> ColorAnnotator <p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing box masks on an image using provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class ColorAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing box masks on an image using provided detections.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n        opacity: float = 0.5,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            opacity (float): Opacity of the overlay mask. Must be between `0` and `1`.\n            color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.color_lookup: ColorLookup = color_lookup\n        self.opacity = opacity\n\n    @ensure_cv2_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene with box masks based on the provided detections.\n\n        Args:\n            scene (ImageType): The image where bounding boxes will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            detections = sv.Detections(...)\n\n            color_annotator = sv.ColorAnnotator()\n            annotated_frame = color_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n\n        ![box-mask-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/box-mask-annotator-example-purple.png)\n        \"\"\"\n        scene_with_boxes = scene.copy()\n        for detection_idx in range(len(detections)):\n            x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            cv2.rectangle(\n                img=scene_with_boxes,\n                pt1=(x1, y1),\n                pt2=(x2, y2),\n                color=color.as_bgr(),\n                thickness=-1,\n            )\n\n        cv2.addWeighted(\n            scene_with_boxes, self.opacity, scene, 1 - self.opacity, gamma=0, dst=scene\n        )\n        return scene\n</code></pre> CircleAnnotator <p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing circle on an image using provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class CircleAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing circle on an image using provided detections.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n        thickness: int = 2,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            thickness (int): Thickness of the circle line.\n            color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n\n        self.color: Union[Color, ColorPalette] = color\n        self.thickness: int = thickness\n        self.color_lookup: ColorLookup = color_lookup\n\n    @ensure_cv2_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene with circles based on the provided detections.\n\n        Args:\n            scene (ImageType): The image where box corners will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            detections = sv.Detections(...)\n\n            circle_annotator = sv.CircleAnnotator()\n            annotated_frame = circle_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n\n\n        ![circle-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/circle-annotator-example-purple.png)\n        \"\"\"\n        for detection_idx in range(len(detections)):\n            x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n            center = ((x1 + x2) // 2, (y1 + y2) // 2)\n            distance = sqrt((x1 - center[0]) ** 2 + (y1 - center[1]) ** 2)\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            cv2.circle(\n                img=scene,\n                center=center,\n                radius=int(distance),\n                color=color.as_bgr(),\n                thickness=self.thickness,\n            )\n\n        return scene\n</code></pre> DotAnnotator <p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing dots on an image at specific coordinates based on provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class DotAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing dots on an image at specific coordinates based on provided\n    detections.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n        radius: int = 4,\n        position: Position = Position.CENTER,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n        outline_thickness: int = 0,\n        outline_color: Union[Color, ColorPalette] = Color.BLACK,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            radius (int): Radius of the drawn dots.\n            position (Position): The anchor position for placing the dot.\n            color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n            outline_thickness (int): Thickness of the outline of the dot.\n            outline_color (Union[Color, ColorPalette]): The color or color palette to\n                use for outline. It is activated by setting outline_thickness to a value\n                greater than 0.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.radius: int = radius\n        self.position: Position = position\n        self.color_lookup: ColorLookup = color_lookup\n        self.outline_thickness = outline_thickness\n        self.outline_color: Union[Color, ColorPalette] = outline_color\n\n    @ensure_cv2_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene with dots based on the provided detections.\n\n        Args:\n            scene (ImageType): The image where dots will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            detections = sv.Detections(...)\n\n            dot_annotator = sv.DotAnnotator()\n            annotated_frame = dot_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n\n        ![dot-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/dot-annotator-example-purple.png)\n        \"\"\"\n        xy = detections.get_anchors_coordinates(anchor=self.position)\n        for detection_idx in range(len(detections)):\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            center = (int(xy[detection_idx, 0]), int(xy[detection_idx, 1]))\n\n            cv2.circle(scene, center, self.radius, color.as_bgr(), -1)\n            if self.outline_thickness:\n                outline_color = resolve_color(\n                    color=self.outline_color,\n                    detections=detections,\n                    detection_idx=detection_idx,\n                    color_lookup=self.color_lookup\n                    if custom_color_lookup is None\n                    else custom_color_lookup,\n                )\n                cv2.circle(\n                    scene,\n                    center,\n                    self.radius,\n                    outline_color.as_bgr(),\n                    self.outline_thickness,\n                )\n        return scene\n</code></pre> TriangleAnnotator <p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing triangle markers on an image at specific coordinates based on provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class TriangleAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing triangle markers on an image at specific coordinates based on\n    provided detections.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n        base: int = 10,\n        height: int = 10,\n        position: Position = Position.TOP_CENTER,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n        outline_thickness: int = 0,\n        outline_color: Union[Color, ColorPalette] = Color.BLACK,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            base (int): The base width of the triangle.\n            height (int): The height of the triangle.\n            position (Position): The anchor position for placing the triangle.\n            color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n            outline_thickness (int): Thickness of the outline of the triangle.\n            outline_color (Union[Color, ColorPalette]): The color or color palette to\n                use for outline. It is activated by setting outline_thickness to a value\n                greater than 0.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.base: int = base\n        self.height: int = height\n        self.position: Position = position\n        self.color_lookup: ColorLookup = color_lookup\n        self.outline_thickness: int = outline_thickness\n        self.outline_color: Union[Color, ColorPalette] = outline_color\n\n    @ensure_cv2_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene with triangles based on the provided detections.\n\n        Args:\n            scene (ImageType): The image where triangles will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            detections = sv.Detections(...)\n\n            triangle_annotator = sv.TriangleAnnotator()\n            annotated_frame = triangle_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n\n        ![triangle-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/triangle-annotator-example.png)\n        \"\"\"\n        xy = detections.get_anchors_coordinates(anchor=self.position)\n        for detection_idx in range(len(detections)):\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            tip_x, tip_y = int(xy[detection_idx, 0]), int(xy[detection_idx, 1])\n            vertices = np.array(\n                [\n                    [tip_x - self.base // 2, tip_y - self.height],\n                    [tip_x + self.base // 2, tip_y - self.height],\n                    [tip_x, tip_y],\n                ],\n                np.int32,\n            )\n\n            cv2.fillPoly(scene, [vertices], color.as_bgr())\n            if self.outline_thickness:\n                outline_color = resolve_color(\n                    color=self.outline_color,\n                    detections=detections,\n                    detection_idx=detection_idx,\n                    color_lookup=self.color_lookup\n                    if custom_color_lookup is None\n                    else custom_color_lookup,\n                )\n                cv2.polylines(\n                    scene,\n                    [vertices],\n                    True,\n                    outline_color.as_bgr(),\n                    thickness=self.outline_thickness,\n                )\n        return scene\n</code></pre> EllipseAnnotator <p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing ellipses on an image using provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class EllipseAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing ellipses on an image using provided detections.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n        thickness: int = 2,\n        start_angle: int = -45,\n        end_angle: int = 235,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            thickness (int): Thickness of the ellipse lines.\n            start_angle (int): Starting angle of the ellipse.\n            end_angle (int): Ending angle of the ellipse.\n            color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.thickness: int = thickness\n        self.start_angle: int = start_angle\n        self.end_angle: int = end_angle\n        self.color_lookup: ColorLookup = color_lookup\n\n    @ensure_cv2_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene with ellipses based on the provided detections.\n\n        Args:\n            scene (ImageType): The image where ellipses will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            detections = sv.Detections(...)\n\n            ellipse_annotator = sv.EllipseAnnotator()\n            annotated_frame = ellipse_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n\n        ![ellipse-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/ellipse-annotator-example-purple.png)\n        \"\"\"\n        for detection_idx in range(len(detections)):\n            x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            center = (int((x1 + x2) / 2), y2)\n            width = x2 - x1\n            cv2.ellipse(\n                scene,\n                center=center,\n                axes=(int(width), int(0.35 * width)),\n                angle=0.0,\n                startAngle=self.start_angle,\n                endAngle=self.end_angle,\n                color=color.as_bgr(),\n                thickness=self.thickness,\n                lineType=cv2.LINE_4,\n            )\n        return scene\n</code></pre> HaloAnnotator <p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing Halos on an image using provided detections.</p> <p>Warning</p> <p>This annotator uses <code>sv.Detections.mask</code>.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class HaloAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing Halos on an image using provided detections.\n\n    !!! warning\n\n        This annotator uses `sv.Detections.mask`.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n        opacity: float = 0.8,\n        kernel_size: int = 40,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            opacity (float): Opacity of the overlay mask. Must be between `0` and `1`.\n            kernel_size (int): The size of the average pooling kernel used for creating\n                the halo.\n            color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.opacity = opacity\n        self.color_lookup: ColorLookup = color_lookup\n        self.kernel_size: int = kernel_size\n\n    @ensure_cv2_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene with halos based on the provided detections.\n\n        Args:\n            scene (ImageType): The image where masks will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            detections = sv.Detections(...)\n\n            halo_annotator = sv.HaloAnnotator()\n            annotated_frame = halo_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n\n        ![halo-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/halo-annotator-example-purple.png)\n        \"\"\"\n        if detections.mask is None:\n            return scene\n        colored_mask = np.zeros_like(scene, dtype=np.uint8)\n        fmask = np.array([False] * scene.shape[0] * scene.shape[1]).reshape(\n            scene.shape[0], scene.shape[1]\n        )\n\n        for detection_idx in np.flip(np.argsort(detections.area)):\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            mask = detections.mask[detection_idx]\n            fmask = np.logical_or(fmask, mask)\n            color_bgr = color.as_bgr()\n            colored_mask[mask] = color_bgr\n\n        colored_mask = cv2.blur(colored_mask, (self.kernel_size, self.kernel_size))\n        colored_mask[fmask] = [0, 0, 0]\n        gray = cv2.cvtColor(colored_mask, cv2.COLOR_BGR2GRAY)\n        alpha = self.opacity * gray / gray.max()\n        alpha_mask = alpha[:, :, np.newaxis]\n        blended_scene = np.uint8(scene * (1 - alpha_mask) + colored_mask * self.opacity)\n        np.copyto(scene, blended_scene)\n        return scene\n</code></pre> PercentageBarAnnotator <p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing percentage bars on an image using provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class PercentageBarAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing percentage bars on an image using provided detections.\n    \"\"\"\n\n    def __init__(\n        self,\n        height: int = 16,\n        width: int = 80,\n        color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n        border_color: Color = Color.BLACK,\n        position: Position = Position.TOP_CENTER,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n        border_thickness: int = None,\n    ):\n        \"\"\"\n        Args:\n            height (int): The height in pixels of the percentage bar.\n            width (int): The width in pixels of the percentage bar.\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            border_color (Color): The color of the border lines.\n            position (Position): The anchor position of drawing the percentage bar.\n            color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n            border_thickness (int): The thickness of the border lines.\n        \"\"\"\n        self.height: int = height\n        self.width: int = width\n        self.color: Union[Color, ColorPalette] = color\n        self.border_color: Color = border_color\n        self.position: Position = position\n        self.color_lookup: ColorLookup = color_lookup\n\n        if border_thickness is None:\n            self.border_thickness = int(0.15 * self.height)\n\n    @ensure_cv2_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n        custom_values: Optional[np.ndarray] = None,\n    ) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene with percentage bars based on the provided\n        detections. The percentage bars visually represent the confidence or custom\n        values associated with each detection.\n\n        Args:\n            scene (ImageType): The image where percentage bars will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n            custom_values (Optional[np.ndarray]): Custom values array to use instead\n                of the default detection confidences. This array should have the\n                same length as the number of detections and contain a value between\n                0 and 1 (inclusive) for each detection, representing the percentage\n                to be displayed.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            detections = sv.Detections(...)\n\n            percentage_bar_annotator = sv.PercentageBarAnnotator()\n            annotated_frame = percentage_bar_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n\n        ![percentage-bar-example](https://media.roboflow.com/\n        supervision-annotator-examples/percentage-bar-annotator-example-purple.png)\n        \"\"\"\n        self.validate_custom_values(\n            custom_values=custom_values, detections_count=len(detections)\n        )\n        anchors = detections.get_anchors_coordinates(anchor=self.position)\n        for detection_idx in range(len(detections)):\n            anchor = anchors[detection_idx]\n            border_coordinates = self.calculate_border_coordinates(\n                anchor_xy=(int(anchor[0]), int(anchor[1])),\n                border_wh=(self.width, self.height),\n                position=self.position,\n            )\n            border_width = border_coordinates[1][0] - border_coordinates[0][0]\n\n            value = (\n                custom_values[detection_idx]\n                if custom_values is not None\n                else detections.confidence[detection_idx]\n            )\n\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            cv2.rectangle(\n                img=scene,\n                pt1=border_coordinates[0],\n                pt2=(\n                    border_coordinates[0][0] + int(border_width * value),\n                    border_coordinates[1][1],\n                ),\n                color=color.as_bgr(),\n                thickness=-1,\n            )\n            cv2.rectangle(\n                img=scene,\n                pt1=border_coordinates[0],\n                pt2=border_coordinates[1],\n                color=self.border_color.as_bgr(),\n                thickness=self.border_thickness,\n            )\n        return scene\n\n    @staticmethod\n    def calculate_border_coordinates(\n        anchor_xy: Tuple[int, int], border_wh: Tuple[int, int], position: Position\n    ) -&gt; Tuple[Tuple[int, int], Tuple[int, int]]:\n        cx, cy = anchor_xy\n        width, height = border_wh\n\n        if position == Position.TOP_LEFT:\n            return (cx - width, cy - height), (cx, cy)\n        elif position == Position.TOP_CENTER:\n            return (cx - width // 2, cy), (cx + width // 2, cy - height)\n        elif position == Position.TOP_RIGHT:\n            return (cx, cy), (cx + width, cy - height)\n        elif position == Position.CENTER_LEFT:\n            return (cx - width, cy - height // 2), (cx, cy + height // 2)\n        elif position == Position.CENTER or position == Position.CENTER_OF_MASS:\n            return (\n                (cx - width // 2, cy - height // 2),\n                (cx + width // 2, cy + height // 2),\n            )\n        elif position == Position.CENTER_RIGHT:\n            return (cx, cy - height // 2), (cx + width, cy + height // 2)\n        elif position == Position.BOTTOM_LEFT:\n            return (cx - width, cy), (cx, cy + height)\n        elif position == Position.BOTTOM_CENTER:\n            return (cx - width // 2, cy), (cx + width // 2, cy + height)\n        elif position == Position.BOTTOM_RIGHT:\n            return (cx, cy), (cx + width, cy + height)\n\n    @staticmethod\n    def validate_custom_values(\n        custom_values: Optional[Union[np.ndarray, List[float]]], detections_count: int\n    ) -&gt; None:\n        if custom_values is not None:\n            if not isinstance(custom_values, (np.ndarray, list)):\n                raise TypeError(\n                    \"custom_values must be either a numpy array or a list of floats.\"\n                )\n\n            if len(custom_values) != detections_count:\n                raise ValueError(\n                    \"The length of custom_values must match the number of detections.\"\n                )\n\n            if not all(0 &lt;= value &lt;= 1 for value in custom_values):\n                raise ValueError(\"All values in custom_values must be between 0 and 1.\")\n</code></pre> HeatMapAnnotator <p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing heatmaps on an image based on provided detections. Heat accumulates over time and is drawn as a semi-transparent overlay of blurred circles.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class HeatMapAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing heatmaps on an image based on provided detections.\n    Heat accumulates over time and is drawn as a semi-transparent overlay\n    of blurred circles.\n    \"\"\"\n\n    def __init__(\n        self,\n        position: Position = Position.BOTTOM_CENTER,\n        opacity: float = 0.2,\n        radius: int = 40,\n        kernel_size: int = 25,\n        top_hue: int = 0,\n        low_hue: int = 125,\n    ):\n        \"\"\"\n        Args:\n            position (Position): The position of the heatmap. Defaults to\n                `BOTTOM_CENTER`.\n            opacity (float): Opacity of the overlay mask, between 0 and 1.\n            radius (int): Radius of the heat circle.\n            kernel_size (int): Kernel size for blurring the heatmap.\n            top_hue (int): Hue at the top of the heatmap. Defaults to 0 (red).\n            low_hue (int): Hue at the bottom of the heatmap. Defaults to 125 (blue).\n        \"\"\"\n        self.position = position\n        self.opacity = opacity\n        self.radius = radius\n        self.kernel_size = kernel_size\n        self.heat_mask = None\n        self.top_hue = top_hue\n        self.low_hue = low_hue\n\n    @ensure_cv2_image_for_annotation\n    def annotate(self, scene: ImageType, detections: Detections) -&gt; ImageType:\n        \"\"\"\n        Annotates the scene with a heatmap based on the provided detections.\n\n        Args:\n            scene (ImageType): The image where the heatmap will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n            from ultralytics import YOLO\n\n            model = YOLO('yolov8x.pt')\n\n            heat_map_annotator = sv.HeatMapAnnotator()\n\n            video_info = sv.VideoInfo.from_video_path(video_path='...')\n            frames_generator = get_video_frames_generator(source_path='...')\n\n            with sv.VideoSink(target_path='...', video_info=video_info) as sink:\n               for frame in frames_generator:\n                   result = model(frame)[0]\n                   detections = sv.Detections.from_ultralytics(result)\n                   annotated_frame = heat_map_annotator.annotate(\n                       scene=frame.copy(),\n                       detections=detections)\n                   sink.write_frame(frame=annotated_frame)\n            ```\n\n        ![heatmap-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/heat-map-annotator-example-purple.png)\n        \"\"\"\n\n        if self.heat_mask is None:\n            self.heat_mask = np.zeros(scene.shape[:2])\n        mask = np.zeros(scene.shape[:2])\n        for xy in detections.get_anchors_coordinates(self.position):\n            cv2.circle(mask, (int(xy[0]), int(xy[1])), self.radius, 1, -1)\n        self.heat_mask = mask + self.heat_mask\n        temp = self.heat_mask.copy()\n        temp = self.low_hue - temp / temp.max() * (self.low_hue - self.top_hue)\n        temp = temp.astype(np.uint8)\n        if self.kernel_size is not None:\n            temp = cv2.blur(temp, (self.kernel_size, self.kernel_size))\n        hsv = np.zeros(scene.shape)\n        hsv[..., 0] = temp\n        hsv[..., 1] = 255\n        hsv[..., 2] = 255\n        temp = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)\n        mask = cv2.cvtColor(self.heat_mask.astype(np.uint8), cv2.COLOR_GRAY2BGR) &gt; 0\n        scene[mask] = cv2.addWeighted(temp, self.opacity, scene, 1 - self.opacity, 0)[\n            mask\n        ]\n        return scene\n</code></pre> MaskAnnotator <p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing masks on an image using provided detections.</p> <p>Warning</p> <p>This annotator uses <code>sv.Detections.mask</code>.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class MaskAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing masks on an image using provided detections.\n\n    !!! warning\n\n        This annotator uses `sv.Detections.mask`.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n        opacity: float = 0.5,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            opacity (float): Opacity of the overlay mask. Must be between `0` and `1`.\n            color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.opacity = opacity\n        self.color_lookup: ColorLookup = color_lookup\n\n    @ensure_cv2_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene with masks based on the provided detections.\n\n        Args:\n            scene (ImageType): The image where masks will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            detections = sv.Detections(...)\n\n            mask_annotator = sv.MaskAnnotator()\n            annotated_frame = mask_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n\n        ![mask-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/mask-annotator-example-purple.png)\n        \"\"\"\n        if detections.mask is None:\n            return scene\n\n        colored_mask = np.array(scene, copy=True, dtype=np.uint8)\n\n        for detection_idx in np.flip(np.argsort(detections.area)):\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            mask = detections.mask[detection_idx]\n            colored_mask[mask] = color.as_bgr()\n\n        cv2.addWeighted(\n            colored_mask, self.opacity, scene, 1 - self.opacity, 0, dst=scene\n        )\n        return scene\n</code></pre> PolygonAnnotator <p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing polygons on an image using provided detections.</p> <p>Warning</p> <p>This annotator uses <code>sv.Detections.mask</code>.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class PolygonAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing polygons on an image using provided detections.\n\n    !!! warning\n\n        This annotator uses `sv.Detections.mask`.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n        thickness: int = 2,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating detections.\n            thickness (int): Thickness of the polygon lines.\n            color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.thickness: int = thickness\n        self.color_lookup: ColorLookup = color_lookup\n\n    @ensure_cv2_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene with polygons based on the provided detections.\n\n        Args:\n            scene (ImageType): The image where polygons will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            detections = sv.Detections(...)\n\n            polygon_annotator = sv.PolygonAnnotator()\n            annotated_frame = polygon_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n\n        ![polygon-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/polygon-annotator-example-purple.png)\n        \"\"\"\n        if detections.mask is None:\n            return scene\n\n        for detection_idx in range(len(detections)):\n            mask = detections.mask[detection_idx]\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            for polygon in mask_to_polygons(mask=mask):\n                scene = draw_polygon(\n                    scene=scene,\n                    polygon=polygon,\n                    color=color,\n                    thickness=self.thickness,\n                )\n\n        return scene\n</code></pre> LabelAnnotator <p>               Bases: <code>BaseAnnotator</code></p> <p>A class for annotating labels on an image using provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class LabelAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for annotating labels on an image using provided detections.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n        text_color: Union[Color, ColorPalette] = Color.WHITE,\n        text_scale: float = 0.5,\n        text_thickness: int = 1,\n        text_padding: int = 10,\n        text_position: Position = Position.TOP_LEFT,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n        border_radius: int = 0,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating the text background.\n            text_color (Union[Color, ColorPalette]): The color or color palette to use\n                for the text.\n            text_scale (float): Font scale for the text.\n            text_thickness (int): Thickness of the text characters.\n            text_padding (int): Padding around the text within its background box.\n            text_position (Position): Position of the text relative to the detection.\n                Possible values are defined in the `Position` enum.\n            color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n            border_radius (int): The radius to apply round edges. If the selected\n                value is higher than the lower dimension, width or height, is clipped.\n        \"\"\"\n        self.border_radius: int = border_radius\n        self.color: Union[Color, ColorPalette] = color\n        self.text_color: Union[Color, ColorPalette] = text_color\n        self.text_scale: float = text_scale\n        self.text_thickness: int = text_thickness\n        self.text_padding: int = text_padding\n        self.text_anchor: Position = text_position\n        self.color_lookup: ColorLookup = color_lookup\n\n    @ensure_cv2_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n        labels: Optional[List[str]] = None,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene with labels based on the provided detections.\n\n        Args:\n            scene (ImageType): The image where labels will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n            labels (Optional[List[str]]): Custom labels for each detection.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            detections = sv.Detections(...)\n\n            labels = [\n                f\"{class_name} {confidence:.2f}\"\n                for class_name, confidence\n                in zip(detections['class_name'], detections.confidence)\n            ]\n\n            label_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n            annotated_frame = label_annotator.annotate(\n                scene=image.copy(),\n                detections=detections,\n                labels=labels\n            )\n            ```\n\n        ![label-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/label-annotator-example-purple.png)\n        \"\"\"\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        anchors_coordinates = detections.get_anchors_coordinates(\n            anchor=self.text_anchor\n        ).astype(int)\n        if labels is not None and len(labels) != len(detections):\n            raise ValueError(\n                f\"The number of labels ({len(labels)}) does not match the \"\n                f\"number of detections ({len(detections)}). Each detection \"\n                f\"should have exactly 1 label.\"\n            )\n\n        for detection_idx, center_coordinates in enumerate(anchors_coordinates):\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=(\n                    self.color_lookup\n                    if custom_color_lookup is None\n                    else custom_color_lookup\n                ),\n            )\n\n            text_color = resolve_color(\n                color=self.text_color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=(\n                    self.color_lookup\n                    if custom_color_lookup is None\n                    else custom_color_lookup\n                ),\n            )\n\n            if labels is not None:\n                text = labels[detection_idx]\n            elif detections[CLASS_NAME_DATA_FIELD] is not None:\n                text = detections[CLASS_NAME_DATA_FIELD][detection_idx]\n            elif detections.class_id is not None:\n                text = str(detections.class_id[detection_idx])\n            else:\n                text = str(detection_idx)\n\n            text_w, text_h = cv2.getTextSize(\n                text=text,\n                fontFace=font,\n                fontScale=self.text_scale,\n                thickness=self.text_thickness,\n            )[0]\n            text_w_padded = text_w + 2 * self.text_padding\n            text_h_padded = text_h + 2 * self.text_padding\n            text_background_xyxy = resolve_text_background_xyxy(\n                center_coordinates=tuple(center_coordinates),\n                text_wh=(text_w_padded, text_h_padded),\n                position=self.text_anchor,\n            )\n\n            text_x = text_background_xyxy[0] + self.text_padding\n            text_y = text_background_xyxy[1] + self.text_padding + text_h\n\n            self.draw_rounded_rectangle(\n                scene=scene,\n                xyxy=text_background_xyxy,\n                color=color.as_bgr(),\n                border_radius=self.border_radius,\n            )\n            cv2.putText(\n                img=scene,\n                text=text,\n                org=(text_x, text_y),\n                fontFace=font,\n                fontScale=self.text_scale,\n                color=text_color.as_bgr(),\n                thickness=self.text_thickness,\n                lineType=cv2.LINE_AA,\n            )\n        return scene\n\n    @staticmethod\n    def draw_rounded_rectangle(\n        scene: np.ndarray,\n        xyxy: Tuple[int, int, int, int],\n        color: Tuple[int, int, int],\n        border_radius: int,\n    ) -&gt; np.ndarray:\n        x1, y1, x2, y2 = xyxy\n        width = x2 - x1\n        height = y2 - y1\n\n        border_radius = min(border_radius, min(width, height) // 2)\n\n        rectangle_coordinates = [\n            ((x1 + border_radius, y1), (x2 - border_radius, y2)),\n            ((x1, y1 + border_radius), (x2, y2 - border_radius)),\n        ]\n        circle_centers = [\n            (x1 + border_radius, y1 + border_radius),\n            (x2 - border_radius, y1 + border_radius),\n            (x1 + border_radius, y2 - border_radius),\n            (x2 - border_radius, y2 - border_radius),\n        ]\n\n        for coordinates in rectangle_coordinates:\n            cv2.rectangle(\n                img=scene,\n                pt1=coordinates[0],\n                pt2=coordinates[1],\n                color=color,\n                thickness=-1,\n            )\n        for center in circle_centers:\n            cv2.circle(\n                img=scene,\n                center=center,\n                radius=border_radius,\n                color=color,\n                thickness=-1,\n            )\n        return scene\n</code></pre> RichLabelAnnotator <p>               Bases: <code>BaseAnnotator</code></p> <p>A class for annotating labels on an image using provided detections, with support for Unicode characters by using a custom font.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class RichLabelAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for annotating labels on an image using provided detections,\n    with support for Unicode characters by using a custom font.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n        text_color: Union[Color, ColorPalette] = Color.WHITE,\n        font_path: Optional[str] = None,\n        font_size: int = 10,\n        text_padding: int = 10,\n        text_position: Position = Position.TOP_LEFT,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n        border_radius: int = 0,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color or color palette to use for\n                annotating the text background.\n            text_color (Union[Color, ColorPalette]): The color to use for the text.\n            font_path (Optional[str]): Path to the font file (e.g., \".ttf\" or \".otf\")\n                to use for rendering text. If `None`, the default PIL font will be used.\n            font_size (int): Font size for the text.\n            text_padding (int): Padding around the text within its background box.\n            text_position (Position): Position of the text relative to the detection.\n                Possible values are defined in the `Position` enum.\n            color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n            border_radius (int): The radius to apply round edges. If the selected\n                value is higher than the lower dimension, width or height, is clipped.\n        \"\"\"\n        self.color = color\n        self.text_color = text_color\n        self.text_padding = text_padding\n        self.text_anchor = text_position\n        self.color_lookup = color_lookup\n        self.border_radius = border_radius\n        if font_path is not None:\n            try:\n                self.font = ImageFont.truetype(font_path, font_size)\n            except OSError:\n                print(f\"Font path '{font_path}' not found. Using PIL's default font.\")\n                self.font = self._load_default_font(font_size)\n        else:\n            self.font = self._load_default_font(font_size)\n\n    @ensure_pil_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n        labels: Optional[List[str]] = None,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene with labels based on the provided\n        detections, with support for Unicode characters.\n\n        Args:\n            scene (ImageType): The image where labels will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n            labels (Optional[List[str]]): Custom labels for each detection.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            detections = sv.Detections(...)\n\n            labels = [\n                f\"{class_name} {confidence:.2f}\"\n                for class_name, confidence\n                in zip(detections['class_name'], detections.confidence)\n            ]\n\n            rich_label_annotator = sv.RichLabelAnnotator(font_path=\"path/to/font.ttf\")\n            annotated_frame = label_annotator.annotate(\n                scene=image.copy(),\n                detections=detections,\n                labels=labels\n            )\n            ```\n\n        \"\"\"\n        draw = ImageDraw.Draw(scene)\n        anchors_coordinates = detections.get_anchors_coordinates(\n            anchor=self.text_anchor\n        ).astype(int)\n        if labels is not None and len(labels) != len(detections):\n            raise ValueError(\n                f\"The number of labels provided ({len(labels)}) does not match the \"\n                f\"number of detections ({len(detections)}). Each detection should have \"\n                f\"a corresponding label.\"\n            )\n        for detection_idx, center_coordinates in enumerate(anchors_coordinates):\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=(\n                    self.color_lookup\n                    if custom_color_lookup is None\n                    else custom_color_lookup\n                ),\n            )\n\n            text_color = resolve_color(\n                color=self.text_color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=(\n                    self.color_lookup\n                    if custom_color_lookup is None\n                    else custom_color_lookup\n                ),\n            )\n\n            if labels is not None:\n                text = labels[detection_idx]\n            elif detections[CLASS_NAME_DATA_FIELD] is not None:\n                text = detections[CLASS_NAME_DATA_FIELD][detection_idx]\n            elif detections.class_id is not None:\n                text = str(detections.class_id[detection_idx])\n            else:\n                text = str(detection_idx)\n\n            left, top, right, bottom = draw.textbbox((0, 0), text, font=self.font)\n            text_width = right - left\n            text_height = bottom - top\n            text_w_padded = text_width + 2 * self.text_padding\n            text_h_padded = text_height + 2 * self.text_padding\n            text_background_xyxy = resolve_text_background_xyxy(\n                center_coordinates=tuple(center_coordinates),\n                text_wh=(text_w_padded, text_h_padded),\n                position=self.text_anchor,\n            )\n\n            text_x = text_background_xyxy[0] + self.text_padding - left\n            text_y = text_background_xyxy[1] + self.text_padding - top\n\n            draw.rounded_rectangle(\n                text_background_xyxy,\n                radius=self.border_radius,\n                fill=color.as_rgb(),\n                outline=None,\n            )\n            draw.text(\n                xy=(text_x, text_y),\n                text=text,\n                font=self.font,\n                fill=text_color.as_rgb(),\n            )\n        return scene\n\n    @staticmethod\n    def _load_default_font(size):\n        \"\"\"\n        PIL either loads a font that accepts a size (e.g. on my machine)\n        or raises an error saying `load_default` does not accept arguments\n        (e.g. in Colab).\n        \"\"\"\n        try:\n            font = ImageFont.load_default(size)\n        except TypeError:\n            font = ImageFont.load_default()\n        return font\n</code></pre> BlurAnnotator <p>               Bases: <code>BaseAnnotator</code></p> <p>A class for blurring regions in an image using provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class BlurAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for blurring regions in an image using provided detections.\n    \"\"\"\n\n    def __init__(self, kernel_size: int = 15):\n        \"\"\"\n        Args:\n            kernel_size (int): The size of the average pooling kernel used for blurring.\n        \"\"\"\n        self.kernel_size: int = kernel_size\n\n    @ensure_cv2_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n    ) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene by blurring regions based on the provided detections.\n\n        Args:\n            scene (ImageType): The image where blurring will be applied.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            detections = sv.Detections(...)\n\n            blur_annotator = sv.BlurAnnotator()\n            annotated_frame = circle_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n\n        ![blur-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/blur-annotator-example-purple.png)\n        \"\"\"\n        image_height, image_width = scene.shape[:2]\n        clipped_xyxy = clip_boxes(\n            xyxy=detections.xyxy, resolution_wh=(image_width, image_height)\n        ).astype(int)\n\n        for x1, y1, x2, y2 in clipped_xyxy:\n            roi = scene[y1:y2, x1:x2]\n            roi = cv2.blur(roi, (self.kernel_size, self.kernel_size))\n            scene[y1:y2, x1:x2] = roi\n\n        return scene\n</code></pre> PixelateAnnotator <p>               Bases: <code>BaseAnnotator</code></p> <p>A class for pixelating regions in an image using provided detections.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class PixelateAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for pixelating regions in an image using provided detections.\n    \"\"\"\n\n    def __init__(self, pixel_size: int = 20):\n        \"\"\"\n        Args:\n            pixel_size (int): The size of the pixelation.\n        \"\"\"\n        self.pixel_size: int = pixel_size\n\n    @ensure_cv2_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n    ) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene by pixelating regions based on the provided\n            detections.\n\n        Args:\n            scene (ImageType): The image where pixelating will be applied.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            detections = sv.Detections(...)\n\n            pixelate_annotator = sv.PixelateAnnotator()\n            annotated_frame = pixelate_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n\n        ![pixelate-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/pixelate-annotator-example-10.png)\n        \"\"\"\n        image_height, image_width = scene.shape[:2]\n        clipped_xyxy = clip_boxes(\n            xyxy=detections.xyxy, resolution_wh=(image_width, image_height)\n        ).astype(int)\n\n        for x1, y1, x2, y2 in clipped_xyxy:\n            roi = scene[y1:y2, x1:x2]\n            scaled_up_roi = cv2.resize(\n                src=roi, dsize=None, fx=1 / self.pixel_size, fy=1 / self.pixel_size\n            )\n            scaled_down_roi = cv2.resize(\n                src=scaled_up_roi,\n                dsize=(roi.shape[1], roi.shape[0]),\n                interpolation=cv2.INTER_NEAREST,\n            )\n\n            scene[y1:y2, x1:x2] = scaled_down_roi\n\n        return scene\n</code></pre> TraceAnnotator <p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing trace paths on an image based on detection coordinates.</p> <p>Warning</p> <p>This annotator uses the <code>sv.Detections.tracker_id</code>. Read here to learn how to plug tracking into your inference pipeline.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class TraceAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing trace paths on an image based on detection coordinates.\n\n    !!! warning\n\n        This annotator uses the `sv.Detections.tracker_id`. Read\n        [here](/latest/trackers/) to learn how to plug\n        tracking into your inference pipeline.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n        position: Position = Position.CENTER,\n        trace_length: int = 30,\n        thickness: int = 2,\n        color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, ColorPalette]): The color to draw the trace, can be\n                a single color or a color palette.\n            position (Position): The position of the trace.\n                Defaults to `CENTER`.\n            trace_length (int): The maximum length of the trace in terms of historical\n                points. Defaults to `30`.\n            thickness (int): The thickness of the trace lines. Defaults to `2`.\n            color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n                Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.color: Union[Color, ColorPalette] = color\n        self.trace = Trace(max_size=trace_length, anchor=position)\n        self.thickness = thickness\n        self.color_lookup: ColorLookup = color_lookup\n\n    @ensure_cv2_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; ImageType:\n        \"\"\"\n        Draws trace paths on the frame based on the detection coordinates provided.\n\n        Args:\n            scene (ImageType): The image on which the traces will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): The detections which include coordinates for\n                which the traces will be drawn.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n            from ultralytics import YOLO\n\n            model = YOLO('yolov8x.pt')\n            trace_annotator = sv.TraceAnnotator()\n\n            video_info = sv.VideoInfo.from_video_path(video_path='...')\n            frames_generator = sv.get_video_frames_generator(source_path='...')\n            tracker = sv.ByteTrack()\n\n            with sv.VideoSink(target_path='...', video_info=video_info) as sink:\n               for frame in frames_generator:\n                   result = model(frame)[0]\n                   detections = sv.Detections.from_ultralytics(result)\n                   detections = tracker.update_with_detections(detections)\n                   annotated_frame = trace_annotator.annotate(\n                       scene=frame.copy(),\n                       detections=detections)\n                   sink.write_frame(frame=annotated_frame)\n            ```\n\n        ![trace-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/trace-annotator-example-purple.png)\n        \"\"\"\n        self.trace.put(detections)\n\n        for detection_idx in range(len(detections)):\n            tracker_id = int(detections.tracker_id[detection_idx])\n            color = resolve_color(\n                color=self.color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            xy = self.trace.get(tracker_id=tracker_id)\n            if len(xy) &gt; 1:\n                scene = cv2.polylines(\n                    scene,\n                    [xy.astype(np.int32)],\n                    False,\n                    color=color.as_bgr(),\n                    thickness=self.thickness,\n                )\n        return scene\n</code></pre> CropAnnotator <p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing scaled up crops of detections on the scene.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class CropAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing scaled up crops of detections on the scene.\n    \"\"\"\n\n    def __init__(\n        self,\n        position: Position = Position.TOP_CENTER,\n        scale_factor: float = 2.0,\n        border_color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n        border_thickness: int = 2,\n        border_color_lookup: ColorLookup = ColorLookup.CLASS,\n    ):\n        \"\"\"\n        Args:\n            position (Position): The anchor position for placing the cropped and scaled\n                part of the detection in the scene.\n            scale_factor (float): The factor by which to scale the cropped image part. A\n                factor of 2, for example, would double the size of the cropped area,\n                allowing for a closer view of the detection.\n            border_color (Union[Color, ColorPalette]): The color or color palette to\n                use for annotating border around the cropped area.\n            border_thickness (int): The thickness of the border around the cropped area.\n            border_color_lookup (ColorLookup): Strategy for mapping colors to\n                annotations. Options are `INDEX`, `CLASS`, `TRACK`.\n        \"\"\"\n        self.position: Position = position\n        self.scale_factor: float = scale_factor\n        self.border_color: Union[Color, ColorPalette] = border_color\n        self.border_thickness: int = border_thickness\n        self.border_color_lookup: ColorLookup = border_color_lookup\n\n    @ensure_cv2_image_for_annotation\n    def annotate(\n        self,\n        scene: ImageType,\n        detections: Detections,\n        custom_color_lookup: Optional[np.ndarray] = None,\n    ) -&gt; ImageType:\n        \"\"\"\n        Annotates the provided scene with scaled and cropped parts of the image based\n        on the provided detections. Each detection is cropped from the original scene\n        and scaled according to the annotator's scale factor before being placed back\n        onto the scene at the specified position.\n\n\n        Args:\n            scene (ImageType): The image where cropped detection will be placed.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n            custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n                Allows to override the default color mapping strategy.\n\n        Returns:\n            The annotated image.\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            detections = sv.Detections(...)\n\n            crop_annotator = sv.CropAnnotator()\n            annotated_frame = crop_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n        \"\"\"\n        crops = [\n            crop_image(image=scene, xyxy=xyxy) for xyxy in detections.xyxy.astype(int)\n        ]\n        resized_crops = [\n            scale_image(image=crop, scale_factor=self.scale_factor) for crop in crops\n        ]\n        anchors = detections.get_anchors_coordinates(anchor=self.position).astype(int)\n\n        for idx, (resized_crop, anchor) in enumerate(zip(resized_crops, anchors)):\n            crop_wh = resized_crop.shape[1], resized_crop.shape[0]\n            (x1, y1), (x2, y2) = self.calculate_crop_coordinates(\n                anchor=anchor, crop_wh=crop_wh, position=self.position\n            )\n            scene = overlay_image(image=scene, overlay=resized_crop, anchor=(x1, y1))\n            color = resolve_color(\n                color=self.border_color,\n                detections=detections,\n                detection_idx=idx,\n                color_lookup=self.border_color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            cv2.rectangle(\n                img=scene,\n                pt1=(x1, y1),\n                pt2=(x2, y2),\n                color=color.as_bgr(),\n                thickness=self.border_thickness,\n            )\n\n        return scene\n\n    @staticmethod\n    def calculate_crop_coordinates(\n        anchor: Tuple[int, int], crop_wh: Tuple[int, int], position: Position\n    ) -&gt; Tuple[Tuple[int, int], Tuple[int, int]]:\n        anchor_x, anchor_y = anchor\n        width, height = crop_wh\n\n        if position == Position.TOP_LEFT:\n            return (anchor_x - width, anchor_y - height), (anchor_x, anchor_y)\n        elif position == Position.TOP_CENTER:\n            return (\n                (anchor_x - width // 2, anchor_y - height),\n                (anchor_x + width // 2, anchor_y),\n            )\n        elif position == Position.TOP_RIGHT:\n            return (anchor_x, anchor_y - height), (anchor_x + width, anchor_y)\n        elif position == Position.CENTER_LEFT:\n            return (\n                (anchor_x - width, anchor_y - height // 2),\n                (anchor_x, anchor_y + height // 2),\n            )\n        elif position == Position.CENTER or position == Position.CENTER_OF_MASS:\n            return (\n                (anchor_x - width // 2, anchor_y - height // 2),\n                (anchor_x + width // 2, anchor_y + height // 2),\n            )\n        elif position == Position.CENTER_RIGHT:\n            return (\n                (anchor_x, anchor_y - height // 2),\n                (anchor_x + width, anchor_y + height // 2),\n            )\n        elif position == Position.BOTTOM_LEFT:\n            return (anchor_x - width, anchor_y), (anchor_x, anchor_y + height)\n        elif position == Position.BOTTOM_CENTER:\n            return (\n                (anchor_x - width // 2, anchor_y),\n                (anchor_x + width // 2, anchor_y + height),\n            )\n        elif position == Position.BOTTOM_RIGHT:\n            return (anchor_x, anchor_y), (anchor_x + width, anchor_y + height)\n</code></pre> BackgroundOverlayAnnotator <p>               Bases: <code>BaseAnnotator</code></p> <p>A class for drawing a colored overlay on the background of an image outside the region of detections.</p> <p>If masks are provided, the background is colored outside the masks. If masks are not provided, the background is colored outside the bounding boxes.</p> <p>You can use the <code>force_box</code> parameter to force the annotator to use bounding boxes.</p> <p>Warning</p> <p>This annotator uses <code>sv.Detections.mask</code>.</p> Source code in <code>supervision/annotators/core.py</code> <pre><code>class BackgroundOverlayAnnotator(BaseAnnotator):\n    \"\"\"\n    A class for drawing a colored overlay on the background of an image outside\n    the region of detections.\n\n    If masks are provided, the background is colored outside the masks.\n    If masks are not provided, the background is colored outside the bounding boxes.\n\n    You can use the `force_box` parameter to force the annotator to use bounding boxes.\n\n    !!! warning\n\n        This annotator uses `sv.Detections.mask`.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Color = Color.BLACK,\n        opacity: float = 0.5,\n        force_box: bool = False,\n    ):\n        \"\"\"\n        Args:\n            color (Color): The color to use for annotating detections.\n            opacity (float): Opacity of the overlay mask. Must be between `0` and `1`.\n            force_box (bool): If `True`, forces the annotator to use bounding boxes when\n                masks are provided in the supplied sv.Detections.\n        \"\"\"\n        self.color: Color = color\n        self.opacity = opacity\n        self.force_box = force_box\n\n    @ensure_cv2_image_for_annotation\n    def annotate(self, scene: ImageType, detections: Detections) -&gt; ImageType:\n        \"\"\"\n        Applies a colored overlay to the scene outside of the detected regions.\n\n        Args:\n            scene (ImageType): The image where masks will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray`\n                or `PIL.Image.Image`.\n            detections (Detections): Object detections to annotate.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            detections = sv.Detections(...)\n\n            background_overlay_annotator = sv.BackgroundOverlayAnnotator()\n            annotated_frame = background_overlay_annotator.annotate(\n                scene=image.copy(),\n                detections=detections\n            )\n            ```\n\n        ![background-overlay-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/background-color-annotator-example-purple.png)\n        \"\"\"\n        colored_mask = np.full_like(scene, self.color.as_bgr(), dtype=np.uint8)\n\n        cv2.addWeighted(\n            scene, 1 - self.opacity, colored_mask, self.opacity, 0, dst=colored_mask\n        )\n\n        if detections.mask is None or self.force_box:\n            for x1, y1, x2, y2 in detections.xyxy.astype(int):\n                colored_mask[y1:y2, x1:x2] = scene[y1:y2, x1:x2]\n        else:\n            for mask in detections.mask:\n                colored_mask[mask] = scene[mask]\n\n        np.copyto(scene, colored_mask)\n        return scene\n</code></pre> ColorLookup <p>               Bases: <code>Enum</code></p> <p>Enumeration class to define strategies for mapping colors to annotations.</p> This enum supports three different lookup strategies <ul> <li><code>INDEX</code>: Colors are determined by the index of the detection within the scene.</li> <li><code>CLASS</code>: Colors are determined by the class label of the detected object.</li> <li><code>TRACK</code>: Colors are determined by the tracking identifier of the object.</li> </ul> Source code in <code>supervision/annotators/utils.py</code> <pre><code>class ColorLookup(Enum):\n    \"\"\"\n    Enumeration class to define strategies for mapping colors to annotations.\n\n    This enum supports three different lookup strategies:\n        - `INDEX`: Colors are determined by the index of the detection within the scene.\n        - `CLASS`: Colors are determined by the class label of the detected object.\n        - `TRACK`: Colors are determined by the tracking identifier of the object.\n    \"\"\"\n\n    INDEX = \"index\"\n    CLASS = \"class\"\n    TRACK = \"track\"\n\n    @classmethod\n    def list(cls):\n        return list(map(lambda c: c.value, cls))\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.BoxAnnotator-functions","title":"Functions","text":""},{"location":"detection/annotators/#supervision.annotators.core.BoxAnnotator.__init__","title":"<code>__init__(color=ColorPalette.DEFAULT, thickness=2, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>DEFAULT</code> <code>thickness</code> <code>int</code> <p>Thickness of the bounding box lines.</p> <code>2</code> <code>color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n    thickness: int = 2,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        thickness (int): Thickness of the bounding box lines.\n        color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.thickness: int = thickness\n    self.color_lookup: ColorLookup = color_lookup\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.BoxAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with bounding boxes based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where bounding boxes will be drawn. <code>ImageType</code></p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nbox_annotator = sv.BoxAnnotator()\nannotated_frame = box_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene with bounding boxes based on the provided detections.\n\n    Args:\n        scene (ImageType): The image where bounding boxes will be drawn. `ImageType`\n        is a flexible type, accepting either `numpy.ndarray` or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        detections = sv.Detections(...)\n\n        box_annotator = sv.BoxAnnotator()\n        annotated_frame = box_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n\n    ![bounding-box-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/bounding-box-annotator-example-purple.png)\n    \"\"\"\n    for detection_idx in range(len(detections)):\n        x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        cv2.rectangle(\n            img=scene,\n            pt1=(x1, y1),\n            pt2=(x2, y2),\n            color=color.as_bgr(),\n            thickness=self.thickness,\n        )\n    return scene\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.RoundBoxAnnotator-functions","title":"Functions","text":""},{"location":"detection/annotators/#supervision.annotators.core.RoundBoxAnnotator.__init__","title":"<code>__init__(color=ColorPalette.DEFAULT, thickness=2, color_lookup=ColorLookup.CLASS, roundness=0.6)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>DEFAULT</code> <code>thickness</code> <code>int</code> <p>Thickness of the bounding box lines.</p> <code>2</code> <code>color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> <code>roundness</code> <code>float</code> <p>Percent of roundness for edges of bounding box. Value must be float 0 &lt; roundness &lt;= 1.0 By default roundness percent is calculated based on smaller side length (width or height).</p> <code>0.6</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n    thickness: int = 2,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n    roundness: float = 0.6,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        thickness (int): Thickness of the bounding box lines.\n        color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n        roundness (float): Percent of roundness for edges of bounding box.\n            Value must be float 0 &lt; roundness &lt;= 1.0\n            By default roundness percent is calculated based on smaller side\n            length (width or height).\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.thickness: int = thickness\n    self.color_lookup: ColorLookup = color_lookup\n    if not 0 &lt; roundness &lt;= 1.0:\n        raise ValueError(\"roundness attribute must be float between (0, 1.0]\")\n    self.roundness: float = roundness\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.RoundBoxAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with bounding boxes with rounded edges based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where rounded bounding boxes will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nround_box_annotator = sv.RoundBoxAnnotator()\nannotated_frame = round_box_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene with bounding boxes with rounded edges\n    based on the provided detections.\n\n    Args:\n        scene (ImageType): The image where rounded bounding boxes will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        detections = sv.Detections(...)\n\n        round_box_annotator = sv.RoundBoxAnnotator()\n        annotated_frame = round_box_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n\n    ![round-box-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/round-box-annotator-example-purple.png)\n    \"\"\"\n\n    for detection_idx in range(len(detections)):\n        x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n\n        radius = (\n            int((x2 - x1) // 2 * self.roundness)\n            if abs(x1 - x2) &lt; abs(y1 - y2)\n            else int((y2 - y1) // 2 * self.roundness)\n        )\n\n        circle_coordinates = [\n            ((x1 + radius), (y1 + radius)),\n            ((x2 - radius), (y1 + radius)),\n            ((x2 - radius), (y2 - radius)),\n            ((x1 + radius), (y2 - radius)),\n        ]\n\n        line_coordinates = [\n            ((x1 + radius, y1), (x2 - radius, y1)),\n            ((x2, y1 + radius), (x2, y2 - radius)),\n            ((x1 + radius, y2), (x2 - radius, y2)),\n            ((x1, y1 + radius), (x1, y2 - radius)),\n        ]\n\n        start_angles = (180, 270, 0, 90)\n        end_angles = (270, 360, 90, 180)\n\n        for center_coordinates, line, start_angle, end_angle in zip(\n            circle_coordinates, line_coordinates, start_angles, end_angles\n        ):\n            cv2.ellipse(\n                img=scene,\n                center=center_coordinates,\n                axes=(radius, radius),\n                angle=0,\n                startAngle=start_angle,\n                endAngle=end_angle,\n                color=color.as_bgr(),\n                thickness=self.thickness,\n            )\n\n            cv2.line(\n                img=scene,\n                pt1=line[0],\n                pt2=line[1],\n                color=color.as_bgr(),\n                thickness=self.thickness,\n            )\n\n    return scene\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.BoxCornerAnnotator-functions","title":"Functions","text":""},{"location":"detection/annotators/#supervision.annotators.core.BoxCornerAnnotator.__init__","title":"<code>__init__(color=ColorPalette.DEFAULT, thickness=4, corner_length=15, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>DEFAULT</code> <code>thickness</code> <code>int</code> <p>Thickness of the corner lines.</p> <code>4</code> <code>corner_length</code> <code>int</code> <p>Length of each corner line.</p> <code>15</code> <code>color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n    thickness: int = 4,\n    corner_length: int = 15,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        thickness (int): Thickness of the corner lines.\n        corner_length (int): Length of each corner line.\n        color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.thickness: int = thickness\n    self.corner_length: int = corner_length\n    self.color_lookup: ColorLookup = color_lookup\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.BoxCornerAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with box corners based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where box corners will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\ncorner_annotator = sv.BoxCornerAnnotator()\nannotated_frame = corner_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene with box corners based on the provided detections.\n\n    Args:\n        scene (ImageType): The image where box corners will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        detections = sv.Detections(...)\n\n        corner_annotator = sv.BoxCornerAnnotator()\n        annotated_frame = corner_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n\n    ![box-corner-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/box-corner-annotator-example-purple.png)\n    \"\"\"\n    for detection_idx in range(len(detections)):\n        x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        corners = [(x1, y1), (x2, y1), (x1, y2), (x2, y2)]\n\n        for x, y in corners:\n            x_end = x + self.corner_length if x == x1 else x - self.corner_length\n            cv2.line(\n                scene, (x, y), (x_end, y), color.as_bgr(), thickness=self.thickness\n            )\n\n            y_end = y + self.corner_length if y == y1 else y - self.corner_length\n            cv2.line(\n                scene, (x, y), (x, y_end), color.as_bgr(), thickness=self.thickness\n            )\n    return scene\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.OrientedBoxAnnotator-functions","title":"Functions","text":""},{"location":"detection/annotators/#supervision.annotators.core.OrientedBoxAnnotator.__init__","title":"<code>__init__(color=ColorPalette.DEFAULT, thickness=2, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>DEFAULT</code> <code>thickness</code> <code>int</code> <p>Thickness of the bounding box lines.</p> <code>2</code> <code>color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n    thickness: int = 2,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        thickness (int): Thickness of the bounding box lines.\n        color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.thickness: int = thickness\n    self.color_lookup: ColorLookup = color_lookup\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.OrientedBoxAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with oriented bounding boxes based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where bounding boxes will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nmodel = YOLO(\"yolov8n-obb.pt\")\n\nresult = model(image)[0]\ndetections = sv.Detections.from_ultralytics(result)\n\noriented_box_annotator = sv.OrientedBoxAnnotator()\nannotated_frame = oriented_box_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> Source code in <code>supervision/annotators/core.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene with oriented bounding boxes based on the provided detections.\n\n    Args:\n        scene (ImageType): The image where bounding boxes will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import cv2\n        import supervision as sv\n        from ultralytics import YOLO\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        model = YOLO(\"yolov8n-obb.pt\")\n\n        result = model(image)[0]\n        detections = sv.Detections.from_ultralytics(result)\n\n        oriented_box_annotator = sv.OrientedBoxAnnotator()\n        annotated_frame = oriented_box_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n    \"\"\"  # noqa E501 // docs\n\n    if detections.data is None or ORIENTED_BOX_COORDINATES not in detections.data:\n        return scene\n\n    for detection_idx in range(len(detections)):\n        bbox = np.intp(detections.data.get(ORIENTED_BOX_COORDINATES)[detection_idx])\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n\n        cv2.drawContours(scene, [bbox], 0, color.as_bgr(), self.thickness)\n\n    return scene\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.ColorAnnotator-functions","title":"Functions","text":""},{"location":"detection/annotators/#supervision.annotators.core.ColorAnnotator.__init__","title":"<code>__init__(color=ColorPalette.DEFAULT, opacity=0.5, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>DEFAULT</code> <code>opacity</code> <code>float</code> <p>Opacity of the overlay mask. Must be between <code>0</code> and <code>1</code>.</p> <code>0.5</code> <code>color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n    opacity: float = 0.5,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        opacity (float): Opacity of the overlay mask. Must be between `0` and `1`.\n        color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.color_lookup: ColorLookup = color_lookup\n    self.opacity = opacity\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.ColorAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with box masks based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where bounding boxes will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\ncolor_annotator = sv.ColorAnnotator()\nannotated_frame = color_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene with box masks based on the provided detections.\n\n    Args:\n        scene (ImageType): The image where bounding boxes will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        detections = sv.Detections(...)\n\n        color_annotator = sv.ColorAnnotator()\n        annotated_frame = color_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n\n    ![box-mask-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/box-mask-annotator-example-purple.png)\n    \"\"\"\n    scene_with_boxes = scene.copy()\n    for detection_idx in range(len(detections)):\n        x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        cv2.rectangle(\n            img=scene_with_boxes,\n            pt1=(x1, y1),\n            pt2=(x2, y2),\n            color=color.as_bgr(),\n            thickness=-1,\n        )\n\n    cv2.addWeighted(\n        scene_with_boxes, self.opacity, scene, 1 - self.opacity, gamma=0, dst=scene\n    )\n    return scene\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.CircleAnnotator-functions","title":"Functions","text":""},{"location":"detection/annotators/#supervision.annotators.core.CircleAnnotator.__init__","title":"<code>__init__(color=ColorPalette.DEFAULT, thickness=2, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>DEFAULT</code> <code>thickness</code> <code>int</code> <p>Thickness of the circle line.</p> <code>2</code> <code>color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n    thickness: int = 2,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        thickness (int): Thickness of the circle line.\n        color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n\n    self.color: Union[Color, ColorPalette] = color\n    self.thickness: int = thickness\n    self.color_lookup: ColorLookup = color_lookup\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.CircleAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with circles based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where box corners will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\ncircle_annotator = sv.CircleAnnotator()\nannotated_frame = circle_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene with circles based on the provided detections.\n\n    Args:\n        scene (ImageType): The image where box corners will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        detections = sv.Detections(...)\n\n        circle_annotator = sv.CircleAnnotator()\n        annotated_frame = circle_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n\n\n    ![circle-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/circle-annotator-example-purple.png)\n    \"\"\"\n    for detection_idx in range(len(detections)):\n        x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n        center = ((x1 + x2) // 2, (y1 + y2) // 2)\n        distance = sqrt((x1 - center[0]) ** 2 + (y1 - center[1]) ** 2)\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        cv2.circle(\n            img=scene,\n            center=center,\n            radius=int(distance),\n            color=color.as_bgr(),\n            thickness=self.thickness,\n        )\n\n    return scene\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.DotAnnotator-functions","title":"Functions","text":""},{"location":"detection/annotators/#supervision.annotators.core.DotAnnotator.__init__","title":"<code>__init__(color=ColorPalette.DEFAULT, radius=4, position=Position.CENTER, color_lookup=ColorLookup.CLASS, outline_thickness=0, outline_color=Color.BLACK)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>DEFAULT</code> <code>radius</code> <code>int</code> <p>Radius of the drawn dots.</p> <code>4</code> <code>position</code> <code>Position</code> <p>The anchor position for placing the dot.</p> <code>CENTER</code> <code>color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> <code>outline_thickness</code> <code>int</code> <p>Thickness of the outline of the dot.</p> <code>0</code> <code>outline_color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for outline. It is activated by setting outline_thickness to a value greater than 0.</p> <code>BLACK</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n    radius: int = 4,\n    position: Position = Position.CENTER,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n    outline_thickness: int = 0,\n    outline_color: Union[Color, ColorPalette] = Color.BLACK,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        radius (int): Radius of the drawn dots.\n        position (Position): The anchor position for placing the dot.\n        color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n        outline_thickness (int): Thickness of the outline of the dot.\n        outline_color (Union[Color, ColorPalette]): The color or color palette to\n            use for outline. It is activated by setting outline_thickness to a value\n            greater than 0.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.radius: int = radius\n    self.position: Position = position\n    self.color_lookup: ColorLookup = color_lookup\n    self.outline_thickness = outline_thickness\n    self.outline_color: Union[Color, ColorPalette] = outline_color\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.DotAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with dots based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where dots will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\ndot_annotator = sv.DotAnnotator()\nannotated_frame = dot_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene with dots based on the provided detections.\n\n    Args:\n        scene (ImageType): The image where dots will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        detections = sv.Detections(...)\n\n        dot_annotator = sv.DotAnnotator()\n        annotated_frame = dot_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n\n    ![dot-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/dot-annotator-example-purple.png)\n    \"\"\"\n    xy = detections.get_anchors_coordinates(anchor=self.position)\n    for detection_idx in range(len(detections)):\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        center = (int(xy[detection_idx, 0]), int(xy[detection_idx, 1]))\n\n        cv2.circle(scene, center, self.radius, color.as_bgr(), -1)\n        if self.outline_thickness:\n            outline_color = resolve_color(\n                color=self.outline_color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            cv2.circle(\n                scene,\n                center,\n                self.radius,\n                outline_color.as_bgr(),\n                self.outline_thickness,\n            )\n    return scene\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.TriangleAnnotator-functions","title":"Functions","text":""},{"location":"detection/annotators/#supervision.annotators.core.TriangleAnnotator.__init__","title":"<code>__init__(color=ColorPalette.DEFAULT, base=10, height=10, position=Position.TOP_CENTER, color_lookup=ColorLookup.CLASS, outline_thickness=0, outline_color=Color.BLACK)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>DEFAULT</code> <code>base</code> <code>int</code> <p>The base width of the triangle.</p> <code>10</code> <code>height</code> <code>int</code> <p>The height of the triangle.</p> <code>10</code> <code>position</code> <code>Position</code> <p>The anchor position for placing the triangle.</p> <code>TOP_CENTER</code> <code>color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> <code>outline_thickness</code> <code>int</code> <p>Thickness of the outline of the triangle.</p> <code>0</code> <code>outline_color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for outline. It is activated by setting outline_thickness to a value greater than 0.</p> <code>BLACK</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n    base: int = 10,\n    height: int = 10,\n    position: Position = Position.TOP_CENTER,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n    outline_thickness: int = 0,\n    outline_color: Union[Color, ColorPalette] = Color.BLACK,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        base (int): The base width of the triangle.\n        height (int): The height of the triangle.\n        position (Position): The anchor position for placing the triangle.\n        color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n        outline_thickness (int): Thickness of the outline of the triangle.\n        outline_color (Union[Color, ColorPalette]): The color or color palette to\n            use for outline. It is activated by setting outline_thickness to a value\n            greater than 0.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.base: int = base\n    self.height: int = height\n    self.position: Position = position\n    self.color_lookup: ColorLookup = color_lookup\n    self.outline_thickness: int = outline_thickness\n    self.outline_color: Union[Color, ColorPalette] = outline_color\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.TriangleAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with triangles based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where triangles will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\ntriangle_annotator = sv.TriangleAnnotator()\nannotated_frame = triangle_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene with triangles based on the provided detections.\n\n    Args:\n        scene (ImageType): The image where triangles will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        detections = sv.Detections(...)\n\n        triangle_annotator = sv.TriangleAnnotator()\n        annotated_frame = triangle_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n\n    ![triangle-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/triangle-annotator-example.png)\n    \"\"\"\n    xy = detections.get_anchors_coordinates(anchor=self.position)\n    for detection_idx in range(len(detections)):\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        tip_x, tip_y = int(xy[detection_idx, 0]), int(xy[detection_idx, 1])\n        vertices = np.array(\n            [\n                [tip_x - self.base // 2, tip_y - self.height],\n                [tip_x + self.base // 2, tip_y - self.height],\n                [tip_x, tip_y],\n            ],\n            np.int32,\n        )\n\n        cv2.fillPoly(scene, [vertices], color.as_bgr())\n        if self.outline_thickness:\n            outline_color = resolve_color(\n                color=self.outline_color,\n                detections=detections,\n                detection_idx=detection_idx,\n                color_lookup=self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup,\n            )\n            cv2.polylines(\n                scene,\n                [vertices],\n                True,\n                outline_color.as_bgr(),\n                thickness=self.outline_thickness,\n            )\n    return scene\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.EllipseAnnotator-functions","title":"Functions","text":""},{"location":"detection/annotators/#supervision.annotators.core.EllipseAnnotator.__init__","title":"<code>__init__(color=ColorPalette.DEFAULT, thickness=2, start_angle=-45, end_angle=235, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>DEFAULT</code> <code>thickness</code> <code>int</code> <p>Thickness of the ellipse lines.</p> <code>2</code> <code>start_angle</code> <code>int</code> <p>Starting angle of the ellipse.</p> <code>-45</code> <code>end_angle</code> <code>int</code> <p>Ending angle of the ellipse.</p> <code>235</code> <code>color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n    thickness: int = 2,\n    start_angle: int = -45,\n    end_angle: int = 235,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        thickness (int): Thickness of the ellipse lines.\n        start_angle (int): Starting angle of the ellipse.\n        end_angle (int): Ending angle of the ellipse.\n        color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.thickness: int = thickness\n    self.start_angle: int = start_angle\n    self.end_angle: int = end_angle\n    self.color_lookup: ColorLookup = color_lookup\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.EllipseAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with ellipses based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where ellipses will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nellipse_annotator = sv.EllipseAnnotator()\nannotated_frame = ellipse_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene with ellipses based on the provided detections.\n\n    Args:\n        scene (ImageType): The image where ellipses will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        detections = sv.Detections(...)\n\n        ellipse_annotator = sv.EllipseAnnotator()\n        annotated_frame = ellipse_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n\n    ![ellipse-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/ellipse-annotator-example-purple.png)\n    \"\"\"\n    for detection_idx in range(len(detections)):\n        x1, y1, x2, y2 = detections.xyxy[detection_idx].astype(int)\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        center = (int((x1 + x2) / 2), y2)\n        width = x2 - x1\n        cv2.ellipse(\n            scene,\n            center=center,\n            axes=(int(width), int(0.35 * width)),\n            angle=0.0,\n            startAngle=self.start_angle,\n            endAngle=self.end_angle,\n            color=color.as_bgr(),\n            thickness=self.thickness,\n            lineType=cv2.LINE_4,\n        )\n    return scene\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.HaloAnnotator-functions","title":"Functions","text":""},{"location":"detection/annotators/#supervision.annotators.core.HaloAnnotator.__init__","title":"<code>__init__(color=ColorPalette.DEFAULT, opacity=0.8, kernel_size=40, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>DEFAULT</code> <code>opacity</code> <code>float</code> <p>Opacity of the overlay mask. Must be between <code>0</code> and <code>1</code>.</p> <code>0.8</code> <code>kernel_size</code> <code>int</code> <p>The size of the average pooling kernel used for creating the halo.</p> <code>40</code> <code>color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n    opacity: float = 0.8,\n    kernel_size: int = 40,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        opacity (float): Opacity of the overlay mask. Must be between `0` and `1`.\n        kernel_size (int): The size of the average pooling kernel used for creating\n            the halo.\n        color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.opacity = opacity\n    self.color_lookup: ColorLookup = color_lookup\n    self.kernel_size: int = kernel_size\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.HaloAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with halos based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where masks will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nhalo_annotator = sv.HaloAnnotator()\nannotated_frame = halo_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene with halos based on the provided detections.\n\n    Args:\n        scene (ImageType): The image where masks will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        detections = sv.Detections(...)\n\n        halo_annotator = sv.HaloAnnotator()\n        annotated_frame = halo_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n\n    ![halo-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/halo-annotator-example-purple.png)\n    \"\"\"\n    if detections.mask is None:\n        return scene\n    colored_mask = np.zeros_like(scene, dtype=np.uint8)\n    fmask = np.array([False] * scene.shape[0] * scene.shape[1]).reshape(\n        scene.shape[0], scene.shape[1]\n    )\n\n    for detection_idx in np.flip(np.argsort(detections.area)):\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        mask = detections.mask[detection_idx]\n        fmask = np.logical_or(fmask, mask)\n        color_bgr = color.as_bgr()\n        colored_mask[mask] = color_bgr\n\n    colored_mask = cv2.blur(colored_mask, (self.kernel_size, self.kernel_size))\n    colored_mask[fmask] = [0, 0, 0]\n    gray = cv2.cvtColor(colored_mask, cv2.COLOR_BGR2GRAY)\n    alpha = self.opacity * gray / gray.max()\n    alpha_mask = alpha[:, :, np.newaxis]\n    blended_scene = np.uint8(scene * (1 - alpha_mask) + colored_mask * self.opacity)\n    np.copyto(scene, blended_scene)\n    return scene\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.PercentageBarAnnotator-functions","title":"Functions","text":""},{"location":"detection/annotators/#supervision.annotators.core.PercentageBarAnnotator.__init__","title":"<code>__init__(height=16, width=80, color=ColorPalette.DEFAULT, border_color=Color.BLACK, position=Position.TOP_CENTER, color_lookup=ColorLookup.CLASS, border_thickness=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>The height in pixels of the percentage bar.</p> <code>16</code> <code>width</code> <code>int</code> <p>The width in pixels of the percentage bar.</p> <code>80</code> <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>DEFAULT</code> <code>border_color</code> <code>Color</code> <p>The color of the border lines.</p> <code>BLACK</code> <code>position</code> <code>Position</code> <p>The anchor position of drawing the percentage bar.</p> <code>TOP_CENTER</code> <code>color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> <code>border_thickness</code> <code>int</code> <p>The thickness of the border lines.</p> <code>None</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    height: int = 16,\n    width: int = 80,\n    color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n    border_color: Color = Color.BLACK,\n    position: Position = Position.TOP_CENTER,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n    border_thickness: int = None,\n):\n    \"\"\"\n    Args:\n        height (int): The height in pixels of the percentage bar.\n        width (int): The width in pixels of the percentage bar.\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        border_color (Color): The color of the border lines.\n        position (Position): The anchor position of drawing the percentage bar.\n        color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n        border_thickness (int): The thickness of the border lines.\n    \"\"\"\n    self.height: int = height\n    self.width: int = width\n    self.color: Union[Color, ColorPalette] = color\n    self.border_color: Color = border_color\n    self.position: Position = position\n    self.color_lookup: ColorLookup = color_lookup\n\n    if border_thickness is None:\n        self.border_thickness = int(0.15 * self.height)\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.PercentageBarAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None, custom_values=None)</code>","text":"<p>Annotates the given scene with percentage bars based on the provided detections. The percentage bars visually represent the confidence or custom values associated with each detection.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where percentage bars will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <code>custom_values</code> <code>Optional[ndarray]</code> <p>Custom values array to use instead of the default detection confidences. This array should have the same length as the number of detections and contain a value between 0 and 1 (inclusive) for each detection, representing the percentage to be displayed.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\npercentage_bar_annotator = sv.PercentageBarAnnotator()\nannotated_frame = percentage_bar_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n    custom_values: Optional[np.ndarray] = None,\n) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene with percentage bars based on the provided\n    detections. The percentage bars visually represent the confidence or custom\n    values associated with each detection.\n\n    Args:\n        scene (ImageType): The image where percentage bars will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n        custom_values (Optional[np.ndarray]): Custom values array to use instead\n            of the default detection confidences. This array should have the\n            same length as the number of detections and contain a value between\n            0 and 1 (inclusive) for each detection, representing the percentage\n            to be displayed.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        detections = sv.Detections(...)\n\n        percentage_bar_annotator = sv.PercentageBarAnnotator()\n        annotated_frame = percentage_bar_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n\n    ![percentage-bar-example](https://media.roboflow.com/\n    supervision-annotator-examples/percentage-bar-annotator-example-purple.png)\n    \"\"\"\n    self.validate_custom_values(\n        custom_values=custom_values, detections_count=len(detections)\n    )\n    anchors = detections.get_anchors_coordinates(anchor=self.position)\n    for detection_idx in range(len(detections)):\n        anchor = anchors[detection_idx]\n        border_coordinates = self.calculate_border_coordinates(\n            anchor_xy=(int(anchor[0]), int(anchor[1])),\n            border_wh=(self.width, self.height),\n            position=self.position,\n        )\n        border_width = border_coordinates[1][0] - border_coordinates[0][0]\n\n        value = (\n            custom_values[detection_idx]\n            if custom_values is not None\n            else detections.confidence[detection_idx]\n        )\n\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        cv2.rectangle(\n            img=scene,\n            pt1=border_coordinates[0],\n            pt2=(\n                border_coordinates[0][0] + int(border_width * value),\n                border_coordinates[1][1],\n            ),\n            color=color.as_bgr(),\n            thickness=-1,\n        )\n        cv2.rectangle(\n            img=scene,\n            pt1=border_coordinates[0],\n            pt2=border_coordinates[1],\n            color=self.border_color.as_bgr(),\n            thickness=self.border_thickness,\n        )\n    return scene\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.HeatMapAnnotator-functions","title":"Functions","text":""},{"location":"detection/annotators/#supervision.annotators.core.HeatMapAnnotator.__init__","title":"<code>__init__(position=Position.BOTTOM_CENTER, opacity=0.2, radius=40, kernel_size=25, top_hue=0, low_hue=125)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>position</code> <code>Position</code> <p>The position of the heatmap. Defaults to <code>BOTTOM_CENTER</code>.</p> <code>BOTTOM_CENTER</code> <code>opacity</code> <code>float</code> <p>Opacity of the overlay mask, between 0 and 1.</p> <code>0.2</code> <code>radius</code> <code>int</code> <p>Radius of the heat circle.</p> <code>40</code> <code>kernel_size</code> <code>int</code> <p>Kernel size for blurring the heatmap.</p> <code>25</code> <code>top_hue</code> <code>int</code> <p>Hue at the top of the heatmap. Defaults to 0 (red).</p> <code>0</code> <code>low_hue</code> <code>int</code> <p>Hue at the bottom of the heatmap. Defaults to 125 (blue).</p> <code>125</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    position: Position = Position.BOTTOM_CENTER,\n    opacity: float = 0.2,\n    radius: int = 40,\n    kernel_size: int = 25,\n    top_hue: int = 0,\n    low_hue: int = 125,\n):\n    \"\"\"\n    Args:\n        position (Position): The position of the heatmap. Defaults to\n            `BOTTOM_CENTER`.\n        opacity (float): Opacity of the overlay mask, between 0 and 1.\n        radius (int): Radius of the heat circle.\n        kernel_size (int): Kernel size for blurring the heatmap.\n        top_hue (int): Hue at the top of the heatmap. Defaults to 0 (red).\n        low_hue (int): Hue at the bottom of the heatmap. Defaults to 125 (blue).\n    \"\"\"\n    self.position = position\n    self.opacity = opacity\n    self.radius = radius\n    self.kernel_size = kernel_size\n    self.heat_mask = None\n    self.top_hue = top_hue\n    self.low_hue = low_hue\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.HeatMapAnnotator.annotate","title":"<code>annotate(scene, detections)</code>","text":"<p>Annotates the scene with a heatmap based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where the heatmap will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8x.pt')\n\nheat_map_annotator = sv.HeatMapAnnotator()\n\nvideo_info = sv.VideoInfo.from_video_path(video_path='...')\nframes_generator = get_video_frames_generator(source_path='...')\n\nwith sv.VideoSink(target_path='...', video_info=video_info) as sink:\n   for frame in frames_generator:\n       result = model(frame)[0]\n       detections = sv.Detections.from_ultralytics(result)\n       annotated_frame = heat_map_annotator.annotate(\n           scene=frame.copy(),\n           detections=detections)\n       sink.write_frame(frame=annotated_frame)\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(self, scene: ImageType, detections: Detections) -&gt; ImageType:\n    \"\"\"\n    Annotates the scene with a heatmap based on the provided detections.\n\n    Args:\n        scene (ImageType): The image where the heatmap will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n        from ultralytics import YOLO\n\n        model = YOLO('yolov8x.pt')\n\n        heat_map_annotator = sv.HeatMapAnnotator()\n\n        video_info = sv.VideoInfo.from_video_path(video_path='...')\n        frames_generator = get_video_frames_generator(source_path='...')\n\n        with sv.VideoSink(target_path='...', video_info=video_info) as sink:\n           for frame in frames_generator:\n               result = model(frame)[0]\n               detections = sv.Detections.from_ultralytics(result)\n               annotated_frame = heat_map_annotator.annotate(\n                   scene=frame.copy(),\n                   detections=detections)\n               sink.write_frame(frame=annotated_frame)\n        ```\n\n    ![heatmap-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/heat-map-annotator-example-purple.png)\n    \"\"\"\n\n    if self.heat_mask is None:\n        self.heat_mask = np.zeros(scene.shape[:2])\n    mask = np.zeros(scene.shape[:2])\n    for xy in detections.get_anchors_coordinates(self.position):\n        cv2.circle(mask, (int(xy[0]), int(xy[1])), self.radius, 1, -1)\n    self.heat_mask = mask + self.heat_mask\n    temp = self.heat_mask.copy()\n    temp = self.low_hue - temp / temp.max() * (self.low_hue - self.top_hue)\n    temp = temp.astype(np.uint8)\n    if self.kernel_size is not None:\n        temp = cv2.blur(temp, (self.kernel_size, self.kernel_size))\n    hsv = np.zeros(scene.shape)\n    hsv[..., 0] = temp\n    hsv[..., 1] = 255\n    hsv[..., 2] = 255\n    temp = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2BGR)\n    mask = cv2.cvtColor(self.heat_mask.astype(np.uint8), cv2.COLOR_GRAY2BGR) &gt; 0\n    scene[mask] = cv2.addWeighted(temp, self.opacity, scene, 1 - self.opacity, 0)[\n        mask\n    ]\n    return scene\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.MaskAnnotator-functions","title":"Functions","text":""},{"location":"detection/annotators/#supervision.annotators.core.MaskAnnotator.__init__","title":"<code>__init__(color=ColorPalette.DEFAULT, opacity=0.5, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>DEFAULT</code> <code>opacity</code> <code>float</code> <p>Opacity of the overlay mask. Must be between <code>0</code> and <code>1</code>.</p> <code>0.5</code> <code>color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n    opacity: float = 0.5,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        opacity (float): Opacity of the overlay mask. Must be between `0` and `1`.\n        color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.opacity = opacity\n    self.color_lookup: ColorLookup = color_lookup\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.MaskAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with masks based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where masks will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nmask_annotator = sv.MaskAnnotator()\nannotated_frame = mask_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene with masks based on the provided detections.\n\n    Args:\n        scene (ImageType): The image where masks will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        detections = sv.Detections(...)\n\n        mask_annotator = sv.MaskAnnotator()\n        annotated_frame = mask_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n\n    ![mask-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/mask-annotator-example-purple.png)\n    \"\"\"\n    if detections.mask is None:\n        return scene\n\n    colored_mask = np.array(scene, copy=True, dtype=np.uint8)\n\n    for detection_idx in np.flip(np.argsort(detections.area)):\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        mask = detections.mask[detection_idx]\n        colored_mask[mask] = color.as_bgr()\n\n    cv2.addWeighted(\n        colored_mask, self.opacity, scene, 1 - self.opacity, 0, dst=scene\n    )\n    return scene\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.PolygonAnnotator-functions","title":"Functions","text":""},{"location":"detection/annotators/#supervision.annotators.core.PolygonAnnotator.__init__","title":"<code>__init__(color=ColorPalette.DEFAULT, thickness=2, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating detections.</p> <code>DEFAULT</code> <code>thickness</code> <code>int</code> <p>Thickness of the polygon lines.</p> <code>2</code> <code>color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n    thickness: int = 2,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating detections.\n        thickness (int): Thickness of the polygon lines.\n        color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.thickness: int = thickness\n    self.color_lookup: ColorLookup = color_lookup\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.PolygonAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with polygons based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where polygons will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\npolygon_annotator = sv.PolygonAnnotator()\nannotated_frame = polygon_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene with polygons based on the provided detections.\n\n    Args:\n        scene (ImageType): The image where polygons will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        detections = sv.Detections(...)\n\n        polygon_annotator = sv.PolygonAnnotator()\n        annotated_frame = polygon_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n\n    ![polygon-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/polygon-annotator-example-purple.png)\n    \"\"\"\n    if detections.mask is None:\n        return scene\n\n    for detection_idx in range(len(detections)):\n        mask = detections.mask[detection_idx]\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        for polygon in mask_to_polygons(mask=mask):\n            scene = draw_polygon(\n                scene=scene,\n                polygon=polygon,\n                color=color,\n                thickness=self.thickness,\n            )\n\n    return scene\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.LabelAnnotator-functions","title":"Functions","text":""},{"location":"detection/annotators/#supervision.annotators.core.LabelAnnotator.__init__","title":"<code>__init__(color=ColorPalette.DEFAULT, text_color=Color.WHITE, text_scale=0.5, text_thickness=1, text_padding=10, text_position=Position.TOP_LEFT, color_lookup=ColorLookup.CLASS, border_radius=0)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating the text background.</p> <code>DEFAULT</code> <code>text_color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for the text.</p> <code>WHITE</code> <code>text_scale</code> <code>float</code> <p>Font scale for the text.</p> <code>0.5</code> <code>text_thickness</code> <code>int</code> <p>Thickness of the text characters.</p> <code>1</code> <code>text_padding</code> <code>int</code> <p>Padding around the text within its background box.</p> <code>10</code> <code>text_position</code> <code>Position</code> <p>Position of the text relative to the detection. Possible values are defined in the <code>Position</code> enum.</p> <code>TOP_LEFT</code> <code>color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> <code>border_radius</code> <code>int</code> <p>The radius to apply round edges. If the selected value is higher than the lower dimension, width or height, is clipped.</p> <code>0</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n    text_color: Union[Color, ColorPalette] = Color.WHITE,\n    text_scale: float = 0.5,\n    text_thickness: int = 1,\n    text_padding: int = 10,\n    text_position: Position = Position.TOP_LEFT,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n    border_radius: int = 0,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating the text background.\n        text_color (Union[Color, ColorPalette]): The color or color palette to use\n            for the text.\n        text_scale (float): Font scale for the text.\n        text_thickness (int): Thickness of the text characters.\n        text_padding (int): Padding around the text within its background box.\n        text_position (Position): Position of the text relative to the detection.\n            Possible values are defined in the `Position` enum.\n        color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n        border_radius (int): The radius to apply round edges. If the selected\n            value is higher than the lower dimension, width or height, is clipped.\n    \"\"\"\n    self.border_radius: int = border_radius\n    self.color: Union[Color, ColorPalette] = color\n    self.text_color: Union[Color, ColorPalette] = text_color\n    self.text_scale: float = text_scale\n    self.text_thickness: int = text_thickness\n    self.text_padding: int = text_padding\n    self.text_anchor: Position = text_position\n    self.color_lookup: ColorLookup = color_lookup\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.LabelAnnotator.annotate","title":"<code>annotate(scene, detections, labels=None, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with labels based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where labels will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>labels</code> <code>Optional[List[str]]</code> <p>Custom labels for each detection.</p> <code>None</code> <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nlabels = [\n    f\"{class_name} {confidence:.2f}\"\n    for class_name, confidence\n    in zip(detections['class_name'], detections.confidence)\n]\n\nlabel_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\nannotated_frame = label_annotator.annotate(\n    scene=image.copy(),\n    detections=detections,\n    labels=labels\n)\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n    labels: Optional[List[str]] = None,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene with labels based on the provided detections.\n\n    Args:\n        scene (ImageType): The image where labels will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n        labels (Optional[List[str]]): Custom labels for each detection.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        detections = sv.Detections(...)\n\n        labels = [\n            f\"{class_name} {confidence:.2f}\"\n            for class_name, confidence\n            in zip(detections['class_name'], detections.confidence)\n        ]\n\n        label_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n        annotated_frame = label_annotator.annotate(\n            scene=image.copy(),\n            detections=detections,\n            labels=labels\n        )\n        ```\n\n    ![label-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/label-annotator-example-purple.png)\n    \"\"\"\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    anchors_coordinates = detections.get_anchors_coordinates(\n        anchor=self.text_anchor\n    ).astype(int)\n    if labels is not None and len(labels) != len(detections):\n        raise ValueError(\n            f\"The number of labels ({len(labels)}) does not match the \"\n            f\"number of detections ({len(detections)}). Each detection \"\n            f\"should have exactly 1 label.\"\n        )\n\n    for detection_idx, center_coordinates in enumerate(anchors_coordinates):\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=(\n                self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup\n            ),\n        )\n\n        text_color = resolve_color(\n            color=self.text_color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=(\n                self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup\n            ),\n        )\n\n        if labels is not None:\n            text = labels[detection_idx]\n        elif detections[CLASS_NAME_DATA_FIELD] is not None:\n            text = detections[CLASS_NAME_DATA_FIELD][detection_idx]\n        elif detections.class_id is not None:\n            text = str(detections.class_id[detection_idx])\n        else:\n            text = str(detection_idx)\n\n        text_w, text_h = cv2.getTextSize(\n            text=text,\n            fontFace=font,\n            fontScale=self.text_scale,\n            thickness=self.text_thickness,\n        )[0]\n        text_w_padded = text_w + 2 * self.text_padding\n        text_h_padded = text_h + 2 * self.text_padding\n        text_background_xyxy = resolve_text_background_xyxy(\n            center_coordinates=tuple(center_coordinates),\n            text_wh=(text_w_padded, text_h_padded),\n            position=self.text_anchor,\n        )\n\n        text_x = text_background_xyxy[0] + self.text_padding\n        text_y = text_background_xyxy[1] + self.text_padding + text_h\n\n        self.draw_rounded_rectangle(\n            scene=scene,\n            xyxy=text_background_xyxy,\n            color=color.as_bgr(),\n            border_radius=self.border_radius,\n        )\n        cv2.putText(\n            img=scene,\n            text=text,\n            org=(text_x, text_y),\n            fontFace=font,\n            fontScale=self.text_scale,\n            color=text_color.as_bgr(),\n            thickness=self.text_thickness,\n            lineType=cv2.LINE_AA,\n        )\n    return scene\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.RichLabelAnnotator-functions","title":"Functions","text":""},{"location":"detection/annotators/#supervision.annotators.core.RichLabelAnnotator.__init__","title":"<code>__init__(color=ColorPalette.DEFAULT, text_color=Color.WHITE, font_path=None, font_size=10, text_padding=10, text_position=Position.TOP_LEFT, color_lookup=ColorLookup.CLASS, border_radius=0)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating the text background.</p> <code>DEFAULT</code> <code>text_color</code> <code>Union[Color, ColorPalette]</code> <p>The color to use for the text.</p> <code>WHITE</code> <code>font_path</code> <code>Optional[str]</code> <p>Path to the font file (e.g., \".ttf\" or \".otf\") to use for rendering text. If <code>None</code>, the default PIL font will be used.</p> <code>None</code> <code>font_size</code> <code>int</code> <p>Font size for the text.</p> <code>10</code> <code>text_padding</code> <code>int</code> <p>Padding around the text within its background box.</p> <code>10</code> <code>text_position</code> <code>Position</code> <p>Position of the text relative to the detection. Possible values are defined in the <code>Position</code> enum.</p> <code>TOP_LEFT</code> <code>color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> <code>border_radius</code> <code>int</code> <p>The radius to apply round edges. If the selected value is higher than the lower dimension, width or height, is clipped.</p> <code>0</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n    text_color: Union[Color, ColorPalette] = Color.WHITE,\n    font_path: Optional[str] = None,\n    font_size: int = 10,\n    text_padding: int = 10,\n    text_position: Position = Position.TOP_LEFT,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n    border_radius: int = 0,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color or color palette to use for\n            annotating the text background.\n        text_color (Union[Color, ColorPalette]): The color to use for the text.\n        font_path (Optional[str]): Path to the font file (e.g., \".ttf\" or \".otf\")\n            to use for rendering text. If `None`, the default PIL font will be used.\n        font_size (int): Font size for the text.\n        text_padding (int): Padding around the text within its background box.\n        text_position (Position): Position of the text relative to the detection.\n            Possible values are defined in the `Position` enum.\n        color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n        border_radius (int): The radius to apply round edges. If the selected\n            value is higher than the lower dimension, width or height, is clipped.\n    \"\"\"\n    self.color = color\n    self.text_color = text_color\n    self.text_padding = text_padding\n    self.text_anchor = text_position\n    self.color_lookup = color_lookup\n    self.border_radius = border_radius\n    if font_path is not None:\n        try:\n            self.font = ImageFont.truetype(font_path, font_size)\n        except OSError:\n            print(f\"Font path '{font_path}' not found. Using PIL's default font.\")\n            self.font = self._load_default_font(font_size)\n    else:\n        self.font = self._load_default_font(font_size)\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.RichLabelAnnotator.annotate","title":"<code>annotate(scene, detections, labels=None, custom_color_lookup=None)</code>","text":"<p>Annotates the given scene with labels based on the provided detections, with support for Unicode characters.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where labels will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>labels</code> <code>Optional[List[str]]</code> <p>Custom labels for each detection.</p> <code>None</code> <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nlabels = [\n    f\"{class_name} {confidence:.2f}\"\n    for class_name, confidence\n    in zip(detections['class_name'], detections.confidence)\n]\n\nrich_label_annotator = sv.RichLabelAnnotator(font_path=\"path/to/font.ttf\")\nannotated_frame = label_annotator.annotate(\n    scene=image.copy(),\n    detections=detections,\n    labels=labels\n)\n</code></pre> Source code in <code>supervision/annotators/core.py</code> <pre><code>@ensure_pil_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n    labels: Optional[List[str]] = None,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene with labels based on the provided\n    detections, with support for Unicode characters.\n\n    Args:\n        scene (ImageType): The image where labels will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n        labels (Optional[List[str]]): Custom labels for each detection.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        detections = sv.Detections(...)\n\n        labels = [\n            f\"{class_name} {confidence:.2f}\"\n            for class_name, confidence\n            in zip(detections['class_name'], detections.confidence)\n        ]\n\n        rich_label_annotator = sv.RichLabelAnnotator(font_path=\"path/to/font.ttf\")\n        annotated_frame = label_annotator.annotate(\n            scene=image.copy(),\n            detections=detections,\n            labels=labels\n        )\n        ```\n\n    \"\"\"\n    draw = ImageDraw.Draw(scene)\n    anchors_coordinates = detections.get_anchors_coordinates(\n        anchor=self.text_anchor\n    ).astype(int)\n    if labels is not None and len(labels) != len(detections):\n        raise ValueError(\n            f\"The number of labels provided ({len(labels)}) does not match the \"\n            f\"number of detections ({len(detections)}). Each detection should have \"\n            f\"a corresponding label.\"\n        )\n    for detection_idx, center_coordinates in enumerate(anchors_coordinates):\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=(\n                self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup\n            ),\n        )\n\n        text_color = resolve_color(\n            color=self.text_color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=(\n                self.color_lookup\n                if custom_color_lookup is None\n                else custom_color_lookup\n            ),\n        )\n\n        if labels is not None:\n            text = labels[detection_idx]\n        elif detections[CLASS_NAME_DATA_FIELD] is not None:\n            text = detections[CLASS_NAME_DATA_FIELD][detection_idx]\n        elif detections.class_id is not None:\n            text = str(detections.class_id[detection_idx])\n        else:\n            text = str(detection_idx)\n\n        left, top, right, bottom = draw.textbbox((0, 0), text, font=self.font)\n        text_width = right - left\n        text_height = bottom - top\n        text_w_padded = text_width + 2 * self.text_padding\n        text_h_padded = text_height + 2 * self.text_padding\n        text_background_xyxy = resolve_text_background_xyxy(\n            center_coordinates=tuple(center_coordinates),\n            text_wh=(text_w_padded, text_h_padded),\n            position=self.text_anchor,\n        )\n\n        text_x = text_background_xyxy[0] + self.text_padding - left\n        text_y = text_background_xyxy[1] + self.text_padding - top\n\n        draw.rounded_rectangle(\n            text_background_xyxy,\n            radius=self.border_radius,\n            fill=color.as_rgb(),\n            outline=None,\n        )\n        draw.text(\n            xy=(text_x, text_y),\n            text=text,\n            font=self.font,\n            fill=text_color.as_rgb(),\n        )\n    return scene\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.BlurAnnotator-functions","title":"Functions","text":""},{"location":"detection/annotators/#supervision.annotators.core.BlurAnnotator.__init__","title":"<code>__init__(kernel_size=15)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>kernel_size</code> <code>int</code> <p>The size of the average pooling kernel used for blurring.</p> <code>15</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(self, kernel_size: int = 15):\n    \"\"\"\n    Args:\n        kernel_size (int): The size of the average pooling kernel used for blurring.\n    \"\"\"\n    self.kernel_size: int = kernel_size\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.BlurAnnotator.annotate","title":"<code>annotate(scene, detections)</code>","text":"<p>Annotates the given scene by blurring regions based on the provided detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where blurring will be applied. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nblur_annotator = sv.BlurAnnotator()\nannotated_frame = circle_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene by blurring regions based on the provided detections.\n\n    Args:\n        scene (ImageType): The image where blurring will be applied.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        detections = sv.Detections(...)\n\n        blur_annotator = sv.BlurAnnotator()\n        annotated_frame = circle_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n\n    ![blur-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/blur-annotator-example-purple.png)\n    \"\"\"\n    image_height, image_width = scene.shape[:2]\n    clipped_xyxy = clip_boxes(\n        xyxy=detections.xyxy, resolution_wh=(image_width, image_height)\n    ).astype(int)\n\n    for x1, y1, x2, y2 in clipped_xyxy:\n        roi = scene[y1:y2, x1:x2]\n        roi = cv2.blur(roi, (self.kernel_size, self.kernel_size))\n        scene[y1:y2, x1:x2] = roi\n\n    return scene\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.PixelateAnnotator-functions","title":"Functions","text":""},{"location":"detection/annotators/#supervision.annotators.core.PixelateAnnotator.__init__","title":"<code>__init__(pixel_size=20)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>pixel_size</code> <code>int</code> <p>The size of the pixelation.</p> <code>20</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(self, pixel_size: int = 20):\n    \"\"\"\n    Args:\n        pixel_size (int): The size of the pixelation.\n    \"\"\"\n    self.pixel_size: int = pixel_size\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.PixelateAnnotator.annotate","title":"<code>annotate(scene, detections)</code>","text":"<p>Annotates the given scene by pixelating regions based on the provided     detections.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where pixelating will be applied. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\npixelate_annotator = sv.PixelateAnnotator()\nannotated_frame = pixelate_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene by pixelating regions based on the provided\n        detections.\n\n    Args:\n        scene (ImageType): The image where pixelating will be applied.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        detections = sv.Detections(...)\n\n        pixelate_annotator = sv.PixelateAnnotator()\n        annotated_frame = pixelate_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n\n    ![pixelate-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/pixelate-annotator-example-10.png)\n    \"\"\"\n    image_height, image_width = scene.shape[:2]\n    clipped_xyxy = clip_boxes(\n        xyxy=detections.xyxy, resolution_wh=(image_width, image_height)\n    ).astype(int)\n\n    for x1, y1, x2, y2 in clipped_xyxy:\n        roi = scene[y1:y2, x1:x2]\n        scaled_up_roi = cv2.resize(\n            src=roi, dsize=None, fx=1 / self.pixel_size, fy=1 / self.pixel_size\n        )\n        scaled_down_roi = cv2.resize(\n            src=scaled_up_roi,\n            dsize=(roi.shape[1], roi.shape[0]),\n            interpolation=cv2.INTER_NEAREST,\n        )\n\n        scene[y1:y2, x1:x2] = scaled_down_roi\n\n    return scene\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.TraceAnnotator-functions","title":"Functions","text":""},{"location":"detection/annotators/#supervision.annotators.core.TraceAnnotator.__init__","title":"<code>__init__(color=ColorPalette.DEFAULT, position=Position.CENTER, trace_length=30, thickness=2, color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, ColorPalette]</code> <p>The color to draw the trace, can be a single color or a color palette.</p> <code>DEFAULT</code> <code>position</code> <code>Position</code> <p>The position of the trace. Defaults to <code>CENTER</code>.</p> <code>CENTER</code> <code>trace_length</code> <code>int</code> <p>The maximum length of the trace in terms of historical points. Defaults to <code>30</code>.</p> <code>30</code> <code>thickness</code> <code>int</code> <p>The thickness of the trace lines. Defaults to <code>2</code>.</p> <code>2</code> <code>color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n    position: Position = Position.CENTER,\n    trace_length: int = 30,\n    thickness: int = 2,\n    color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, ColorPalette]): The color to draw the trace, can be\n            a single color or a color palette.\n        position (Position): The position of the trace.\n            Defaults to `CENTER`.\n        trace_length (int): The maximum length of the trace in terms of historical\n            points. Defaults to `30`.\n        thickness (int): The thickness of the trace lines. Defaults to `2`.\n        color_lookup (ColorLookup): Strategy for mapping colors to annotations.\n            Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.color: Union[Color, ColorPalette] = color\n    self.trace = Trace(max_size=trace_length, anchor=position)\n    self.thickness = thickness\n    self.color_lookup: ColorLookup = color_lookup\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.TraceAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Draws trace paths on the frame based on the detection coordinates provided.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image on which the traces will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>The detections which include coordinates for which the traces will be drawn.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8x.pt')\ntrace_annotator = sv.TraceAnnotator()\n\nvideo_info = sv.VideoInfo.from_video_path(video_path='...')\nframes_generator = sv.get_video_frames_generator(source_path='...')\ntracker = sv.ByteTrack()\n\nwith sv.VideoSink(target_path='...', video_info=video_info) as sink:\n   for frame in frames_generator:\n       result = model(frame)[0]\n       detections = sv.Detections.from_ultralytics(result)\n       detections = tracker.update_with_detections(detections)\n       annotated_frame = trace_annotator.annotate(\n           scene=frame.copy(),\n           detections=detections)\n       sink.write_frame(frame=annotated_frame)\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; ImageType:\n    \"\"\"\n    Draws trace paths on the frame based on the detection coordinates provided.\n\n    Args:\n        scene (ImageType): The image on which the traces will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): The detections which include coordinates for\n            which the traces will be drawn.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n        from ultralytics import YOLO\n\n        model = YOLO('yolov8x.pt')\n        trace_annotator = sv.TraceAnnotator()\n\n        video_info = sv.VideoInfo.from_video_path(video_path='...')\n        frames_generator = sv.get_video_frames_generator(source_path='...')\n        tracker = sv.ByteTrack()\n\n        with sv.VideoSink(target_path='...', video_info=video_info) as sink:\n           for frame in frames_generator:\n               result = model(frame)[0]\n               detections = sv.Detections.from_ultralytics(result)\n               detections = tracker.update_with_detections(detections)\n               annotated_frame = trace_annotator.annotate(\n                   scene=frame.copy(),\n                   detections=detections)\n               sink.write_frame(frame=annotated_frame)\n        ```\n\n    ![trace-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/trace-annotator-example-purple.png)\n    \"\"\"\n    self.trace.put(detections)\n\n    for detection_idx in range(len(detections)):\n        tracker_id = int(detections.tracker_id[detection_idx])\n        color = resolve_color(\n            color=self.color,\n            detections=detections,\n            detection_idx=detection_idx,\n            color_lookup=self.color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        xy = self.trace.get(tracker_id=tracker_id)\n        if len(xy) &gt; 1:\n            scene = cv2.polylines(\n                scene,\n                [xy.astype(np.int32)],\n                False,\n                color=color.as_bgr(),\n                thickness=self.thickness,\n            )\n    return scene\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.CropAnnotator-functions","title":"Functions","text":""},{"location":"detection/annotators/#supervision.annotators.core.CropAnnotator.__init__","title":"<code>__init__(position=Position.TOP_CENTER, scale_factor=2.0, border_color=ColorPalette.DEFAULT, border_thickness=2, border_color_lookup=ColorLookup.CLASS)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>position</code> <code>Position</code> <p>The anchor position for placing the cropped and scaled part of the detection in the scene.</p> <code>TOP_CENTER</code> <code>scale_factor</code> <code>float</code> <p>The factor by which to scale the cropped image part. A factor of 2, for example, would double the size of the cropped area, allowing for a closer view of the detection.</p> <code>2.0</code> <code>border_color</code> <code>Union[Color, ColorPalette]</code> <p>The color or color palette to use for annotating border around the cropped area.</p> <code>DEFAULT</code> <code>border_thickness</code> <code>int</code> <p>The thickness of the border around the cropped area.</p> <code>2</code> <code>border_color_lookup</code> <code>ColorLookup</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>.</p> <code>CLASS</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    position: Position = Position.TOP_CENTER,\n    scale_factor: float = 2.0,\n    border_color: Union[Color, ColorPalette] = ColorPalette.DEFAULT,\n    border_thickness: int = 2,\n    border_color_lookup: ColorLookup = ColorLookup.CLASS,\n):\n    \"\"\"\n    Args:\n        position (Position): The anchor position for placing the cropped and scaled\n            part of the detection in the scene.\n        scale_factor (float): The factor by which to scale the cropped image part. A\n            factor of 2, for example, would double the size of the cropped area,\n            allowing for a closer view of the detection.\n        border_color (Union[Color, ColorPalette]): The color or color palette to\n            use for annotating border around the cropped area.\n        border_thickness (int): The thickness of the border around the cropped area.\n        border_color_lookup (ColorLookup): Strategy for mapping colors to\n            annotations. Options are `INDEX`, `CLASS`, `TRACK`.\n    \"\"\"\n    self.position: Position = position\n    self.scale_factor: float = scale_factor\n    self.border_color: Union[Color, ColorPalette] = border_color\n    self.border_thickness: int = border_thickness\n    self.border_color_lookup: ColorLookup = border_color_lookup\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.CropAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None)</code>","text":"<p>Annotates the provided scene with scaled and cropped parts of the image based on the provided detections. Each detection is cropped from the original scene and scaled according to the annotator's scale factor before being placed back onto the scene at the specified position.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where cropped detection will be placed. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <code>custom_color_lookup</code> <code>Optional[ndarray]</code> <p>Custom color lookup array. Allows to override the default color mapping strategy.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image.</p> Example <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\ncrop_annotator = sv.CropAnnotator()\nannotated_frame = crop_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> Source code in <code>supervision/annotators/core.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(\n    self,\n    scene: ImageType,\n    detections: Detections,\n    custom_color_lookup: Optional[np.ndarray] = None,\n) -&gt; ImageType:\n    \"\"\"\n    Annotates the provided scene with scaled and cropped parts of the image based\n    on the provided detections. Each detection is cropped from the original scene\n    and scaled according to the annotator's scale factor before being placed back\n    onto the scene at the specified position.\n\n\n    Args:\n        scene (ImageType): The image where cropped detection will be placed.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n        custom_color_lookup (Optional[np.ndarray]): Custom color lookup array.\n            Allows to override the default color mapping strategy.\n\n    Returns:\n        The annotated image.\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        detections = sv.Detections(...)\n\n        crop_annotator = sv.CropAnnotator()\n        annotated_frame = crop_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n    \"\"\"\n    crops = [\n        crop_image(image=scene, xyxy=xyxy) for xyxy in detections.xyxy.astype(int)\n    ]\n    resized_crops = [\n        scale_image(image=crop, scale_factor=self.scale_factor) for crop in crops\n    ]\n    anchors = detections.get_anchors_coordinates(anchor=self.position).astype(int)\n\n    for idx, (resized_crop, anchor) in enumerate(zip(resized_crops, anchors)):\n        crop_wh = resized_crop.shape[1], resized_crop.shape[0]\n        (x1, y1), (x2, y2) = self.calculate_crop_coordinates(\n            anchor=anchor, crop_wh=crop_wh, position=self.position\n        )\n        scene = overlay_image(image=scene, overlay=resized_crop, anchor=(x1, y1))\n        color = resolve_color(\n            color=self.border_color,\n            detections=detections,\n            detection_idx=idx,\n            color_lookup=self.border_color_lookup\n            if custom_color_lookup is None\n            else custom_color_lookup,\n        )\n        cv2.rectangle(\n            img=scene,\n            pt1=(x1, y1),\n            pt2=(x2, y2),\n            color=color.as_bgr(),\n            thickness=self.border_thickness,\n        )\n\n    return scene\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.BackgroundOverlayAnnotator-functions","title":"Functions","text":""},{"location":"detection/annotators/#supervision.annotators.core.BackgroundOverlayAnnotator.__init__","title":"<code>__init__(color=Color.BLACK, opacity=0.5, force_box=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Color</code> <p>The color to use for annotating detections.</p> <code>BLACK</code> <code>opacity</code> <code>float</code> <p>Opacity of the overlay mask. Must be between <code>0</code> and <code>1</code>.</p> <code>0.5</code> <code>force_box</code> <code>bool</code> <p>If <code>True</code>, forces the annotator to use bounding boxes when masks are provided in the supplied sv.Detections.</p> <code>False</code> Source code in <code>supervision/annotators/core.py</code> <pre><code>def __init__(\n    self,\n    color: Color = Color.BLACK,\n    opacity: float = 0.5,\n    force_box: bool = False,\n):\n    \"\"\"\n    Args:\n        color (Color): The color to use for annotating detections.\n        opacity (float): Opacity of the overlay mask. Must be between `0` and `1`.\n        force_box (bool): If `True`, forces the annotator to use bounding boxes when\n            masks are provided in the supplied sv.Detections.\n    \"\"\"\n    self.color: Color = color\n    self.opacity = opacity\n    self.force_box = force_box\n</code></pre>"},{"location":"detection/annotators/#supervision.annotators.core.BackgroundOverlayAnnotator.annotate","title":"<code>annotate(scene, detections)</code>","text":"<p>Applies a colored overlay to the scene outside of the detected regions.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where masks will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>detections</code> <code>Detections</code> <p>Object detections to annotate.</p> required <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\ndetections = sv.Detections(...)\n\nbackground_overlay_annotator = sv.BackgroundOverlayAnnotator()\nannotated_frame = background_overlay_annotator.annotate(\n    scene=image.copy(),\n    detections=detections\n)\n</code></pre> <p></p> Source code in <code>supervision/annotators/core.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(self, scene: ImageType, detections: Detections) -&gt; ImageType:\n    \"\"\"\n    Applies a colored overlay to the scene outside of the detected regions.\n\n    Args:\n        scene (ImageType): The image where masks will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray`\n            or `PIL.Image.Image`.\n        detections (Detections): Object detections to annotate.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        detections = sv.Detections(...)\n\n        background_overlay_annotator = sv.BackgroundOverlayAnnotator()\n        annotated_frame = background_overlay_annotator.annotate(\n            scene=image.copy(),\n            detections=detections\n        )\n        ```\n\n    ![background-overlay-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/background-color-annotator-example-purple.png)\n    \"\"\"\n    colored_mask = np.full_like(scene, self.color.as_bgr(), dtype=np.uint8)\n\n    cv2.addWeighted(\n        scene, 1 - self.opacity, colored_mask, self.opacity, 0, dst=colored_mask\n    )\n\n    if detections.mask is None or self.force_box:\n        for x1, y1, x2, y2 in detections.xyxy.astype(int):\n            colored_mask[y1:y2, x1:x2] = scene[y1:y2, x1:x2]\n    else:\n        for mask in detections.mask:\n            colored_mask[mask] = scene[mask]\n\n    np.copyto(scene, colored_mask)\n    return scene\n</code></pre>"},{"location":"detection/core/","title":"Detections","text":"<p>The <code>sv.Detections</code> class in the Supervision library standardizes results from various object detection and segmentation models into a consistent format. This class simplifies data manipulation and filtering, providing a uniform API for integration with Supervision trackers, annotators, and tools.</p> InferenceUltralyticsTransformers <p>Use <code>sv.Detections.from_inference</code> method, which accepts model results from both detection and segmentation models.</p> <pre><code>import cv2\nimport supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8n-640\")\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nresults = model.infer(image)[0]\ndetections = sv.Detections.from_inference(results)\n</code></pre> <p>Use <code>sv.Detections.from_ultralytics</code> method, which accepts model results from both detection and segmentation models.</p> <pre><code>import cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nresults = model(image)[0]\ndetections = sv.Detections.from_ultralytics(results)\n</code></pre> <p>Use <code>sv.Detections.from_transformers</code> method, which accepts model results from both detection and segmentation models.</p> <pre><code>import torch\nimport supervision as sv\nfrom PIL import Image\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n\nimage = Image.open(&lt;SOURCE_IMAGE_PATH&gt;)\ninputs = processor(images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nwidth, height = image.size\ntarget_size = torch.tensor([[height, width]])\nresults = processor.post_process_object_detection(\n    outputs=outputs, target_sizes=target_size)[0]\ndetections = sv.Detections.from_transformers(\n    transformers_results=results,\n    id2label=model.config.id2label)\n</code></pre> <p>Attributes:</p> Name Type Description <code>xyxy</code> <code>ndarray</code> <p>An array of shape <code>(n, 4)</code> containing the bounding boxes coordinates in format <code>[x1, y1, x2, y2]</code></p> <code>mask</code> <code>Optional[ndarray]</code> <p>(Optional[np.ndarray]): An array of shape <code>(n, H, W)</code> containing the segmentation masks.</p> <code>confidence</code> <code>Optional[ndarray]</code> <p>An array of shape <code>(n,)</code> containing the confidence scores of the detections.</p> <code>class_id</code> <code>Optional[ndarray]</code> <p>An array of shape <code>(n,)</code> containing the class ids of the detections.</p> <code>tracker_id</code> <code>Optional[ndarray]</code> <p>An array of shape <code>(n,)</code> containing the tracker ids of the detections.</p> <code>data</code> <code>Dict[str, Union[ndarray, List]]</code> <p>A dictionary containing additional data where each key is a string representing the data type, and the value is either a NumPy array or a list of corresponding data.</p> Source code in <code>supervision/detection/core.py</code> <pre><code>@dataclass\nclass Detections:\n    \"\"\"\n    The `sv.Detections` class in the Supervision library standardizes results from\n    various object detection and segmentation models into a consistent format. This\n    class simplifies data manipulation and filtering, providing a uniform API for\n    integration with Supervision [trackers](/trackers/), [annotators](/detection/annotators/), and [tools](/detection/tools/line_zone/).\n\n    === \"Inference\"\n\n        Use [`sv.Detections.from_inference`](/detection/core/#supervision.detection.core.Detections.from_inference)\n        method, which accepts model results from both detection and segmentation models.\n\n        ```python\n        import cv2\n        import supervision as sv\n        from inference import get_model\n\n        model = get_model(model_id=\"yolov8n-640\")\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        results = model.infer(image)[0]\n        detections = sv.Detections.from_inference(results)\n        ```\n\n    === \"Ultralytics\"\n\n        Use [`sv.Detections.from_ultralytics`](/detection/core/#supervision.detection.core.Detections.from_ultralytics)\n        method, which accepts model results from both detection and segmentation models.\n\n        ```python\n        import cv2\n        import supervision as sv\n        from ultralytics import YOLO\n\n        model = YOLO(\"yolov8n.pt\")\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        results = model(image)[0]\n        detections = sv.Detections.from_ultralytics(results)\n        ```\n\n    === \"Transformers\"\n\n        Use [`sv.Detections.from_transformers`](/detection/core/#supervision.detection.core.Detections.from_transformers)\n        method, which accepts model results from both detection and segmentation models.\n\n        ```python\n        import torch\n        import supervision as sv\n        from PIL import Image\n        from transformers import DetrImageProcessor, DetrForObjectDetection\n\n        processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n        model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n\n        image = Image.open(&lt;SOURCE_IMAGE_PATH&gt;)\n        inputs = processor(images=image, return_tensors=\"pt\")\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        width, height = image.size\n        target_size = torch.tensor([[height, width]])\n        results = processor.post_process_object_detection(\n            outputs=outputs, target_sizes=target_size)[0]\n        detections = sv.Detections.from_transformers(\n            transformers_results=results,\n            id2label=model.config.id2label)\n        ```\n\n    Attributes:\n        xyxy (np.ndarray): An array of shape `(n, 4)` containing\n            the bounding boxes coordinates in format `[x1, y1, x2, y2]`\n        mask: (Optional[np.ndarray]): An array of shape\n            `(n, H, W)` containing the segmentation masks.\n        confidence (Optional[np.ndarray]): An array of shape\n            `(n,)` containing the confidence scores of the detections.\n        class_id (Optional[np.ndarray]): An array of shape\n            `(n,)` containing the class ids of the detections.\n        tracker_id (Optional[np.ndarray]): An array of shape\n            `(n,)` containing the tracker ids of the detections.\n        data (Dict[str, Union[np.ndarray, List]]): A dictionary containing additional\n            data where each key is a string representing the data type, and the value\n            is either a NumPy array or a list of corresponding data.\n    \"\"\"  # noqa: E501 // docs\n\n    xyxy: np.ndarray\n    mask: Optional[np.ndarray] = None\n    confidence: Optional[np.ndarray] = None\n    class_id: Optional[np.ndarray] = None\n    tracker_id: Optional[np.ndarray] = None\n    data: Dict[str, Union[np.ndarray, List]] = field(default_factory=dict)\n\n    def __post_init__(self):\n        validate_detections_fields(\n            xyxy=self.xyxy,\n            mask=self.mask,\n            confidence=self.confidence,\n            class_id=self.class_id,\n            tracker_id=self.tracker_id,\n            data=self.data,\n        )\n\n    def __len__(self):\n        \"\"\"\n        Returns the number of detections in the Detections object.\n        \"\"\"\n        return len(self.xyxy)\n\n    def __iter__(\n        self,\n    ) -&gt; Iterator[\n        Tuple[\n            np.ndarray,\n            Optional[np.ndarray],\n            Optional[float],\n            Optional[int],\n            Optional[int],\n            Dict[str, Union[np.ndarray, List]],\n        ]\n    ]:\n        \"\"\"\n        Iterates over the Detections object and yield a tuple of\n        `(xyxy, mask, confidence, class_id, tracker_id, data)` for each detection.\n        \"\"\"\n        for i in range(len(self.xyxy)):\n            yield (\n                self.xyxy[i],\n                self.mask[i] if self.mask is not None else None,\n                self.confidence[i] if self.confidence is not None else None,\n                self.class_id[i] if self.class_id is not None else None,\n                self.tracker_id[i] if self.tracker_id is not None else None,\n                get_data_item(self.data, i),\n            )\n\n    def __eq__(self, other: Detections):\n        return all(\n            [\n                np.array_equal(self.xyxy, other.xyxy),\n                np.array_equal(self.mask, other.mask),\n                np.array_equal(self.class_id, other.class_id),\n                np.array_equal(self.confidence, other.confidence),\n                np.array_equal(self.tracker_id, other.tracker_id),\n                is_data_equal(self.data, other.data),\n            ]\n        )\n\n    @classmethod\n    def from_yolov5(cls, yolov5_results) -&gt; Detections:\n        \"\"\"\n        Creates a Detections instance from a\n        [YOLOv5](https://github.com/ultralytics/yolov5) inference result.\n\n        Args:\n            yolov5_results (yolov5.models.common.Detections):\n                The output Detections instance from YOLOv5\n\n        Returns:\n            Detections: A new Detections object.\n\n        Example:\n            ```python\n            import cv2\n            import torch\n            import supervision as sv\n\n            image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n            model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n            result = model(image)\n            detections = sv.Detections.from_yolov5(result)\n            ```\n        \"\"\"\n        yolov5_detections_predictions = yolov5_results.pred[0].cpu().cpu().numpy()\n\n        return cls(\n            xyxy=yolov5_detections_predictions[:, :4],\n            confidence=yolov5_detections_predictions[:, 4],\n            class_id=yolov5_detections_predictions[:, 5].astype(int),\n        )\n\n    @classmethod\n    def from_ultralytics(cls, ultralytics_results) -&gt; Detections:\n        \"\"\"\n        Creates a `sv.Detections` instance from a\n        [YOLOv8](https://github.com/ultralytics/ultralytics) inference result.\n\n        !!! Note\n\n            `from_ultralytics` is compatible with\n            [detection](https://docs.ultralytics.com/tasks/detect/),\n            [segmentation](https://docs.ultralytics.com/tasks/segment/), and\n            [OBB](https://docs.ultralytics.com/tasks/obb/) models.\n\n        Args:\n            ultralytics_results (ultralytics.yolo.engine.results.Results):\n                The output Results instance from Ultralytics\n\n        Returns:\n            Detections: A new Detections object.\n\n        Example:\n            ```python\n            import cv2\n            import supervision as sv\n            from ultralytics import YOLO\n\n            image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n            model = YOLO('yolov8s.pt')\n            results = model(image)[0]\n            detections = sv.Detections.from_ultralytics(results)\n            ```\n        \"\"\"  # noqa: E501 // docs\n\n        if hasattr(ultralytics_results, \"obb\") and ultralytics_results.obb is not None:\n            class_id = ultralytics_results.obb.cls.cpu().numpy().astype(int)\n            class_names = np.array([ultralytics_results.names[i] for i in class_id])\n            oriented_box_coordinates = ultralytics_results.obb.xyxyxyxy.cpu().numpy()\n            return cls(\n                xyxy=ultralytics_results.obb.xyxy.cpu().numpy(),\n                confidence=ultralytics_results.obb.conf.cpu().numpy(),\n                class_id=class_id,\n                tracker_id=ultralytics_results.obb.id.int().cpu().numpy()\n                if ultralytics_results.obb.id is not None\n                else None,\n                data={\n                    ORIENTED_BOX_COORDINATES: oriented_box_coordinates,\n                    CLASS_NAME_DATA_FIELD: class_names,\n                },\n            )\n\n        if hasattr(ultralytics_results, \"boxes\") and ultralytics_results.boxes is None:\n            masks = extract_ultralytics_masks(ultralytics_results)\n            return cls(\n                xyxy=mask_to_xyxy(masks),\n                mask=masks,\n                class_id=np.arange(len(ultralytics_results)),\n            )\n\n        class_id = ultralytics_results.boxes.cls.cpu().numpy().astype(int)\n        class_names = np.array([ultralytics_results.names[i] for i in class_id])\n        return cls(\n            xyxy=ultralytics_results.boxes.xyxy.cpu().numpy(),\n            confidence=ultralytics_results.boxes.conf.cpu().numpy(),\n            class_id=class_id,\n            mask=extract_ultralytics_masks(ultralytics_results),\n            tracker_id=ultralytics_results.boxes.id.int().cpu().numpy()\n            if ultralytics_results.boxes.id is not None\n            else None,\n            data={CLASS_NAME_DATA_FIELD: class_names},\n        )\n\n    @classmethod\n    def from_yolo_nas(cls, yolo_nas_results) -&gt; Detections:\n        \"\"\"\n        Creates a Detections instance from a\n        [YOLO-NAS](https://github.com/Deci-AI/super-gradients/blob/master/YOLONAS.md)\n        inference result.\n\n        Args:\n            yolo_nas_results (ImageDetectionPrediction):\n                The output Results instance from YOLO-NAS\n                ImageDetectionPrediction is coming from\n                'super_gradients.training.models.prediction_results'\n\n        Returns:\n            Detections: A new Detections object.\n\n        Example:\n            ```python\n            import cv2\n            from super_gradients.training import models\n            import supervision as sv\n\n            image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n            model = models.get('yolo_nas_l', pretrained_weights=\"coco\")\n\n            result = list(model.predict(image, conf=0.35))[0]\n            detections = sv.Detections.from_yolo_nas(result)\n            ```\n        \"\"\"\n        if np.asarray(yolo_nas_results.prediction.bboxes_xyxy).shape[0] == 0:\n            return cls.empty()\n\n        return cls(\n            xyxy=yolo_nas_results.prediction.bboxes_xyxy,\n            confidence=yolo_nas_results.prediction.confidence,\n            class_id=yolo_nas_results.prediction.labels.astype(int),\n        )\n\n    @classmethod\n    def from_tensorflow(\n        cls, tensorflow_results: dict, resolution_wh: tuple\n    ) -&gt; Detections:\n        \"\"\"\n        Creates a Detections instance from a\n        [Tensorflow Hub](https://www.tensorflow.org/hub/tutorials/tf2_object_detection)\n        inference result.\n\n        Args:\n            tensorflow_results (dict):\n                The output results from Tensorflow Hub.\n\n        Returns:\n            Detections: A new Detections object.\n\n        Example:\n            ```python\n            import tensorflow as tf\n            import tensorflow_hub as hub\n            import numpy as np\n            import cv2\n\n            module_handle = \"https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1\"\n            model = hub.load(module_handle)\n            img = np.array(cv2.imread(SOURCE_IMAGE_PATH))\n            result = model(img)\n            detections = sv.Detections.from_tensorflow(result)\n            ```\n        \"\"\"  # noqa: E501 // docs\n\n        boxes = tensorflow_results[\"detection_boxes\"][0].numpy()\n        boxes[:, [0, 2]] *= resolution_wh[0]\n        boxes[:, [1, 3]] *= resolution_wh[1]\n        boxes = boxes[:, [1, 0, 3, 2]]\n        return cls(\n            xyxy=boxes,\n            confidence=tensorflow_results[\"detection_scores\"][0].numpy(),\n            class_id=tensorflow_results[\"detection_classes\"][0].numpy().astype(int),\n        )\n\n    @classmethod\n    def from_deepsparse(cls, deepsparse_results) -&gt; Detections:\n        \"\"\"\n        Creates a Detections instance from a\n        [DeepSparse](https://github.com/neuralmagic/deepsparse)\n        inference result.\n\n        Args:\n            deepsparse_results (deepsparse.yolo.schemas.YOLOOutput):\n                The output Results instance from DeepSparse.\n\n        Returns:\n            Detections: A new Detections object.\n\n        Example:\n            ```python\n            import supervision as sv\n            from deepsparse import Pipeline\n\n            yolo_pipeline = Pipeline.create(\n                task=\"yolo\",\n                model_path = \"zoo:cv/detection/yolov5-l/pytorch/ultralytics/coco/pruned80_quant-none\"\n             )\n            result = yolo_pipeline(&lt;SOURCE IMAGE PATH&gt;)\n            detections = sv.Detections.from_deepsparse(result)\n            ```\n        \"\"\"  # noqa: E501 // docs\n\n        if np.asarray(deepsparse_results.boxes[0]).shape[0] == 0:\n            return cls.empty()\n\n        return cls(\n            xyxy=np.array(deepsparse_results.boxes[0]),\n            confidence=np.array(deepsparse_results.scores[0]),\n            class_id=np.array(deepsparse_results.labels[0]).astype(float).astype(int),\n        )\n\n    @classmethod\n    def from_mmdetection(cls, mmdet_results) -&gt; Detections:\n        \"\"\"\n        Creates a Detections instance from a\n        [mmdetection](https://github.com/open-mmlab/mmdetection) and\n        [mmyolo](https://github.com/open-mmlab/mmyolo) inference result.\n\n        Args:\n            mmdet_results (mmdet.structures.DetDataSample):\n                The output Results instance from MMDetection.\n\n        Returns:\n            Detections: A new Detections object.\n\n        Example:\n            ```python\n            import cv2\n            import supervision as sv\n            from mmdet.apis import init_detector, inference_detector\n\n            image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n            model = init_detector(&lt;CONFIG_PATH&gt;, &lt;WEIGHTS_PATH&gt;, device=&lt;DEVICE&gt;)\n\n            result = inference_detector(model, image)\n            detections = sv.Detections.from_mmdetection(result)\n            ```\n        \"\"\"  # noqa: E501 // docs\n\n        return cls(\n            xyxy=mmdet_results.pred_instances.bboxes.cpu().numpy(),\n            confidence=mmdet_results.pred_instances.scores.cpu().numpy(),\n            class_id=mmdet_results.pred_instances.labels.cpu().numpy().astype(int),\n            mask=mmdet_results.pred_instances.masks.cpu().numpy()\n            if \"masks\" in mmdet_results.pred_instances\n            else None,\n        )\n\n    @classmethod\n    def from_transformers(\n        cls, transformers_results: dict, id2label: Optional[Dict[int, str]] = None\n    ) -&gt; Detections:\n        \"\"\"\n        Creates a Detections instance from object detection or panoptic, semantic\n        and instance segmentation\n        [Transformer](https://github.com/huggingface/transformers) inference result.\n\n        Args:\n            transformers_results (Union[dict, torch.Tensor]):  Inference results from\n                your Transformers model. This can be either a dictionary containing\n                valuable outputs like `scores`, `labels`, `boxes`, `masks`,\n                `segments_info`, and `segmentation`, or a `torch.Tensor` holding a\n                segmentation map where values represent class IDs.\n            id2label (Optional[Dict[int, str]]): A dictionary mapping class IDs to\n                labels, typically part of the `transformers` model configuration. If\n                provided, the resulting dictionary will include class names.\n\n        Returns:\n            Detections: A new Detections object.\n\n        Example:\n            ```python\n            import torch\n            import supervision as sv\n            from PIL import Image\n            from transformers import DetrImageProcessor, DetrForObjectDetection\n\n            processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n            model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n\n            image = Image.open(&lt;SOURCE_IMAGE_PATH&gt;)\n            inputs = processor(images=image, return_tensors=\"pt\")\n\n            with torch.no_grad():\n                outputs = model(**inputs)\n\n            width, height = image.size\n            target_size = torch.tensor([[height, width]])\n            results = processor.post_process_object_detection(\n                outputs=outputs, target_sizes=target_size)[0]\n\n            detections = sv.Detections.from_transformers(\n                transformers_results=results,\n                id2label=model.config.id2label\n            )\n            ```\n        \"\"\"  # noqa: E501 // docs\n\n        if (\n            transformers_results.__class__.__name__ == \"Tensor\"\n            or \"segmentation\" in transformers_results\n        ):\n            return cls(\n                **process_transformers_v5_segmentation_result(\n                    transformers_results, id2label\n                )\n            )\n\n        if \"masks\" in transformers_results or \"png_string\" in transformers_results:\n            return cls(\n                **process_transformers_v4_segmentation_result(\n                    transformers_results, id2label\n                )\n            )\n\n        if \"boxes\" in transformers_results:\n            return cls(\n                **process_transformers_detection_result(transformers_results, id2label)\n            )\n\n    @classmethod\n    def from_detectron2(cls, detectron2_results) -&gt; Detections:\n        \"\"\"\n        Create a Detections object from the\n        [Detectron2](https://github.com/facebookresearch/detectron2) inference result.\n\n        Args:\n            detectron2_results: The output of a\n                Detectron2 model containing instances with prediction data.\n\n        Returns:\n            (Detections): A Detections object containing the bounding boxes,\n                class IDs, and confidences of the predictions.\n\n        Example:\n            ```python\n            import cv2\n            import supervision as sv\n            from detectron2.engine import DefaultPredictor\n            from detectron2.config import get_cfg\n\n\n            image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n            cfg = get_cfg()\n            cfg.merge_from_file(&lt;CONFIG_PATH&gt;)\n            cfg.MODEL.WEIGHTS = &lt;WEIGHTS_PATH&gt;\n            predictor = DefaultPredictor(cfg)\n\n            result = predictor(image)\n            detections = sv.Detections.from_detectron2(result)\n            ```\n        \"\"\"\n\n        return cls(\n            xyxy=detectron2_results[\"instances\"].pred_boxes.tensor.cpu().numpy(),\n            confidence=detectron2_results[\"instances\"].scores.cpu().numpy(),\n            mask=detectron2_results[\"instances\"].pred_masks.cpu().numpy()\n            if hasattr(detectron2_results[\"instances\"], \"pred_masks\")\n            else None,\n            class_id=detectron2_results[\"instances\"]\n            .pred_classes.cpu()\n            .numpy()\n            .astype(int),\n        )\n\n    @classmethod\n    def from_inference(cls, roboflow_result: Union[dict, Any]) -&gt; Detections:\n        \"\"\"\n        Create a `sv.Detections` object from the [Roboflow](https://roboflow.com/)\n        API inference result or the [Inference](https://inference.roboflow.com/)\n        package results. This method extracts bounding boxes, class IDs,\n        confidences, and class names from the Roboflow API result and encapsulates\n        them into a Detections object.\n\n        Args:\n            roboflow_result (dict, any): The result from the\n                Roboflow API or Inference package containing predictions.\n\n        Returns:\n            (Detections): A Detections object containing the bounding boxes, class IDs,\n                and confidences of the predictions.\n\n        Example:\n            ```python\n            import cv2\n            import supervision as sv\n            from inference import get_model\n\n            image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n            model = get_model(model_id=\"yolov8s-640\")\n\n            result = model.infer(image)[0]\n            detections = sv.Detections.from_inference(result)\n            ```\n        \"\"\"\n        with suppress(AttributeError):\n            roboflow_result = roboflow_result.dict(exclude_none=True, by_alias=True)\n        xyxy, confidence, class_id, masks, trackers, data = process_roboflow_result(\n            roboflow_result=roboflow_result\n        )\n\n        if np.asarray(xyxy).shape[0] == 0:\n            empty_detection = cls.empty()\n            empty_detection.data = {CLASS_NAME_DATA_FIELD: np.empty(0)}\n            return empty_detection\n\n        return cls(\n            xyxy=xyxy,\n            confidence=confidence,\n            class_id=class_id,\n            mask=masks,\n            tracker_id=trackers,\n            data=data,\n        )\n\n    @classmethod\n    def from_sam(cls, sam_result: List[dict]) -&gt; Detections:\n        \"\"\"\n        Creates a Detections instance from\n        [Segment Anything Model](https://github.com/facebookresearch/segment-anything)\n        inference result.\n\n        Args:\n            sam_result (List[dict]): The output Results instance from SAM\n\n        Returns:\n            Detections: A new Detections object.\n\n        Example:\n            ```python\n            import supervision as sv\n            from segment_anything import (\n                sam_model_registry,\n                SamAutomaticMaskGenerator\n             )\n\n            sam_model_reg = sam_model_registry[MODEL_TYPE]\n            sam = sam_model_reg(checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n            mask_generator = SamAutomaticMaskGenerator(sam)\n            sam_result = mask_generator.generate(IMAGE)\n            detections = sv.Detections.from_sam(sam_result=sam_result)\n            ```\n        \"\"\"\n\n        sorted_generated_masks = sorted(\n            sam_result, key=lambda x: x[\"area\"], reverse=True\n        )\n\n        xywh = np.array([mask[\"bbox\"] for mask in sorted_generated_masks])\n        mask = np.array([mask[\"segmentation\"] for mask in sorted_generated_masks])\n\n        if np.asarray(xywh).shape[0] == 0:\n            return cls.empty()\n\n        xyxy = xywh_to_xyxy(xywh=xywh)\n        return cls(xyxy=xyxy, mask=mask)\n\n    @classmethod\n    def from_azure_analyze_image(\n        cls, azure_result: dict, class_map: Optional[Dict[int, str]] = None\n    ) -&gt; Detections:\n        \"\"\"\n        Creates a Detections instance from [Azure Image Analysis 4.0](\n        https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/\n        concept-object-detection-40).\n\n        Args:\n            azure_result (dict): The result from Azure Image Analysis. It should\n                contain detected objects and their bounding box coordinates.\n            class_map (Optional[Dict[int, str]]): A mapping ofclass IDs (int) to class\n                names (str). If None, a new mapping is created dynamically.\n\n        Returns:\n            Detections: A new Detections object.\n\n        Example:\n            ```python\n            import requests\n            import supervision as sv\n\n            image = open(input, \"rb\").read()\n\n            endpoint = \"https://.cognitiveservices.azure.com/\"\n            subscription_key = \"\"\n\n            headers = {\n                \"Content-Type\": \"application/octet-stream\",\n                \"Ocp-Apim-Subscription-Key\": subscription_key\n             }\n\n            response = requests.post(endpoint,\n                headers=self.headers,\n                data=image\n             ).json()\n\n            detections = sv.Detections.from_azure_analyze_image(response)\n            ```\n        \"\"\"\n        if \"error\" in azure_result:\n            raise ValueError(\n                f'Azure API returned an error {azure_result[\"error\"][\"message\"]}'\n            )\n\n        xyxy, confidences, class_ids = [], [], []\n\n        is_dynamic_mapping = class_map is None\n        if is_dynamic_mapping:\n            class_map = {}\n\n        class_map = {value: key for key, value in class_map.items()}\n\n        for detection in azure_result[\"objectsResult\"][\"values\"]:\n            bbox = detection[\"boundingBox\"]\n\n            tags = detection[\"tags\"]\n\n            x0 = bbox[\"x\"]\n            y0 = bbox[\"y\"]\n            x1 = x0 + bbox[\"w\"]\n            y1 = y0 + bbox[\"h\"]\n\n            for tag in tags:\n                confidence = tag[\"confidence\"]\n                class_name = tag[\"name\"]\n                class_id = class_map.get(class_name, None)\n\n                if is_dynamic_mapping and class_id is None:\n                    class_id = len(class_map)\n                    class_map[class_name] = class_id\n\n                if class_id is not None:\n                    xyxy.append([x0, y0, x1, y1])\n                    confidences.append(confidence)\n                    class_ids.append(class_id)\n\n        if len(xyxy) == 0:\n            return Detections.empty()\n\n        return cls(\n            xyxy=np.array(xyxy),\n            class_id=np.array(class_ids),\n            confidence=np.array(confidences),\n        )\n\n    @classmethod\n    def from_paddledet(cls, paddledet_result) -&gt; Detections:\n        \"\"\"\n        Creates a Detections instance from\n            [PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection)\n            inference result.\n\n        Args:\n            paddledet_result (List[dict]): The output Results instance from PaddleDet\n\n        Returns:\n            Detections: A new Detections object.\n\n        Example:\n            ```python\n            import supervision as sv\n            import paddle\n            from ppdet.engine import Trainer\n            from ppdet.core.workspace import load_config\n\n            weights = ()\n            config = ()\n\n            cfg = load_config(config)\n            trainer = Trainer(cfg, mode='test')\n            trainer.load_weights(weights)\n\n            paddledet_result = trainer.predict([images])[0]\n\n            detections = sv.Detections.from_paddledet(paddledet_result)\n            ```\n        \"\"\"\n\n        if np.asarray(paddledet_result[\"bbox\"][:, 2:6]).shape[0] == 0:\n            return cls.empty()\n\n        return cls(\n            xyxy=paddledet_result[\"bbox\"][:, 2:6],\n            confidence=paddledet_result[\"bbox\"][:, 1],\n            class_id=paddledet_result[\"bbox\"][:, 0].astype(int),\n        )\n\n    @classmethod\n    def from_lmm(\n        cls, lmm: Union[LMM, str], result: Union[str, dict], **kwargs\n    ) -&gt; Detections:\n        \"\"\"\n        Creates a Detections object from the given result string based on the specified\n        Large Multimodal Model (LMM).\n\n        Args:\n            lmm (Union[LMM, str]): The type of LMM (Large Multimodal Model) to use.\n            result (str): The result string containing the detection data.\n            **kwargs: Additional keyword arguments required by the specified LMM.\n\n        Returns:\n            Detections: A new Detections object.\n\n        Raises:\n            ValueError: If the LMM is invalid, required arguments are missing, or\n                disallowed arguments are provided.\n            ValueError: If the specified LMM is not supported.\n\n        Examples:\n            ```python\n            import supervision as sv\n\n            paligemma_result = \"&lt;loc0256&gt;&lt;loc0256&gt;&lt;loc0768&gt;&lt;loc0768&gt; cat\"\n            detections = sv.Detections.from_lmm(\n                sv.LMM.PALIGEMMA,\n                paligemma_result,\n                resolution_wh=(1000, 1000),\n                classes=['cat', 'dog']\n            )\n            detections.xyxy\n            # array([[250., 250., 750., 750.]])\n\n            detections.class_id\n            # array([0])\n            ```\n        \"\"\"\n        lmm = validate_lmm_parameters(lmm, result, kwargs)\n\n        if lmm == LMM.PALIGEMMA:\n            assert isinstance(result, str)\n            xyxy, class_id, class_name = from_paligemma(result, **kwargs)\n            data = {CLASS_NAME_DATA_FIELD: class_name}\n            return cls(xyxy=xyxy, class_id=class_id, data=data)\n\n        if lmm == LMM.FLORENCE_2:\n            assert isinstance(result, dict)\n            xyxy, labels, mask, xyxyxyxy = from_florence_2(result, **kwargs)\n            if len(xyxy) == 0:\n                return cls.empty()\n\n            data = {}\n            if labels is not None:\n                data[CLASS_NAME_DATA_FIELD] = labels\n            if xyxyxyxy is not None:\n                data[ORIENTED_BOX_COORDINATES] = xyxyxyxy\n\n            return cls(xyxy=xyxy, mask=mask, data=data)\n\n        raise ValueError(f\"Unsupported LMM: {lmm}\")\n\n    @classmethod\n    def empty(cls) -&gt; Detections:\n        \"\"\"\n        Create an empty Detections object with no bounding boxes,\n            confidences, or class IDs.\n\n        Returns:\n            (Detections): An empty Detections object.\n\n        Example:\n            ```python\n            from supervision import Detections\n\n            empty_detections = Detections.empty()\n            ```\n        \"\"\"\n        return cls(\n            xyxy=np.empty((0, 4), dtype=np.float32),\n            confidence=np.array([], dtype=np.float32),\n            class_id=np.array([], dtype=int),\n        )\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"\n        Returns `True` if the `Detections` object is considered empty.\n        \"\"\"\n        empty_detections = Detections.empty()\n        empty_detections.data = self.data\n        return self == empty_detections\n\n    @classmethod\n    def merge(cls, detections_list: List[Detections]) -&gt; Detections:\n        \"\"\"\n        Merge a list of Detections objects into a single Detections object.\n\n        This method takes a list of Detections objects and combines their\n        respective fields (`xyxy`, `mask`, `confidence`, `class_id`, and `tracker_id`)\n        into a single Detections object.\n\n        For example, if merging Detections with 3 and 4 detected objects, this method\n        will return a Detections with 7 objects (7 entries in `xyxy`, `mask`, etc).\n\n        !!! Note\n\n            When merging, empty `Detections` objects are ignored.\n\n        Args:\n            detections_list (List[Detections]): A list of Detections objects to merge.\n\n        Returns:\n            (Detections): A single Detections object containing\n                the merged data from the input list.\n\n        Example:\n            ```python\n            import numpy as np\n            import supervision as sv\n\n            detections_1 = sv.Detections(\n                xyxy=np.array([[15, 15, 100, 100], [200, 200, 300, 300]]),\n                class_id=np.array([1, 2]),\n                data={'feature_vector': np.array([0.1, 0.2)])}\n             )\n\n            detections_2 = sv.Detections(\n                xyxy=np.array([[30, 30, 120, 120]]),\n                class_id=np.array([1]),\n                data={'feature_vector': [np.array([0.3])]}\n             )\n\n            merged_detections = Detections.merge([detections_1, detections_2])\n\n            merged_detections.xyxy\n            array([[ 15,  15, 100, 100],\n                   [200, 200, 300, 300],\n                   [ 30,  30, 120, 120]])\n\n            merged_detections.class_id\n            array([1, 2, 1])\n\n            merged_detections.data['feature_vector']\n            array([0.1, 0.2, 0.3])\n            ```\n        \"\"\"\n        detections_list = [\n            detections for detections in detections_list if not detections.is_empty()\n        ]\n\n        if len(detections_list) == 0:\n            return Detections.empty()\n\n        for detections in detections_list:\n            validate_detections_fields(\n                xyxy=detections.xyxy,\n                mask=detections.mask,\n                confidence=detections.confidence,\n                class_id=detections.class_id,\n                tracker_id=detections.tracker_id,\n                data=detections.data,\n            )\n\n        xyxy = np.vstack([d.xyxy for d in detections_list])\n\n        def stack_or_none(name: str):\n            if all(d.__getattribute__(name) is None for d in detections_list):\n                return None\n            if any(d.__getattribute__(name) is None for d in detections_list):\n                raise ValueError(f\"All or none of the '{name}' fields must be None\")\n            return (\n                np.vstack([d.__getattribute__(name) for d in detections_list])\n                if name == \"mask\"\n                else np.hstack([d.__getattribute__(name) for d in detections_list])\n            )\n\n        mask = stack_or_none(\"mask\")\n        confidence = stack_or_none(\"confidence\")\n        class_id = stack_or_none(\"class_id\")\n        tracker_id = stack_or_none(\"tracker_id\")\n\n        data = merge_data([d.data for d in detections_list])\n\n        return cls(\n            xyxy=xyxy,\n            mask=mask,\n            confidence=confidence,\n            class_id=class_id,\n            tracker_id=tracker_id,\n            data=data,\n        )\n\n    def get_anchors_coordinates(self, anchor: Position) -&gt; np.ndarray:\n        \"\"\"\n        Calculates and returns the coordinates of a specific anchor point\n        within the bounding boxes defined by the `xyxy` attribute. The anchor\n        point can be any of the predefined positions in the `Position` enum,\n        such as `CENTER`, `CENTER_LEFT`, `BOTTOM_RIGHT`, etc.\n\n        Args:\n            anchor (Position): An enum specifying the position of the anchor point\n                within the bounding box. Supported positions are defined in the\n                `Position` enum.\n\n        Returns:\n            np.ndarray: An array of shape `(n, 2)`, where `n` is the number of bounding\n                boxes. Each row contains the `[x, y]` coordinates of the specified\n                anchor point for the corresponding bounding box.\n\n        Raises:\n            ValueError: If the provided `anchor` is not supported.\n        \"\"\"\n        if anchor == Position.CENTER:\n            return np.array(\n                [\n                    (self.xyxy[:, 0] + self.xyxy[:, 2]) / 2,\n                    (self.xyxy[:, 1] + self.xyxy[:, 3]) / 2,\n                ]\n            ).transpose()\n        elif anchor == Position.CENTER_OF_MASS:\n            if self.mask is None:\n                raise ValueError(\n                    \"Cannot use `Position.CENTER_OF_MASS` without a detection mask.\"\n                )\n            return calculate_masks_centroids(masks=self.mask)\n        elif anchor == Position.CENTER_LEFT:\n            return np.array(\n                [\n                    self.xyxy[:, 0],\n                    (self.xyxy[:, 1] + self.xyxy[:, 3]) / 2,\n                ]\n            ).transpose()\n        elif anchor == Position.CENTER_RIGHT:\n            return np.array(\n                [\n                    self.xyxy[:, 2],\n                    (self.xyxy[:, 1] + self.xyxy[:, 3]) / 2,\n                ]\n            ).transpose()\n        elif anchor == Position.BOTTOM_CENTER:\n            return np.array(\n                [(self.xyxy[:, 0] + self.xyxy[:, 2]) / 2, self.xyxy[:, 3]]\n            ).transpose()\n        elif anchor == Position.BOTTOM_LEFT:\n            return np.array([self.xyxy[:, 0], self.xyxy[:, 3]]).transpose()\n        elif anchor == Position.BOTTOM_RIGHT:\n            return np.array([self.xyxy[:, 2], self.xyxy[:, 3]]).transpose()\n        elif anchor == Position.TOP_CENTER:\n            return np.array(\n                [(self.xyxy[:, 0] + self.xyxy[:, 2]) / 2, self.xyxy[:, 1]]\n            ).transpose()\n        elif anchor == Position.TOP_LEFT:\n            return np.array([self.xyxy[:, 0], self.xyxy[:, 1]]).transpose()\n        elif anchor == Position.TOP_RIGHT:\n            return np.array([self.xyxy[:, 2], self.xyxy[:, 1]]).transpose()\n\n        raise ValueError(f\"{anchor} is not supported.\")\n\n    def __getitem__(\n        self, index: Union[int, slice, List[int], np.ndarray, str]\n    ) -&gt; Union[Detections, List, np.ndarray, None]:\n        \"\"\"\n        Get a subset of the Detections object or access an item from its data field.\n\n        When provided with an integer, slice, list of integers, or a numpy array, this\n        method returns a new Detections object that represents a subset of the original\n        detections. When provided with a string, it accesses the corresponding item in\n        the data dictionary.\n\n        Args:\n            index (Union[int, slice, List[int], np.ndarray, str]): The index, indices,\n                or key to access a subset of the Detections or an item from the data.\n\n        Returns:\n            Union[Detections, Any]: A subset of the Detections object or an item from\n                the data field.\n\n        Example:\n            ```python\n            import supervision as sv\n\n            detections = sv.Detections()\n\n            first_detection = detections[0]\n            first_10_detections = detections[0:10]\n            some_detections = detections[[0, 2, 4]]\n            class_0_detections = detections[detections.class_id == 0]\n            high_confidence_detections = detections[detections.confidence &gt; 0.5]\n\n            feature_vector = detections['feature_vector']\n            ```\n        \"\"\"\n        if isinstance(index, str):\n            return self.data.get(index)\n        if isinstance(index, int):\n            index = [index]\n        return Detections(\n            xyxy=self.xyxy[index],\n            mask=self.mask[index] if self.mask is not None else None,\n            confidence=self.confidence[index] if self.confidence is not None else None,\n            class_id=self.class_id[index] if self.class_id is not None else None,\n            tracker_id=self.tracker_id[index] if self.tracker_id is not None else None,\n            data=get_data_item(self.data, index),\n        )\n\n    def __setitem__(self, key: str, value: Union[np.ndarray, List]):\n        \"\"\"\n        Set a value in the data dictionary of the Detections object.\n\n        Args:\n            key (str): The key in the data dictionary to set.\n            value (Union[np.ndarray, List]): The value to set for the key.\n\n        Example:\n            ```python\n            import cv2\n            import supervision as sv\n            from ultralytics import YOLO\n\n            image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n            model = YOLO('yolov8s.pt')\n\n            result = model(image)[0]\n            detections = sv.Detections.from_ultralytics(result)\n\n            detections['names'] = [\n                 model.model.names[class_id]\n                 for class_id\n                 in detections.class_id\n             ]\n            ```\n        \"\"\"\n        if not isinstance(value, (np.ndarray, list)):\n            raise TypeError(\"Value must be a np.ndarray or a list\")\n\n        if isinstance(value, list):\n            value = np.array(value)\n\n        self.data[key] = value\n\n    @property\n    def area(self) -&gt; np.ndarray:\n        \"\"\"\n        Calculate the area of each detection in the set of object detections.\n        If masks field is defined property returns are of each mask.\n        If only box is given property return area of each box.\n\n        Returns:\n          np.ndarray: An array of floats containing the area of each detection\n            in the format of `(area_1, area_2, , area_n)`,\n            where n is the number of detections.\n        \"\"\"\n        if self.mask is not None:\n            return np.array([np.sum(mask) for mask in self.mask])\n        else:\n            return self.box_area\n\n    @property\n    def box_area(self) -&gt; np.ndarray:\n        \"\"\"\n        Calculate the area of each bounding box in the set of object detections.\n\n        Returns:\n            np.ndarray: An array of floats containing the area of each bounding\n                box in the format of `(area_1, area_2, , area_n)`,\n                where n is the number of detections.\n        \"\"\"\n        return (self.xyxy[:, 3] - self.xyxy[:, 1]) * (self.xyxy[:, 2] - self.xyxy[:, 0])\n\n    def with_nms(\n        self, threshold: float = 0.5, class_agnostic: bool = False\n    ) -&gt; Detections:\n        \"\"\"\n        Performs non-max suppression on detection set. If the detections result\n        from a segmentation model, the IoU mask is applied. Otherwise, box IoU is used.\n\n        Args:\n            threshold (float, optional): The intersection-over-union threshold\n                to use for non-maximum suppression. I'm the lower the value the more\n                restrictive the NMS becomes. Defaults to 0.5.\n            class_agnostic (bool, optional): Whether to perform class-agnostic\n                non-maximum suppression. If True, the class_id of each detection\n                will be ignored. Defaults to False.\n\n        Returns:\n            Detections: A new Detections object containing the subset of detections\n                after non-maximum suppression.\n\n        Raises:\n            AssertionError: If `confidence` is None and class_agnostic is False.\n                If `class_id` is None and class_agnostic is False.\n        \"\"\"\n        if len(self) == 0:\n            return self\n\n        assert (\n            self.confidence is not None\n        ), \"Detections confidence must be given for NMS to be executed.\"\n\n        if class_agnostic:\n            predictions = np.hstack((self.xyxy, self.confidence.reshape(-1, 1)))\n        else:\n            assert self.class_id is not None, (\n                \"Detections class_id must be given for NMS to be executed. If you\"\n                \" intended to perform class agnostic NMS set class_agnostic=True.\"\n            )\n            predictions = np.hstack(\n                (\n                    self.xyxy,\n                    self.confidence.reshape(-1, 1),\n                    self.class_id.reshape(-1, 1),\n                )\n            )\n\n        if self.mask is not None:\n            indices = mask_non_max_suppression(\n                predictions=predictions, masks=self.mask, iou_threshold=threshold\n            )\n        else:\n            indices = box_non_max_suppression(\n                predictions=predictions, iou_threshold=threshold\n            )\n\n        return self[indices]\n\n    def with_nmm(\n        self, threshold: float = 0.5, class_agnostic: bool = False\n    ) -&gt; Detections:\n        \"\"\"\n        Perform non-maximum merging on the current set of object detections.\n\n        Args:\n            threshold (float, optional): The intersection-over-union threshold\n                to use for non-maximum merging. Defaults to 0.5.\n            class_agnostic (bool, optional): Whether to perform class-agnostic\n                non-maximum merging. If True, the class_id of each detection\n                will be ignored. Defaults to False.\n\n        Returns:\n            Detections: A new Detections object containing the subset of detections\n                after non-maximum merging.\n\n        Raises:\n            AssertionError: If `confidence` is None or `class_id` is None and\n                class_agnostic is False.\n\n        ![non-max-merging](https://media.roboflow.com/supervision-docs/non-max-merging.png){ align=center width=\"800\" }\n        \"\"\"  # noqa: E501 // docs\n        if len(self) == 0:\n            return self\n\n        assert (\n            self.confidence is not None\n        ), \"Detections confidence must be given for NMM to be executed.\"\n\n        if class_agnostic:\n            predictions = np.hstack((self.xyxy, self.confidence.reshape(-1, 1)))\n        else:\n            assert self.class_id is not None, (\n                \"Detections class_id must be given for NMM to be executed. If you\"\n                \" intended to perform class agnostic NMM set class_agnostic=True.\"\n            )\n            predictions = np.hstack(\n                (\n                    self.xyxy,\n                    self.confidence.reshape(-1, 1),\n                    self.class_id.reshape(-1, 1),\n                )\n            )\n\n        merge_groups = box_non_max_merge(\n            predictions=predictions, iou_threshold=threshold\n        )\n\n        result = []\n        for merge_group in merge_groups:\n            unmerged_detections = [self[i] for i in merge_group]\n            merged_detections = merge_inner_detections_objects(\n                unmerged_detections, threshold\n            )\n            result.append(merged_detections)\n\n        return Detections.merge(result)\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections-attributes","title":"Attributes","text":""},{"location":"detection/core/#supervision.detection.core.Detections.area","title":"<code>area: np.ndarray</code>  <code>property</code>","text":"<p>Calculate the area of each detection in the set of object detections. If masks field is defined property returns are of each mask. If only box is given property return area of each box.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array of floats containing the area of each detection in the format of <code>(area_1, area_2, , area_n)</code>, where n is the number of detections.</p>"},{"location":"detection/core/#supervision.detection.core.Detections.box_area","title":"<code>box_area: np.ndarray</code>  <code>property</code>","text":"<p>Calculate the area of each bounding box in the set of object detections.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array of floats containing the area of each bounding box in the format of <code>(area_1, area_2, , area_n)</code>, where n is the number of detections.</p>"},{"location":"detection/core/#supervision.detection.core.Detections-functions","title":"Functions","text":""},{"location":"detection/core/#supervision.detection.core.Detections.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get a subset of the Detections object or access an item from its data field.</p> <p>When provided with an integer, slice, list of integers, or a numpy array, this method returns a new Detections object that represents a subset of the original detections. When provided with a string, it accesses the corresponding item in the data dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[int, slice, List[int], ndarray, str]</code> <p>The index, indices, or key to access a subset of the Detections or an item from the data.</p> required <p>Returns:</p> Type Description <code>Union[Detections, List, ndarray, None]</code> <p>Union[Detections, Any]: A subset of the Detections object or an item from the data field.</p> Example <pre><code>import supervision as sv\n\ndetections = sv.Detections()\n\nfirst_detection = detections[0]\nfirst_10_detections = detections[0:10]\nsome_detections = detections[[0, 2, 4]]\nclass_0_detections = detections[detections.class_id == 0]\nhigh_confidence_detections = detections[detections.confidence &gt; 0.5]\n\nfeature_vector = detections['feature_vector']\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>def __getitem__(\n    self, index: Union[int, slice, List[int], np.ndarray, str]\n) -&gt; Union[Detections, List, np.ndarray, None]:\n    \"\"\"\n    Get a subset of the Detections object or access an item from its data field.\n\n    When provided with an integer, slice, list of integers, or a numpy array, this\n    method returns a new Detections object that represents a subset of the original\n    detections. When provided with a string, it accesses the corresponding item in\n    the data dictionary.\n\n    Args:\n        index (Union[int, slice, List[int], np.ndarray, str]): The index, indices,\n            or key to access a subset of the Detections or an item from the data.\n\n    Returns:\n        Union[Detections, Any]: A subset of the Detections object or an item from\n            the data field.\n\n    Example:\n        ```python\n        import supervision as sv\n\n        detections = sv.Detections()\n\n        first_detection = detections[0]\n        first_10_detections = detections[0:10]\n        some_detections = detections[[0, 2, 4]]\n        class_0_detections = detections[detections.class_id == 0]\n        high_confidence_detections = detections[detections.confidence &gt; 0.5]\n\n        feature_vector = detections['feature_vector']\n        ```\n    \"\"\"\n    if isinstance(index, str):\n        return self.data.get(index)\n    if isinstance(index, int):\n        index = [index]\n    return Detections(\n        xyxy=self.xyxy[index],\n        mask=self.mask[index] if self.mask is not None else None,\n        confidence=self.confidence[index] if self.confidence is not None else None,\n        class_id=self.class_id[index] if self.class_id is not None else None,\n        tracker_id=self.tracker_id[index] if self.tracker_id is not None else None,\n        data=get_data_item(self.data, index),\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterates over the Detections object and yield a tuple of <code>(xyxy, mask, confidence, class_id, tracker_id, data)</code> for each detection.</p> Source code in <code>supervision/detection/core.py</code> <pre><code>def __iter__(\n    self,\n) -&gt; Iterator[\n    Tuple[\n        np.ndarray,\n        Optional[np.ndarray],\n        Optional[float],\n        Optional[int],\n        Optional[int],\n        Dict[str, Union[np.ndarray, List]],\n    ]\n]:\n    \"\"\"\n    Iterates over the Detections object and yield a tuple of\n    `(xyxy, mask, confidence, class_id, tracker_id, data)` for each detection.\n    \"\"\"\n    for i in range(len(self.xyxy)):\n        yield (\n            self.xyxy[i],\n            self.mask[i] if self.mask is not None else None,\n            self.confidence[i] if self.confidence is not None else None,\n            self.class_id[i] if self.class_id is not None else None,\n            self.tracker_id[i] if self.tracker_id is not None else None,\n            get_data_item(self.data, i),\n        )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of detections in the Detections object.</p> Source code in <code>supervision/detection/core.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Returns the number of detections in the Detections object.\n    \"\"\"\n    return len(self.xyxy)\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Set a value in the data dictionary of the Detections object.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key in the data dictionary to set.</p> required <code>value</code> <code>Union[ndarray, List]</code> <p>The value to set for the key.</p> required Example <pre><code>import cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nmodel = YOLO('yolov8s.pt')\n\nresult = model(image)[0]\ndetections = sv.Detections.from_ultralytics(result)\n\ndetections['names'] = [\n     model.model.names[class_id]\n     for class_id\n     in detections.class_id\n ]\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>def __setitem__(self, key: str, value: Union[np.ndarray, List]):\n    \"\"\"\n    Set a value in the data dictionary of the Detections object.\n\n    Args:\n        key (str): The key in the data dictionary to set.\n        value (Union[np.ndarray, List]): The value to set for the key.\n\n    Example:\n        ```python\n        import cv2\n        import supervision as sv\n        from ultralytics import YOLO\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        model = YOLO('yolov8s.pt')\n\n        result = model(image)[0]\n        detections = sv.Detections.from_ultralytics(result)\n\n        detections['names'] = [\n             model.model.names[class_id]\n             for class_id\n             in detections.class_id\n         ]\n        ```\n    \"\"\"\n    if not isinstance(value, (np.ndarray, list)):\n        raise TypeError(\"Value must be a np.ndarray or a list\")\n\n    if isinstance(value, list):\n        value = np.array(value)\n\n    self.data[key] = value\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.empty","title":"<code>empty()</code>  <code>classmethod</code>","text":"<p>Create an empty Detections object with no bounding boxes,     confidences, or class IDs.</p> <p>Returns:</p> Type Description <code>Detections</code> <p>An empty Detections object.</p> Example <pre><code>from supervision import Detections\n\nempty_detections = Detections.empty()\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef empty(cls) -&gt; Detections:\n    \"\"\"\n    Create an empty Detections object with no bounding boxes,\n        confidences, or class IDs.\n\n    Returns:\n        (Detections): An empty Detections object.\n\n    Example:\n        ```python\n        from supervision import Detections\n\n        empty_detections = Detections.empty()\n        ```\n    \"\"\"\n    return cls(\n        xyxy=np.empty((0, 4), dtype=np.float32),\n        confidence=np.array([], dtype=np.float32),\n        class_id=np.array([], dtype=int),\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_azure_analyze_image","title":"<code>from_azure_analyze_image(azure_result, class_map=None)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from Azure Image Analysis 4.0.</p> <p>Parameters:</p> Name Type Description Default <code>azure_result</code> <code>dict</code> <p>The result from Azure Image Analysis. It should contain detected objects and their bounding box coordinates.</p> required <code>class_map</code> <code>Optional[Dict[int, str]]</code> <p>A mapping ofclass IDs (int) to class names (str). If None, a new mapping is created dynamically.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Example <pre><code>import requests\nimport supervision as sv\n\nimage = open(input, \"rb\").read()\n\nendpoint = \"https://.cognitiveservices.azure.com/\"\nsubscription_key = \"\"\n\nheaders = {\n    \"Content-Type\": \"application/octet-stream\",\n    \"Ocp-Apim-Subscription-Key\": subscription_key\n }\n\nresponse = requests.post(endpoint,\n    headers=self.headers,\n    data=image\n ).json()\n\ndetections = sv.Detections.from_azure_analyze_image(response)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_azure_analyze_image(\n    cls, azure_result: dict, class_map: Optional[Dict[int, str]] = None\n) -&gt; Detections:\n    \"\"\"\n    Creates a Detections instance from [Azure Image Analysis 4.0](\n    https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/\n    concept-object-detection-40).\n\n    Args:\n        azure_result (dict): The result from Azure Image Analysis. It should\n            contain detected objects and their bounding box coordinates.\n        class_map (Optional[Dict[int, str]]): A mapping ofclass IDs (int) to class\n            names (str). If None, a new mapping is created dynamically.\n\n    Returns:\n        Detections: A new Detections object.\n\n    Example:\n        ```python\n        import requests\n        import supervision as sv\n\n        image = open(input, \"rb\").read()\n\n        endpoint = \"https://.cognitiveservices.azure.com/\"\n        subscription_key = \"\"\n\n        headers = {\n            \"Content-Type\": \"application/octet-stream\",\n            \"Ocp-Apim-Subscription-Key\": subscription_key\n         }\n\n        response = requests.post(endpoint,\n            headers=self.headers,\n            data=image\n         ).json()\n\n        detections = sv.Detections.from_azure_analyze_image(response)\n        ```\n    \"\"\"\n    if \"error\" in azure_result:\n        raise ValueError(\n            f'Azure API returned an error {azure_result[\"error\"][\"message\"]}'\n        )\n\n    xyxy, confidences, class_ids = [], [], []\n\n    is_dynamic_mapping = class_map is None\n    if is_dynamic_mapping:\n        class_map = {}\n\n    class_map = {value: key for key, value in class_map.items()}\n\n    for detection in azure_result[\"objectsResult\"][\"values\"]:\n        bbox = detection[\"boundingBox\"]\n\n        tags = detection[\"tags\"]\n\n        x0 = bbox[\"x\"]\n        y0 = bbox[\"y\"]\n        x1 = x0 + bbox[\"w\"]\n        y1 = y0 + bbox[\"h\"]\n\n        for tag in tags:\n            confidence = tag[\"confidence\"]\n            class_name = tag[\"name\"]\n            class_id = class_map.get(class_name, None)\n\n            if is_dynamic_mapping and class_id is None:\n                class_id = len(class_map)\n                class_map[class_name] = class_id\n\n            if class_id is not None:\n                xyxy.append([x0, y0, x1, y1])\n                confidences.append(confidence)\n                class_ids.append(class_id)\n\n    if len(xyxy) == 0:\n        return Detections.empty()\n\n    return cls(\n        xyxy=np.array(xyxy),\n        class_id=np.array(class_ids),\n        confidence=np.array(confidences),\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_deepsparse","title":"<code>from_deepsparse(deepsparse_results)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from a DeepSparse inference result.</p> <p>Parameters:</p> Name Type Description Default <code>deepsparse_results</code> <code>YOLOOutput</code> <p>The output Results instance from DeepSparse.</p> required <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Example <pre><code>import supervision as sv\nfrom deepsparse import Pipeline\n\nyolo_pipeline = Pipeline.create(\n    task=\"yolo\",\n    model_path = \"zoo:cv/detection/yolov5-l/pytorch/ultralytics/coco/pruned80_quant-none\"\n )\nresult = yolo_pipeline(&lt;SOURCE IMAGE PATH&gt;)\ndetections = sv.Detections.from_deepsparse(result)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_deepsparse(cls, deepsparse_results) -&gt; Detections:\n    \"\"\"\n    Creates a Detections instance from a\n    [DeepSparse](https://github.com/neuralmagic/deepsparse)\n    inference result.\n\n    Args:\n        deepsparse_results (deepsparse.yolo.schemas.YOLOOutput):\n            The output Results instance from DeepSparse.\n\n    Returns:\n        Detections: A new Detections object.\n\n    Example:\n        ```python\n        import supervision as sv\n        from deepsparse import Pipeline\n\n        yolo_pipeline = Pipeline.create(\n            task=\"yolo\",\n            model_path = \"zoo:cv/detection/yolov5-l/pytorch/ultralytics/coco/pruned80_quant-none\"\n         )\n        result = yolo_pipeline(&lt;SOURCE IMAGE PATH&gt;)\n        detections = sv.Detections.from_deepsparse(result)\n        ```\n    \"\"\"  # noqa: E501 // docs\n\n    if np.asarray(deepsparse_results.boxes[0]).shape[0] == 0:\n        return cls.empty()\n\n    return cls(\n        xyxy=np.array(deepsparse_results.boxes[0]),\n        confidence=np.array(deepsparse_results.scores[0]),\n        class_id=np.array(deepsparse_results.labels[0]).astype(float).astype(int),\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_detectron2","title":"<code>from_detectron2(detectron2_results)</code>  <code>classmethod</code>","text":"<p>Create a Detections object from the Detectron2 inference result.</p> <p>Parameters:</p> Name Type Description Default <code>detectron2_results</code> <p>The output of a Detectron2 model containing instances with prediction data.</p> required <p>Returns:</p> Type Description <code>Detections</code> <p>A Detections object containing the bounding boxes, class IDs, and confidences of the predictions.</p> Example <pre><code>import cv2\nimport supervision as sv\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\n\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\ncfg = get_cfg()\ncfg.merge_from_file(&lt;CONFIG_PATH&gt;)\ncfg.MODEL.WEIGHTS = &lt;WEIGHTS_PATH&gt;\npredictor = DefaultPredictor(cfg)\n\nresult = predictor(image)\ndetections = sv.Detections.from_detectron2(result)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_detectron2(cls, detectron2_results) -&gt; Detections:\n    \"\"\"\n    Create a Detections object from the\n    [Detectron2](https://github.com/facebookresearch/detectron2) inference result.\n\n    Args:\n        detectron2_results: The output of a\n            Detectron2 model containing instances with prediction data.\n\n    Returns:\n        (Detections): A Detections object containing the bounding boxes,\n            class IDs, and confidences of the predictions.\n\n    Example:\n        ```python\n        import cv2\n        import supervision as sv\n        from detectron2.engine import DefaultPredictor\n        from detectron2.config import get_cfg\n\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        cfg = get_cfg()\n        cfg.merge_from_file(&lt;CONFIG_PATH&gt;)\n        cfg.MODEL.WEIGHTS = &lt;WEIGHTS_PATH&gt;\n        predictor = DefaultPredictor(cfg)\n\n        result = predictor(image)\n        detections = sv.Detections.from_detectron2(result)\n        ```\n    \"\"\"\n\n    return cls(\n        xyxy=detectron2_results[\"instances\"].pred_boxes.tensor.cpu().numpy(),\n        confidence=detectron2_results[\"instances\"].scores.cpu().numpy(),\n        mask=detectron2_results[\"instances\"].pred_masks.cpu().numpy()\n        if hasattr(detectron2_results[\"instances\"], \"pred_masks\")\n        else None,\n        class_id=detectron2_results[\"instances\"]\n        .pred_classes.cpu()\n        .numpy()\n        .astype(int),\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_inference","title":"<code>from_inference(roboflow_result)</code>  <code>classmethod</code>","text":"<p>Create a <code>sv.Detections</code> object from the Roboflow API inference result or the Inference package results. This method extracts bounding boxes, class IDs, confidences, and class names from the Roboflow API result and encapsulates them into a Detections object.</p> <p>Parameters:</p> Name Type Description Default <code>roboflow_result</code> <code>(dict, any)</code> <p>The result from the Roboflow API or Inference package containing predictions.</p> required <p>Returns:</p> Type Description <code>Detections</code> <p>A Detections object containing the bounding boxes, class IDs, and confidences of the predictions.</p> Example <pre><code>import cv2\nimport supervision as sv\nfrom inference import get_model\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nmodel = get_model(model_id=\"yolov8s-640\")\n\nresult = model.infer(image)[0]\ndetections = sv.Detections.from_inference(result)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_inference(cls, roboflow_result: Union[dict, Any]) -&gt; Detections:\n    \"\"\"\n    Create a `sv.Detections` object from the [Roboflow](https://roboflow.com/)\n    API inference result or the [Inference](https://inference.roboflow.com/)\n    package results. This method extracts bounding boxes, class IDs,\n    confidences, and class names from the Roboflow API result and encapsulates\n    them into a Detections object.\n\n    Args:\n        roboflow_result (dict, any): The result from the\n            Roboflow API or Inference package containing predictions.\n\n    Returns:\n        (Detections): A Detections object containing the bounding boxes, class IDs,\n            and confidences of the predictions.\n\n    Example:\n        ```python\n        import cv2\n        import supervision as sv\n        from inference import get_model\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        model = get_model(model_id=\"yolov8s-640\")\n\n        result = model.infer(image)[0]\n        detections = sv.Detections.from_inference(result)\n        ```\n    \"\"\"\n    with suppress(AttributeError):\n        roboflow_result = roboflow_result.dict(exclude_none=True, by_alias=True)\n    xyxy, confidence, class_id, masks, trackers, data = process_roboflow_result(\n        roboflow_result=roboflow_result\n    )\n\n    if np.asarray(xyxy).shape[0] == 0:\n        empty_detection = cls.empty()\n        empty_detection.data = {CLASS_NAME_DATA_FIELD: np.empty(0)}\n        return empty_detection\n\n    return cls(\n        xyxy=xyxy,\n        confidence=confidence,\n        class_id=class_id,\n        mask=masks,\n        tracker_id=trackers,\n        data=data,\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_lmm","title":"<code>from_lmm(lmm, result, **kwargs)</code>  <code>classmethod</code>","text":"<p>Creates a Detections object from the given result string based on the specified Large Multimodal Model (LMM).</p> <p>Parameters:</p> Name Type Description Default <code>lmm</code> <code>Union[LMM, str]</code> <p>The type of LMM (Large Multimodal Model) to use.</p> required <code>result</code> <code>str</code> <p>The result string containing the detection data.</p> required <code>**kwargs</code> <p>Additional keyword arguments required by the specified LMM.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the LMM is invalid, required arguments are missing, or disallowed arguments are provided.</p> <code>ValueError</code> <p>If the specified LMM is not supported.</p> <p>Examples:</p> <pre><code>import supervision as sv\n\npaligemma_result = \"&lt;loc0256&gt;&lt;loc0256&gt;&lt;loc0768&gt;&lt;loc0768&gt; cat\"\ndetections = sv.Detections.from_lmm(\n    sv.LMM.PALIGEMMA,\n    paligemma_result,\n    resolution_wh=(1000, 1000),\n    classes=['cat', 'dog']\n)\ndetections.xyxy\n# array([[250., 250., 750., 750.]])\n\ndetections.class_id\n# array([0])\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_lmm(\n    cls, lmm: Union[LMM, str], result: Union[str, dict], **kwargs\n) -&gt; Detections:\n    \"\"\"\n    Creates a Detections object from the given result string based on the specified\n    Large Multimodal Model (LMM).\n\n    Args:\n        lmm (Union[LMM, str]): The type of LMM (Large Multimodal Model) to use.\n        result (str): The result string containing the detection data.\n        **kwargs: Additional keyword arguments required by the specified LMM.\n\n    Returns:\n        Detections: A new Detections object.\n\n    Raises:\n        ValueError: If the LMM is invalid, required arguments are missing, or\n            disallowed arguments are provided.\n        ValueError: If the specified LMM is not supported.\n\n    Examples:\n        ```python\n        import supervision as sv\n\n        paligemma_result = \"&lt;loc0256&gt;&lt;loc0256&gt;&lt;loc0768&gt;&lt;loc0768&gt; cat\"\n        detections = sv.Detections.from_lmm(\n            sv.LMM.PALIGEMMA,\n            paligemma_result,\n            resolution_wh=(1000, 1000),\n            classes=['cat', 'dog']\n        )\n        detections.xyxy\n        # array([[250., 250., 750., 750.]])\n\n        detections.class_id\n        # array([0])\n        ```\n    \"\"\"\n    lmm = validate_lmm_parameters(lmm, result, kwargs)\n\n    if lmm == LMM.PALIGEMMA:\n        assert isinstance(result, str)\n        xyxy, class_id, class_name = from_paligemma(result, **kwargs)\n        data = {CLASS_NAME_DATA_FIELD: class_name}\n        return cls(xyxy=xyxy, class_id=class_id, data=data)\n\n    if lmm == LMM.FLORENCE_2:\n        assert isinstance(result, dict)\n        xyxy, labels, mask, xyxyxyxy = from_florence_2(result, **kwargs)\n        if len(xyxy) == 0:\n            return cls.empty()\n\n        data = {}\n        if labels is not None:\n            data[CLASS_NAME_DATA_FIELD] = labels\n        if xyxyxyxy is not None:\n            data[ORIENTED_BOX_COORDINATES] = xyxyxyxy\n\n        return cls(xyxy=xyxy, mask=mask, data=data)\n\n    raise ValueError(f\"Unsupported LMM: {lmm}\")\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_mmdetection","title":"<code>from_mmdetection(mmdet_results)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from a mmdetection and mmyolo inference result.</p> <p>Parameters:</p> Name Type Description Default <code>mmdet_results</code> <code>DetDataSample</code> <p>The output Results instance from MMDetection.</p> required <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Example <pre><code>import cv2\nimport supervision as sv\nfrom mmdet.apis import init_detector, inference_detector\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nmodel = init_detector(&lt;CONFIG_PATH&gt;, &lt;WEIGHTS_PATH&gt;, device=&lt;DEVICE&gt;)\n\nresult = inference_detector(model, image)\ndetections = sv.Detections.from_mmdetection(result)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_mmdetection(cls, mmdet_results) -&gt; Detections:\n    \"\"\"\n    Creates a Detections instance from a\n    [mmdetection](https://github.com/open-mmlab/mmdetection) and\n    [mmyolo](https://github.com/open-mmlab/mmyolo) inference result.\n\n    Args:\n        mmdet_results (mmdet.structures.DetDataSample):\n            The output Results instance from MMDetection.\n\n    Returns:\n        Detections: A new Detections object.\n\n    Example:\n        ```python\n        import cv2\n        import supervision as sv\n        from mmdet.apis import init_detector, inference_detector\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        model = init_detector(&lt;CONFIG_PATH&gt;, &lt;WEIGHTS_PATH&gt;, device=&lt;DEVICE&gt;)\n\n        result = inference_detector(model, image)\n        detections = sv.Detections.from_mmdetection(result)\n        ```\n    \"\"\"  # noqa: E501 // docs\n\n    return cls(\n        xyxy=mmdet_results.pred_instances.bboxes.cpu().numpy(),\n        confidence=mmdet_results.pred_instances.scores.cpu().numpy(),\n        class_id=mmdet_results.pred_instances.labels.cpu().numpy().astype(int),\n        mask=mmdet_results.pred_instances.masks.cpu().numpy()\n        if \"masks\" in mmdet_results.pred_instances\n        else None,\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_paddledet","title":"<code>from_paddledet(paddledet_result)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from     PaddleDetection     inference result.</p> <p>Parameters:</p> Name Type Description Default <code>paddledet_result</code> <code>List[dict]</code> <p>The output Results instance from PaddleDet</p> required <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Example <pre><code>import supervision as sv\nimport paddle\nfrom ppdet.engine import Trainer\nfrom ppdet.core.workspace import load_config\n\nweights = ()\nconfig = ()\n\ncfg = load_config(config)\ntrainer = Trainer(cfg, mode='test')\ntrainer.load_weights(weights)\n\npaddledet_result = trainer.predict([images])[0]\n\ndetections = sv.Detections.from_paddledet(paddledet_result)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_paddledet(cls, paddledet_result) -&gt; Detections:\n    \"\"\"\n    Creates a Detections instance from\n        [PaddleDetection](https://github.com/PaddlePaddle/PaddleDetection)\n        inference result.\n\n    Args:\n        paddledet_result (List[dict]): The output Results instance from PaddleDet\n\n    Returns:\n        Detections: A new Detections object.\n\n    Example:\n        ```python\n        import supervision as sv\n        import paddle\n        from ppdet.engine import Trainer\n        from ppdet.core.workspace import load_config\n\n        weights = ()\n        config = ()\n\n        cfg = load_config(config)\n        trainer = Trainer(cfg, mode='test')\n        trainer.load_weights(weights)\n\n        paddledet_result = trainer.predict([images])[0]\n\n        detections = sv.Detections.from_paddledet(paddledet_result)\n        ```\n    \"\"\"\n\n    if np.asarray(paddledet_result[\"bbox\"][:, 2:6]).shape[0] == 0:\n        return cls.empty()\n\n    return cls(\n        xyxy=paddledet_result[\"bbox\"][:, 2:6],\n        confidence=paddledet_result[\"bbox\"][:, 1],\n        class_id=paddledet_result[\"bbox\"][:, 0].astype(int),\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_sam","title":"<code>from_sam(sam_result)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from Segment Anything Model inference result.</p> <p>Parameters:</p> Name Type Description Default <code>sam_result</code> <code>List[dict]</code> <p>The output Results instance from SAM</p> required <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Example <pre><code>import supervision as sv\nfrom segment_anything import (\n    sam_model_registry,\n    SamAutomaticMaskGenerator\n )\n\nsam_model_reg = sam_model_registry[MODEL_TYPE]\nsam = sam_model_reg(checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\nmask_generator = SamAutomaticMaskGenerator(sam)\nsam_result = mask_generator.generate(IMAGE)\ndetections = sv.Detections.from_sam(sam_result=sam_result)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_sam(cls, sam_result: List[dict]) -&gt; Detections:\n    \"\"\"\n    Creates a Detections instance from\n    [Segment Anything Model](https://github.com/facebookresearch/segment-anything)\n    inference result.\n\n    Args:\n        sam_result (List[dict]): The output Results instance from SAM\n\n    Returns:\n        Detections: A new Detections object.\n\n    Example:\n        ```python\n        import supervision as sv\n        from segment_anything import (\n            sam_model_registry,\n            SamAutomaticMaskGenerator\n         )\n\n        sam_model_reg = sam_model_registry[MODEL_TYPE]\n        sam = sam_model_reg(checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n        mask_generator = SamAutomaticMaskGenerator(sam)\n        sam_result = mask_generator.generate(IMAGE)\n        detections = sv.Detections.from_sam(sam_result=sam_result)\n        ```\n    \"\"\"\n\n    sorted_generated_masks = sorted(\n        sam_result, key=lambda x: x[\"area\"], reverse=True\n    )\n\n    xywh = np.array([mask[\"bbox\"] for mask in sorted_generated_masks])\n    mask = np.array([mask[\"segmentation\"] for mask in sorted_generated_masks])\n\n    if np.asarray(xywh).shape[0] == 0:\n        return cls.empty()\n\n    xyxy = xywh_to_xyxy(xywh=xywh)\n    return cls(xyxy=xyxy, mask=mask)\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_tensorflow","title":"<code>from_tensorflow(tensorflow_results, resolution_wh)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from a Tensorflow Hub inference result.</p> <p>Parameters:</p> Name Type Description Default <code>tensorflow_results</code> <code>dict</code> <p>The output results from Tensorflow Hub.</p> required <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Example <pre><code>import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nimport cv2\n\nmodule_handle = \"https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1\"\nmodel = hub.load(module_handle)\nimg = np.array(cv2.imread(SOURCE_IMAGE_PATH))\nresult = model(img)\ndetections = sv.Detections.from_tensorflow(result)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_tensorflow(\n    cls, tensorflow_results: dict, resolution_wh: tuple\n) -&gt; Detections:\n    \"\"\"\n    Creates a Detections instance from a\n    [Tensorflow Hub](https://www.tensorflow.org/hub/tutorials/tf2_object_detection)\n    inference result.\n\n    Args:\n        tensorflow_results (dict):\n            The output results from Tensorflow Hub.\n\n    Returns:\n        Detections: A new Detections object.\n\n    Example:\n        ```python\n        import tensorflow as tf\n        import tensorflow_hub as hub\n        import numpy as np\n        import cv2\n\n        module_handle = \"https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1\"\n        model = hub.load(module_handle)\n        img = np.array(cv2.imread(SOURCE_IMAGE_PATH))\n        result = model(img)\n        detections = sv.Detections.from_tensorflow(result)\n        ```\n    \"\"\"  # noqa: E501 // docs\n\n    boxes = tensorflow_results[\"detection_boxes\"][0].numpy()\n    boxes[:, [0, 2]] *= resolution_wh[0]\n    boxes[:, [1, 3]] *= resolution_wh[1]\n    boxes = boxes[:, [1, 0, 3, 2]]\n    return cls(\n        xyxy=boxes,\n        confidence=tensorflow_results[\"detection_scores\"][0].numpy(),\n        class_id=tensorflow_results[\"detection_classes\"][0].numpy().astype(int),\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_transformers","title":"<code>from_transformers(transformers_results, id2label=None)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from object detection or panoptic, semantic and instance segmentation Transformer inference result.</p> <p>Parameters:</p> Name Type Description Default <code>transformers_results</code> <code>Union[dict, Tensor]</code> <p>Inference results from your Transformers model. This can be either a dictionary containing valuable outputs like <code>scores</code>, <code>labels</code>, <code>boxes</code>, <code>masks</code>, <code>segments_info</code>, and <code>segmentation</code>, or a <code>torch.Tensor</code> holding a segmentation map where values represent class IDs.</p> required <code>id2label</code> <code>Optional[Dict[int, str]]</code> <p>A dictionary mapping class IDs to labels, typically part of the <code>transformers</code> model configuration. If provided, the resulting dictionary will include class names.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Example <pre><code>import torch\nimport supervision as sv\nfrom PIL import Image\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n\nimage = Image.open(&lt;SOURCE_IMAGE_PATH&gt;)\ninputs = processor(images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nwidth, height = image.size\ntarget_size = torch.tensor([[height, width]])\nresults = processor.post_process_object_detection(\n    outputs=outputs, target_sizes=target_size)[0]\n\ndetections = sv.Detections.from_transformers(\n    transformers_results=results,\n    id2label=model.config.id2label\n)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_transformers(\n    cls, transformers_results: dict, id2label: Optional[Dict[int, str]] = None\n) -&gt; Detections:\n    \"\"\"\n    Creates a Detections instance from object detection or panoptic, semantic\n    and instance segmentation\n    [Transformer](https://github.com/huggingface/transformers) inference result.\n\n    Args:\n        transformers_results (Union[dict, torch.Tensor]):  Inference results from\n            your Transformers model. This can be either a dictionary containing\n            valuable outputs like `scores`, `labels`, `boxes`, `masks`,\n            `segments_info`, and `segmentation`, or a `torch.Tensor` holding a\n            segmentation map where values represent class IDs.\n        id2label (Optional[Dict[int, str]]): A dictionary mapping class IDs to\n            labels, typically part of the `transformers` model configuration. If\n            provided, the resulting dictionary will include class names.\n\n    Returns:\n        Detections: A new Detections object.\n\n    Example:\n        ```python\n        import torch\n        import supervision as sv\n        from PIL import Image\n        from transformers import DetrImageProcessor, DetrForObjectDetection\n\n        processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n        model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n\n        image = Image.open(&lt;SOURCE_IMAGE_PATH&gt;)\n        inputs = processor(images=image, return_tensors=\"pt\")\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        width, height = image.size\n        target_size = torch.tensor([[height, width]])\n        results = processor.post_process_object_detection(\n            outputs=outputs, target_sizes=target_size)[0]\n\n        detections = sv.Detections.from_transformers(\n            transformers_results=results,\n            id2label=model.config.id2label\n        )\n        ```\n    \"\"\"  # noqa: E501 // docs\n\n    if (\n        transformers_results.__class__.__name__ == \"Tensor\"\n        or \"segmentation\" in transformers_results\n    ):\n        return cls(\n            **process_transformers_v5_segmentation_result(\n                transformers_results, id2label\n            )\n        )\n\n    if \"masks\" in transformers_results or \"png_string\" in transformers_results:\n        return cls(\n            **process_transformers_v4_segmentation_result(\n                transformers_results, id2label\n            )\n        )\n\n    if \"boxes\" in transformers_results:\n        return cls(\n            **process_transformers_detection_result(transformers_results, id2label)\n        )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_ultralytics","title":"<code>from_ultralytics(ultralytics_results)</code>  <code>classmethod</code>","text":"<p>Creates a <code>sv.Detections</code> instance from a YOLOv8 inference result.</p> <p>Note</p> <p><code>from_ultralytics</code> is compatible with detection, segmentation, and OBB models.</p> <p>Parameters:</p> Name Type Description Default <code>ultralytics_results</code> <code>Results</code> <p>The output Results instance from Ultralytics</p> required <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Example <pre><code>import cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nmodel = YOLO('yolov8s.pt')\nresults = model(image)[0]\ndetections = sv.Detections.from_ultralytics(results)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_ultralytics(cls, ultralytics_results) -&gt; Detections:\n    \"\"\"\n    Creates a `sv.Detections` instance from a\n    [YOLOv8](https://github.com/ultralytics/ultralytics) inference result.\n\n    !!! Note\n\n        `from_ultralytics` is compatible with\n        [detection](https://docs.ultralytics.com/tasks/detect/),\n        [segmentation](https://docs.ultralytics.com/tasks/segment/), and\n        [OBB](https://docs.ultralytics.com/tasks/obb/) models.\n\n    Args:\n        ultralytics_results (ultralytics.yolo.engine.results.Results):\n            The output Results instance from Ultralytics\n\n    Returns:\n        Detections: A new Detections object.\n\n    Example:\n        ```python\n        import cv2\n        import supervision as sv\n        from ultralytics import YOLO\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        model = YOLO('yolov8s.pt')\n        results = model(image)[0]\n        detections = sv.Detections.from_ultralytics(results)\n        ```\n    \"\"\"  # noqa: E501 // docs\n\n    if hasattr(ultralytics_results, \"obb\") and ultralytics_results.obb is not None:\n        class_id = ultralytics_results.obb.cls.cpu().numpy().astype(int)\n        class_names = np.array([ultralytics_results.names[i] for i in class_id])\n        oriented_box_coordinates = ultralytics_results.obb.xyxyxyxy.cpu().numpy()\n        return cls(\n            xyxy=ultralytics_results.obb.xyxy.cpu().numpy(),\n            confidence=ultralytics_results.obb.conf.cpu().numpy(),\n            class_id=class_id,\n            tracker_id=ultralytics_results.obb.id.int().cpu().numpy()\n            if ultralytics_results.obb.id is not None\n            else None,\n            data={\n                ORIENTED_BOX_COORDINATES: oriented_box_coordinates,\n                CLASS_NAME_DATA_FIELD: class_names,\n            },\n        )\n\n    if hasattr(ultralytics_results, \"boxes\") and ultralytics_results.boxes is None:\n        masks = extract_ultralytics_masks(ultralytics_results)\n        return cls(\n            xyxy=mask_to_xyxy(masks),\n            mask=masks,\n            class_id=np.arange(len(ultralytics_results)),\n        )\n\n    class_id = ultralytics_results.boxes.cls.cpu().numpy().astype(int)\n    class_names = np.array([ultralytics_results.names[i] for i in class_id])\n    return cls(\n        xyxy=ultralytics_results.boxes.xyxy.cpu().numpy(),\n        confidence=ultralytics_results.boxes.conf.cpu().numpy(),\n        class_id=class_id,\n        mask=extract_ultralytics_masks(ultralytics_results),\n        tracker_id=ultralytics_results.boxes.id.int().cpu().numpy()\n        if ultralytics_results.boxes.id is not None\n        else None,\n        data={CLASS_NAME_DATA_FIELD: class_names},\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_yolo_nas","title":"<code>from_yolo_nas(yolo_nas_results)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from a YOLO-NAS inference result.</p> <p>Parameters:</p> Name Type Description Default <code>yolo_nas_results</code> <code>ImageDetectionPrediction</code> <p>The output Results instance from YOLO-NAS ImageDetectionPrediction is coming from 'super_gradients.training.models.prediction_results'</p> required <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Example <pre><code>import cv2\nfrom super_gradients.training import models\nimport supervision as sv\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nmodel = models.get('yolo_nas_l', pretrained_weights=\"coco\")\n\nresult = list(model.predict(image, conf=0.35))[0]\ndetections = sv.Detections.from_yolo_nas(result)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_yolo_nas(cls, yolo_nas_results) -&gt; Detections:\n    \"\"\"\n    Creates a Detections instance from a\n    [YOLO-NAS](https://github.com/Deci-AI/super-gradients/blob/master/YOLONAS.md)\n    inference result.\n\n    Args:\n        yolo_nas_results (ImageDetectionPrediction):\n            The output Results instance from YOLO-NAS\n            ImageDetectionPrediction is coming from\n            'super_gradients.training.models.prediction_results'\n\n    Returns:\n        Detections: A new Detections object.\n\n    Example:\n        ```python\n        import cv2\n        from super_gradients.training import models\n        import supervision as sv\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        model = models.get('yolo_nas_l', pretrained_weights=\"coco\")\n\n        result = list(model.predict(image, conf=0.35))[0]\n        detections = sv.Detections.from_yolo_nas(result)\n        ```\n    \"\"\"\n    if np.asarray(yolo_nas_results.prediction.bboxes_xyxy).shape[0] == 0:\n        return cls.empty()\n\n    return cls(\n        xyxy=yolo_nas_results.prediction.bboxes_xyxy,\n        confidence=yolo_nas_results.prediction.confidence,\n        class_id=yolo_nas_results.prediction.labels.astype(int),\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.from_yolov5","title":"<code>from_yolov5(yolov5_results)</code>  <code>classmethod</code>","text":"<p>Creates a Detections instance from a YOLOv5 inference result.</p> <p>Parameters:</p> Name Type Description Default <code>yolov5_results</code> <code>Detections</code> <p>The output Detections instance from YOLOv5</p> required <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object.</p> Example <pre><code>import cv2\nimport torch\nimport supervision as sv\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s')\nresult = model(image)\ndetections = sv.Detections.from_yolov5(result)\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef from_yolov5(cls, yolov5_results) -&gt; Detections:\n    \"\"\"\n    Creates a Detections instance from a\n    [YOLOv5](https://github.com/ultralytics/yolov5) inference result.\n\n    Args:\n        yolov5_results (yolov5.models.common.Detections):\n            The output Detections instance from YOLOv5\n\n    Returns:\n        Detections: A new Detections object.\n\n    Example:\n        ```python\n        import cv2\n        import torch\n        import supervision as sv\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n        result = model(image)\n        detections = sv.Detections.from_yolov5(result)\n        ```\n    \"\"\"\n    yolov5_detections_predictions = yolov5_results.pred[0].cpu().cpu().numpy()\n\n    return cls(\n        xyxy=yolov5_detections_predictions[:, :4],\n        confidence=yolov5_detections_predictions[:, 4],\n        class_id=yolov5_detections_predictions[:, 5].astype(int),\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.get_anchors_coordinates","title":"<code>get_anchors_coordinates(anchor)</code>","text":"<p>Calculates and returns the coordinates of a specific anchor point within the bounding boxes defined by the <code>xyxy</code> attribute. The anchor point can be any of the predefined positions in the <code>Position</code> enum, such as <code>CENTER</code>, <code>CENTER_LEFT</code>, <code>BOTTOM_RIGHT</code>, etc.</p> <p>Parameters:</p> Name Type Description Default <code>anchor</code> <code>Position</code> <p>An enum specifying the position of the anchor point within the bounding box. Supported positions are defined in the <code>Position</code> enum.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array of shape <code>(n, 2)</code>, where <code>n</code> is the number of bounding boxes. Each row contains the <code>[x, y]</code> coordinates of the specified anchor point for the corresponding bounding box.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided <code>anchor</code> is not supported.</p> Source code in <code>supervision/detection/core.py</code> <pre><code>def get_anchors_coordinates(self, anchor: Position) -&gt; np.ndarray:\n    \"\"\"\n    Calculates and returns the coordinates of a specific anchor point\n    within the bounding boxes defined by the `xyxy` attribute. The anchor\n    point can be any of the predefined positions in the `Position` enum,\n    such as `CENTER`, `CENTER_LEFT`, `BOTTOM_RIGHT`, etc.\n\n    Args:\n        anchor (Position): An enum specifying the position of the anchor point\n            within the bounding box. Supported positions are defined in the\n            `Position` enum.\n\n    Returns:\n        np.ndarray: An array of shape `(n, 2)`, where `n` is the number of bounding\n            boxes. Each row contains the `[x, y]` coordinates of the specified\n            anchor point for the corresponding bounding box.\n\n    Raises:\n        ValueError: If the provided `anchor` is not supported.\n    \"\"\"\n    if anchor == Position.CENTER:\n        return np.array(\n            [\n                (self.xyxy[:, 0] + self.xyxy[:, 2]) / 2,\n                (self.xyxy[:, 1] + self.xyxy[:, 3]) / 2,\n            ]\n        ).transpose()\n    elif anchor == Position.CENTER_OF_MASS:\n        if self.mask is None:\n            raise ValueError(\n                \"Cannot use `Position.CENTER_OF_MASS` without a detection mask.\"\n            )\n        return calculate_masks_centroids(masks=self.mask)\n    elif anchor == Position.CENTER_LEFT:\n        return np.array(\n            [\n                self.xyxy[:, 0],\n                (self.xyxy[:, 1] + self.xyxy[:, 3]) / 2,\n            ]\n        ).transpose()\n    elif anchor == Position.CENTER_RIGHT:\n        return np.array(\n            [\n                self.xyxy[:, 2],\n                (self.xyxy[:, 1] + self.xyxy[:, 3]) / 2,\n            ]\n        ).transpose()\n    elif anchor == Position.BOTTOM_CENTER:\n        return np.array(\n            [(self.xyxy[:, 0] + self.xyxy[:, 2]) / 2, self.xyxy[:, 3]]\n        ).transpose()\n    elif anchor == Position.BOTTOM_LEFT:\n        return np.array([self.xyxy[:, 0], self.xyxy[:, 3]]).transpose()\n    elif anchor == Position.BOTTOM_RIGHT:\n        return np.array([self.xyxy[:, 2], self.xyxy[:, 3]]).transpose()\n    elif anchor == Position.TOP_CENTER:\n        return np.array(\n            [(self.xyxy[:, 0] + self.xyxy[:, 2]) / 2, self.xyxy[:, 1]]\n        ).transpose()\n    elif anchor == Position.TOP_LEFT:\n        return np.array([self.xyxy[:, 0], self.xyxy[:, 1]]).transpose()\n    elif anchor == Position.TOP_RIGHT:\n        return np.array([self.xyxy[:, 2], self.xyxy[:, 1]]).transpose()\n\n    raise ValueError(f\"{anchor} is not supported.\")\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.is_empty","title":"<code>is_empty()</code>","text":"<p>Returns <code>True</code> if the <code>Detections</code> object is considered empty.</p> Source code in <code>supervision/detection/core.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"\n    Returns `True` if the `Detections` object is considered empty.\n    \"\"\"\n    empty_detections = Detections.empty()\n    empty_detections.data = self.data\n    return self == empty_detections\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.merge","title":"<code>merge(detections_list)</code>  <code>classmethod</code>","text":"<p>Merge a list of Detections objects into a single Detections object.</p> <p>This method takes a list of Detections objects and combines their respective fields (<code>xyxy</code>, <code>mask</code>, <code>confidence</code>, <code>class_id</code>, and <code>tracker_id</code>) into a single Detections object.</p> <p>For example, if merging Detections with 3 and 4 detected objects, this method will return a Detections with 7 objects (7 entries in <code>xyxy</code>, <code>mask</code>, etc).</p> <p>Note</p> <p>When merging, empty <code>Detections</code> objects are ignored.</p> <p>Parameters:</p> Name Type Description Default <code>detections_list</code> <code>List[Detections]</code> <p>A list of Detections objects to merge.</p> required <p>Returns:</p> Type Description <code>Detections</code> <p>A single Detections object containing the merged data from the input list.</p> Example <pre><code>import numpy as np\nimport supervision as sv\n\ndetections_1 = sv.Detections(\n    xyxy=np.array([[15, 15, 100, 100], [200, 200, 300, 300]]),\n    class_id=np.array([1, 2]),\n    data={'feature_vector': np.array([0.1, 0.2)])}\n )\n\ndetections_2 = sv.Detections(\n    xyxy=np.array([[30, 30, 120, 120]]),\n    class_id=np.array([1]),\n    data={'feature_vector': [np.array([0.3])]}\n )\n\nmerged_detections = Detections.merge([detections_1, detections_2])\n\nmerged_detections.xyxy\narray([[ 15,  15, 100, 100],\n       [200, 200, 300, 300],\n       [ 30,  30, 120, 120]])\n\nmerged_detections.class_id\narray([1, 2, 1])\n\nmerged_detections.data['feature_vector']\narray([0.1, 0.2, 0.3])\n</code></pre> Source code in <code>supervision/detection/core.py</code> <pre><code>@classmethod\ndef merge(cls, detections_list: List[Detections]) -&gt; Detections:\n    \"\"\"\n    Merge a list of Detections objects into a single Detections object.\n\n    This method takes a list of Detections objects and combines their\n    respective fields (`xyxy`, `mask`, `confidence`, `class_id`, and `tracker_id`)\n    into a single Detections object.\n\n    For example, if merging Detections with 3 and 4 detected objects, this method\n    will return a Detections with 7 objects (7 entries in `xyxy`, `mask`, etc).\n\n    !!! Note\n\n        When merging, empty `Detections` objects are ignored.\n\n    Args:\n        detections_list (List[Detections]): A list of Detections objects to merge.\n\n    Returns:\n        (Detections): A single Detections object containing\n            the merged data from the input list.\n\n    Example:\n        ```python\n        import numpy as np\n        import supervision as sv\n\n        detections_1 = sv.Detections(\n            xyxy=np.array([[15, 15, 100, 100], [200, 200, 300, 300]]),\n            class_id=np.array([1, 2]),\n            data={'feature_vector': np.array([0.1, 0.2)])}\n         )\n\n        detections_2 = sv.Detections(\n            xyxy=np.array([[30, 30, 120, 120]]),\n            class_id=np.array([1]),\n            data={'feature_vector': [np.array([0.3])]}\n         )\n\n        merged_detections = Detections.merge([detections_1, detections_2])\n\n        merged_detections.xyxy\n        array([[ 15,  15, 100, 100],\n               [200, 200, 300, 300],\n               [ 30,  30, 120, 120]])\n\n        merged_detections.class_id\n        array([1, 2, 1])\n\n        merged_detections.data['feature_vector']\n        array([0.1, 0.2, 0.3])\n        ```\n    \"\"\"\n    detections_list = [\n        detections for detections in detections_list if not detections.is_empty()\n    ]\n\n    if len(detections_list) == 0:\n        return Detections.empty()\n\n    for detections in detections_list:\n        validate_detections_fields(\n            xyxy=detections.xyxy,\n            mask=detections.mask,\n            confidence=detections.confidence,\n            class_id=detections.class_id,\n            tracker_id=detections.tracker_id,\n            data=detections.data,\n        )\n\n    xyxy = np.vstack([d.xyxy for d in detections_list])\n\n    def stack_or_none(name: str):\n        if all(d.__getattribute__(name) is None for d in detections_list):\n            return None\n        if any(d.__getattribute__(name) is None for d in detections_list):\n            raise ValueError(f\"All or none of the '{name}' fields must be None\")\n        return (\n            np.vstack([d.__getattribute__(name) for d in detections_list])\n            if name == \"mask\"\n            else np.hstack([d.__getattribute__(name) for d in detections_list])\n        )\n\n    mask = stack_or_none(\"mask\")\n    confidence = stack_or_none(\"confidence\")\n    class_id = stack_or_none(\"class_id\")\n    tracker_id = stack_or_none(\"tracker_id\")\n\n    data = merge_data([d.data for d in detections_list])\n\n    return cls(\n        xyxy=xyxy,\n        mask=mask,\n        confidence=confidence,\n        class_id=class_id,\n        tracker_id=tracker_id,\n        data=data,\n    )\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.with_nmm","title":"<code>with_nmm(threshold=0.5, class_agnostic=False)</code>","text":"<p>Perform non-maximum merging on the current set of object detections.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The intersection-over-union threshold to use for non-maximum merging. Defaults to 0.5.</p> <code>0.5</code> <code>class_agnostic</code> <code>bool</code> <p>Whether to perform class-agnostic non-maximum merging. If True, the class_id of each detection will be ignored. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object containing the subset of detections after non-maximum merging.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>confidence</code> is None or <code>class_id</code> is None and class_agnostic is False.</p> <p></p> Source code in <code>supervision/detection/core.py</code> <pre><code>def with_nmm(\n    self, threshold: float = 0.5, class_agnostic: bool = False\n) -&gt; Detections:\n    \"\"\"\n    Perform non-maximum merging on the current set of object detections.\n\n    Args:\n        threshold (float, optional): The intersection-over-union threshold\n            to use for non-maximum merging. Defaults to 0.5.\n        class_agnostic (bool, optional): Whether to perform class-agnostic\n            non-maximum merging. If True, the class_id of each detection\n            will be ignored. Defaults to False.\n\n    Returns:\n        Detections: A new Detections object containing the subset of detections\n            after non-maximum merging.\n\n    Raises:\n        AssertionError: If `confidence` is None or `class_id` is None and\n            class_agnostic is False.\n\n    ![non-max-merging](https://media.roboflow.com/supervision-docs/non-max-merging.png){ align=center width=\"800\" }\n    \"\"\"  # noqa: E501 // docs\n    if len(self) == 0:\n        return self\n\n    assert (\n        self.confidence is not None\n    ), \"Detections confidence must be given for NMM to be executed.\"\n\n    if class_agnostic:\n        predictions = np.hstack((self.xyxy, self.confidence.reshape(-1, 1)))\n    else:\n        assert self.class_id is not None, (\n            \"Detections class_id must be given for NMM to be executed. If you\"\n            \" intended to perform class agnostic NMM set class_agnostic=True.\"\n        )\n        predictions = np.hstack(\n            (\n                self.xyxy,\n                self.confidence.reshape(-1, 1),\n                self.class_id.reshape(-1, 1),\n            )\n        )\n\n    merge_groups = box_non_max_merge(\n        predictions=predictions, iou_threshold=threshold\n    )\n\n    result = []\n    for merge_group in merge_groups:\n        unmerged_detections = [self[i] for i in merge_group]\n        merged_detections = merge_inner_detections_objects(\n            unmerged_detections, threshold\n        )\n        result.append(merged_detections)\n\n    return Detections.merge(result)\n</code></pre>"},{"location":"detection/core/#supervision.detection.core.Detections.with_nms","title":"<code>with_nms(threshold=0.5, class_agnostic=False)</code>","text":"<p>Performs non-max suppression on detection set. If the detections result from a segmentation model, the IoU mask is applied. Otherwise, box IoU is used.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>The intersection-over-union threshold to use for non-maximum suppression. I'm the lower the value the more restrictive the NMS becomes. Defaults to 0.5.</p> <code>0.5</code> <code>class_agnostic</code> <code>bool</code> <p>Whether to perform class-agnostic non-maximum suppression. If True, the class_id of each detection will be ignored. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A new Detections object containing the subset of detections after non-maximum suppression.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>confidence</code> is None and class_agnostic is False. If <code>class_id</code> is None and class_agnostic is False.</p> Source code in <code>supervision/detection/core.py</code> <pre><code>def with_nms(\n    self, threshold: float = 0.5, class_agnostic: bool = False\n) -&gt; Detections:\n    \"\"\"\n    Performs non-max suppression on detection set. If the detections result\n    from a segmentation model, the IoU mask is applied. Otherwise, box IoU is used.\n\n    Args:\n        threshold (float, optional): The intersection-over-union threshold\n            to use for non-maximum suppression. I'm the lower the value the more\n            restrictive the NMS becomes. Defaults to 0.5.\n        class_agnostic (bool, optional): Whether to perform class-agnostic\n            non-maximum suppression. If True, the class_id of each detection\n            will be ignored. Defaults to False.\n\n    Returns:\n        Detections: A new Detections object containing the subset of detections\n            after non-maximum suppression.\n\n    Raises:\n        AssertionError: If `confidence` is None and class_agnostic is False.\n            If `class_id` is None and class_agnostic is False.\n    \"\"\"\n    if len(self) == 0:\n        return self\n\n    assert (\n        self.confidence is not None\n    ), \"Detections confidence must be given for NMS to be executed.\"\n\n    if class_agnostic:\n        predictions = np.hstack((self.xyxy, self.confidence.reshape(-1, 1)))\n    else:\n        assert self.class_id is not None, (\n            \"Detections class_id must be given for NMS to be executed. If you\"\n            \" intended to perform class agnostic NMS set class_agnostic=True.\"\n        )\n        predictions = np.hstack(\n            (\n                self.xyxy,\n                self.confidence.reshape(-1, 1),\n                self.class_id.reshape(-1, 1),\n            )\n        )\n\n    if self.mask is not None:\n        indices = mask_non_max_suppression(\n            predictions=predictions, masks=self.mask, iou_threshold=threshold\n        )\n    else:\n        indices = box_non_max_suppression(\n            predictions=predictions, iou_threshold=threshold\n        )\n\n    return self[indices]\n</code></pre>"},{"location":"detection/double_detection_filter/","title":"Double Detection Filter","text":"OverlapFilter <p>               Bases: <code>Enum</code></p> <p>Enum specifying the strategy for filtering overlapping detections.</p> <p>Attributes:</p> Name Type Description <code>NONE</code> <p>Do not filter detections based on overlap.</p> <code>NON_MAX_SUPPRESSION</code> <p>Filter detections using non-max suppression. This means, detections that overlap by more than a set threshold will be discarded, except for the one with the highest confidence.</p> <code>NON_MAX_MERGE</code> <p>Merge detections with non-max merging. This means, detections that overlap by more than a set threshold will be merged into a single detection.</p> Source code in <code>supervision/detection/overlap_filter.py</code> <pre><code>class OverlapFilter(Enum):\n    \"\"\"\n    Enum specifying the strategy for filtering overlapping detections.\n\n    Attributes:\n        NONE: Do not filter detections based on overlap.\n        NON_MAX_SUPPRESSION: Filter detections using non-max suppression. This means,\n            detections that overlap by more than a set threshold will be discarded,\n            except for the one with the highest confidence.\n        NON_MAX_MERGE: Merge detections with non-max merging. This means,\n            detections that overlap by more than a set threshold will be merged\n            into a single detection.\n    \"\"\"\n\n    NONE = \"none\"\n    NON_MAX_SUPPRESSION = \"non_max_suppression\"\n    NON_MAX_MERGE = \"non_max_merge\"\n\n    @classmethod\n    def list(cls):\n        return list(map(lambda c: c.value, cls))\n\n    @classmethod\n    def from_value(cls, value: Union[OverlapFilter, str]) -&gt; OverlapFilter:\n        if isinstance(value, cls):\n            return value\n        if isinstance(value, str):\n            value = value.lower()\n            try:\n                return cls(value)\n            except ValueError:\n                raise ValueError(f\"Invalid value: {value}. Must be one of {cls.list()}\")\n        raise ValueError(\n            f\"Invalid value type: {type(value)}. Must be an instance of \"\n            f\"{cls.__name__} or str.\"\n        )\n</code></pre> box_non_max_suppression <p>Perform Non-Maximum Suppression (NMS) on object detection predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>An array of object detection predictions in the format of <code>(x_min, y_min, x_max, y_max, score)</code> or <code>(x_min, y_min, x_max, y_max, score, class)</code>.</p> required <code>iou_threshold</code> <code>float</code> <p>The intersection-over-union threshold to use for non-maximum suppression.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A boolean array indicating which predictions to keep after n on-maximum suppression.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>iou_threshold</code> is not within the closed range from <code>0</code> to <code>1</code>.</p> Source code in <code>supervision/detection/overlap_filter.py</code> <pre><code>def box_non_max_suppression(\n    predictions: np.ndarray, iou_threshold: float = 0.5\n) -&gt; np.ndarray:\n    \"\"\"\n    Perform Non-Maximum Suppression (NMS) on object detection predictions.\n\n    Args:\n        predictions (np.ndarray): An array of object detection predictions in\n            the format of `(x_min, y_min, x_max, y_max, score)`\n            or `(x_min, y_min, x_max, y_max, score, class)`.\n        iou_threshold (float, optional): The intersection-over-union threshold\n            to use for non-maximum suppression.\n\n    Returns:\n        np.ndarray: A boolean array indicating which predictions to keep after n\n            on-maximum suppression.\n\n    Raises:\n        AssertionError: If `iou_threshold` is not within the\n            closed range from `0` to `1`.\n    \"\"\"\n    assert 0 &lt;= iou_threshold &lt;= 1, (\n        \"Value of `iou_threshold` must be in the closed range from 0 to 1, \"\n        f\"{iou_threshold} given.\"\n    )\n    rows, columns = predictions.shape\n\n    # add column #5 - category filled with zeros for agnostic nms\n    if columns == 5:\n        predictions = np.c_[predictions, np.zeros(rows)]\n\n    # sort predictions column #4 - score\n    sort_index = np.flip(predictions[:, 4].argsort())\n    predictions = predictions[sort_index]\n\n    boxes = predictions[:, :4]\n    categories = predictions[:, 5]\n    ious = box_iou_batch(boxes, boxes)\n    ious = ious - np.eye(rows)\n\n    keep = np.ones(rows, dtype=bool)\n\n    for index, (iou, category) in enumerate(zip(ious, categories)):\n        if not keep[index]:\n            continue\n\n        # drop detections with iou &gt; iou_threshold and\n        # same category as current detections\n        condition = (iou &gt; iou_threshold) &amp; (categories == category)\n        keep = keep &amp; ~condition\n\n    return keep[sort_index.argsort()]\n</code></pre> mask_non_max_suppression <p>Perform Non-Maximum Suppression (NMS) on segmentation predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>A 2D array of object detection predictions in the format of <code>(x_min, y_min, x_max, y_max, score)</code> or <code>(x_min, y_min, x_max, y_max, score, class)</code>. Shape: <code>(N, 5)</code> or <code>(N, 6)</code>, where N is the number of predictions.</p> required <code>masks</code> <code>ndarray</code> <p>A 3D array of binary masks corresponding to the predictions. Shape: <code>(N, H, W)</code>, where N is the number of predictions, and H, W are the dimensions of each mask.</p> required <code>iou_threshold</code> <code>float</code> <p>The intersection-over-union threshold to use for non-maximum suppression.</p> <code>0.5</code> <code>mask_dimension</code> <code>int</code> <p>The dimension to which the masks should be resized before computing IOU values. Defaults to 640.</p> <code>640</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A boolean array indicating which predictions to keep after non-maximum suppression.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>iou_threshold</code> is not within the closed</p> Source code in <code>supervision/detection/overlap_filter.py</code> <pre><code>def mask_non_max_suppression(\n    predictions: np.ndarray,\n    masks: np.ndarray,\n    iou_threshold: float = 0.5,\n    mask_dimension: int = 640,\n) -&gt; np.ndarray:\n    \"\"\"\n    Perform Non-Maximum Suppression (NMS) on segmentation predictions.\n\n    Args:\n        predictions (np.ndarray): A 2D array of object detection predictions in\n            the format of `(x_min, y_min, x_max, y_max, score)`\n            or `(x_min, y_min, x_max, y_max, score, class)`. Shape: `(N, 5)` or\n            `(N, 6)`, where N is the number of predictions.\n        masks (np.ndarray): A 3D array of binary masks corresponding to the predictions.\n            Shape: `(N, H, W)`, where N is the number of predictions, and H, W are the\n            dimensions of each mask.\n        iou_threshold (float, optional): The intersection-over-union threshold\n            to use for non-maximum suppression.\n        mask_dimension (int, optional): The dimension to which the masks should be\n            resized before computing IOU values. Defaults to 640.\n\n    Returns:\n        np.ndarray: A boolean array indicating which predictions to keep after\n            non-maximum suppression.\n\n    Raises:\n        AssertionError: If `iou_threshold` is not within the closed\n        range from `0` to `1`.\n    \"\"\"\n    assert 0 &lt;= iou_threshold &lt;= 1, (\n        \"Value of `iou_threshold` must be in the closed range from 0 to 1, \"\n        f\"{iou_threshold} given.\"\n    )\n    rows, columns = predictions.shape\n\n    if columns == 5:\n        predictions = np.c_[predictions, np.zeros(rows)]\n\n    sort_index = predictions[:, 4].argsort()[::-1]\n    predictions = predictions[sort_index]\n    masks = masks[sort_index]\n    masks_resized = resize_masks(masks, mask_dimension)\n    ious = mask_iou_batch(masks_resized, masks_resized)\n    categories = predictions[:, 5]\n\n    keep = np.ones(rows, dtype=bool)\n    for i in range(rows):\n        if keep[i]:\n            condition = (ious[i] &gt; iou_threshold) &amp; (categories[i] == categories)\n            keep[i + 1 :] = np.where(condition[i + 1 :], False, keep[i + 1 :])\n\n    return keep[sort_index.argsort()]\n</code></pre> box_non_max_merge <p>Apply greedy version of non-maximum merging per category to avoid detecting too many overlapping bounding boxes for a given object.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>NDArray[float64]</code> <p>An array of shape <code>(n, 5)</code> or <code>(n, 6)</code> containing the bounding boxes coordinates in format <code>[x1, y1, x2, y2]</code>, the confidence scores and class_ids. Omit class_id column to allow detections of different classes to be merged.</p> required <code>iou_threshold</code> <code>float</code> <p>The intersection-over-union threshold to use for non-maximum suppression. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>List[List[int]]</code> <p>List[List[int]]: Groups of prediction indices be merged. Each group may have 1 or more elements.</p> Source code in <code>supervision/detection/overlap_filter.py</code> <pre><code>def box_non_max_merge(\n    predictions: npt.NDArray[np.float64],\n    iou_threshold: float = 0.5,\n) -&gt; List[List[int]]:\n    \"\"\"\n    Apply greedy version of non-maximum merging per category to avoid detecting\n    too many overlapping bounding boxes for a given object.\n\n    Args:\n        predictions (npt.NDArray[np.float64]): An array of shape `(n, 5)` or `(n, 6)`\n            containing the bounding boxes coordinates in format `[x1, y1, x2, y2]`,\n            the confidence scores and class_ids. Omit class_id column to allow\n            detections of different classes to be merged.\n        iou_threshold (float, optional): The intersection-over-union threshold\n            to use for non-maximum suppression. Defaults to 0.5.\n\n    Returns:\n        List[List[int]]: Groups of prediction indices be merged.\n            Each group may have 1 or more elements.\n    \"\"\"\n    if predictions.shape[1] == 5:\n        return group_overlapping_boxes(predictions, iou_threshold)\n\n    category_ids = predictions[:, 5]\n    merge_groups = []\n    for category_id in np.unique(category_ids):\n        curr_indices = np.where(category_ids == category_id)[0]\n        merge_class_groups = group_overlapping_boxes(\n            predictions[curr_indices], iou_threshold\n        )\n\n        for merge_class_group in merge_class_groups:\n            merge_groups.append(curr_indices[merge_class_group].tolist())\n\n    for merge_group in merge_groups:\n        if len(merge_group) == 0:\n            raise ValueError(\n                f\"Empty group detected when non-max-merging \"\n                f\"detections: {merge_groups}\"\n            )\n    return merge_groups\n</code></pre>"},{"location":"detection/metrics/","title":"Metrics","text":"ConfusionMatrix <p>Confusion matrix for object detection tasks.</p> <p>Attributes:</p> Name Type Description <code>matrix</code> <code>ndarray</code> <p>An 2D <code>np.ndarray</code> of shape <code>(len(classes) + 1, len(classes) + 1)</code> containing the number of <code>TP</code>, <code>FP</code>, <code>FN</code> and <code>TN</code> for each class.</p> <code>classes</code> <code>List[str]</code> <p>Model class names.</p> <code>conf_threshold</code> <code>float</code> <p>Detection confidence threshold between <code>0</code> and <code>1</code>. Detections with lower confidence will be excluded from the matrix.</p> <code>iou_threshold</code> <code>float</code> <p>Detection IoU threshold between <code>0</code> and <code>1</code>. Detections with lower IoU will be classified as <code>FP</code>.</p> Source code in <code>supervision/metrics/detection.py</code> <pre><code>@dataclass\nclass ConfusionMatrix:\n    \"\"\"\n    Confusion matrix for object detection tasks.\n\n    Attributes:\n        matrix (np.ndarray): An 2D `np.ndarray` of shape\n            `(len(classes) + 1, len(classes) + 1)`\n            containing the number of `TP`, `FP`, `FN` and `TN` for each class.\n        classes (List[str]): Model class names.\n        conf_threshold (float): Detection confidence threshold between `0` and `1`.\n            Detections with lower confidence will be excluded from the matrix.\n        iou_threshold (float): Detection IoU threshold between `0` and `1`.\n            Detections with lower IoU will be classified as `FP`.\n    \"\"\"\n\n    matrix: np.ndarray\n    classes: List[str]\n    conf_threshold: float\n    iou_threshold: float\n\n    @classmethod\n    def from_detections(\n        cls,\n        predictions: List[Detections],\n        targets: List[Detections],\n        classes: List[str],\n        conf_threshold: float = 0.3,\n        iou_threshold: float = 0.5,\n    ) -&gt; ConfusionMatrix:\n        \"\"\"\n        Calculate confusion matrix based on predicted and ground-truth detections.\n\n        Args:\n            targets (List[Detections]): Detections objects from ground-truth.\n            predictions (List[Detections]): Detections objects predicted by the model.\n            classes (List[str]): Model class names.\n            conf_threshold (float): Detection confidence threshold between `0` and `1`.\n                Detections with lower confidence will be excluded.\n            iou_threshold (float): Detection IoU threshold between `0` and `1`.\n                Detections with lower IoU will be classified as `FP`.\n\n        Returns:\n            ConfusionMatrix: New instance of ConfusionMatrix.\n\n        Example:\n            ```python\n            import supervision as sv\n\n            targets = [\n                sv.Detections(...),\n                sv.Detections(...)\n            ]\n\n            predictions = [\n                sv.Detections(...),\n                sv.Detections(...)\n            ]\n\n            confusion_matrix = sv.ConfusionMatrix.from_detections(\n                predictions=predictions,\n                targets=target,\n                classes=['person', ...]\n            )\n\n            print(confusion_matrix.matrix)\n            # np.array([\n            #    [0., 0., 0., 0.],\n            #    [0., 1., 0., 1.],\n            #    [0., 1., 1., 0.],\n            #    [1., 1., 0., 0.]\n            # ])\n            ```\n        \"\"\"\n\n        prediction_tensors = []\n        target_tensors = []\n        for prediction, target in zip(predictions, targets):\n            prediction_tensors.append(\n                detections_to_tensor(prediction, with_confidence=True)\n            )\n            target_tensors.append(detections_to_tensor(target, with_confidence=False))\n        return cls.from_tensors(\n            predictions=prediction_tensors,\n            targets=target_tensors,\n            classes=classes,\n            conf_threshold=conf_threshold,\n            iou_threshold=iou_threshold,\n        )\n\n    @classmethod\n    def from_tensors(\n        cls,\n        predictions: List[np.ndarray],\n        targets: List[np.ndarray],\n        classes: List[str],\n        conf_threshold: float = 0.3,\n        iou_threshold: float = 0.5,\n    ) -&gt; ConfusionMatrix:\n        \"\"\"\n        Calculate confusion matrix based on predicted and ground-truth detections.\n\n        Args:\n            predictions (List[np.ndarray]): Each element of the list describes a single\n                image and has `shape = (M, 6)` where `M` is the number of detected\n                objects. Each row is expected to be in\n                `(x_min, y_min, x_max, y_max, class, conf)` format.\n            targets (List[np.ndarray]): Each element of the list describes a single\n                image and has `shape = (N, 5)` where `N` is the number of\n                ground-truth objects. Each row is expected to be in\n                `(x_min, y_min, x_max, y_max, class)` format.\n            classes (List[str]): Model class names.\n            conf_threshold (float): Detection confidence threshold between `0` and `1`.\n                Detections with lower confidence will be excluded.\n            iou_threshold (float): Detection iou  threshold between `0` and `1`.\n                Detections with lower iou will be classified as `FP`.\n\n        Returns:\n            ConfusionMatrix: New instance of ConfusionMatrix.\n\n        Example:\n            ```python\n            import supervision as sv\n            import numpy as np\n\n            targets = (\n                [\n                    np.array(\n                        [\n                            [0.0, 0.0, 3.0, 3.0, 1],\n                            [2.0, 2.0, 5.0, 5.0, 1],\n                            [6.0, 1.0, 8.0, 3.0, 2],\n                        ]\n                    ),\n                    np.array([1.0, 1.0, 2.0, 2.0, 2]),\n                ]\n            )\n\n            predictions = [\n                np.array(\n                    [\n                        [0.0, 0.0, 3.0, 3.0, 1, 0.9],\n                        [0.1, 0.1, 3.0, 3.0, 0, 0.9],\n                        [6.0, 1.0, 8.0, 3.0, 1, 0.8],\n                        [1.0, 6.0, 2.0, 7.0, 1, 0.8],\n                    ]\n                ),\n                np.array([[1.0, 1.0, 2.0, 2.0, 2, 0.8]])\n            ]\n\n            confusion_matrix = sv.ConfusionMatrix.from_tensors(\n                predictions=predictions,\n                targets=targets,\n                classes=['person', ...]\n            )\n\n            print(confusion_matrix.matrix)\n            # np.array([\n            #     [0., 0., 0., 0.],\n            #     [0., 1., 0., 1.],\n            #     [0., 1., 1., 0.],\n            #     [1., 1., 0., 0.]\n            # ])\n            ```\n        \"\"\"\n        validate_input_tensors(predictions, targets)\n\n        num_classes = len(classes)\n        matrix = np.zeros((num_classes + 1, num_classes + 1))\n        for true_batch, detection_batch in zip(targets, predictions):\n            matrix += cls.evaluate_detection_batch(\n                predictions=detection_batch,\n                targets=true_batch,\n                num_classes=num_classes,\n                conf_threshold=conf_threshold,\n                iou_threshold=iou_threshold,\n            )\n        return cls(\n            matrix=matrix,\n            classes=classes,\n            conf_threshold=conf_threshold,\n            iou_threshold=iou_threshold,\n        )\n\n    @staticmethod\n    def evaluate_detection_batch(\n        predictions: np.ndarray,\n        targets: np.ndarray,\n        num_classes: int,\n        conf_threshold: float,\n        iou_threshold: float,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Calculate confusion matrix for a batch of detections for a single image.\n\n        Args:\n            predictions (np.ndarray): Batch prediction. Describes a single image and\n                has `shape = (M, 6)` where `M` is the number of detected objects.\n                Each row is expected to be in\n                `(x_min, y_min, x_max, y_max, class, conf)` format.\n            targets (np.ndarray): Batch target labels. Describes a single image and\n                has `shape = (N, 5)` where `N` is the number of ground-truth objects.\n                Each row is expected to be in\n                `(x_min, y_min, x_max, y_max, class)` format.\n            num_classes (int): Number of classes.\n            conf_threshold (float): Detection confidence threshold between `0` and `1`.\n                Detections with lower confidence will be excluded.\n            iou_threshold (float): Detection iou  threshold between `0` and `1`.\n                Detections with lower iou will be classified as `FP`.\n\n        Returns:\n            np.ndarray: Confusion matrix based on a single image.\n        \"\"\"\n        result_matrix = np.zeros((num_classes + 1, num_classes + 1))\n\n        conf_idx = 5\n        confidence = predictions[:, conf_idx]\n        detection_batch_filtered = predictions[confidence &gt; conf_threshold]\n\n        class_id_idx = 4\n        true_classes = np.array(targets[:, class_id_idx], dtype=np.int16)\n        detection_classes = np.array(\n            detection_batch_filtered[:, class_id_idx], dtype=np.int16\n        )\n        true_boxes = targets[:, :class_id_idx]\n        detection_boxes = detection_batch_filtered[:, :class_id_idx]\n\n        iou_batch = box_iou_batch(\n            boxes_true=true_boxes, boxes_detection=detection_boxes\n        )\n        matched_idx = np.asarray(iou_batch &gt; iou_threshold).nonzero()\n\n        if matched_idx[0].shape[0]:\n            matches = np.stack(\n                (matched_idx[0], matched_idx[1], iou_batch[matched_idx]), axis=1\n            )\n            matches = ConfusionMatrix._drop_extra_matches(matches=matches)\n        else:\n            matches = np.zeros((0, 3))\n\n        matched_true_idx, matched_detection_idx, _ = matches.transpose().astype(\n            np.int16\n        )\n\n        for i, true_class_value in enumerate(true_classes):\n            j = matched_true_idx == i\n            if matches.shape[0] &gt; 0 and sum(j) == 1:\n                result_matrix[\n                    true_class_value, detection_classes[matched_detection_idx[j]]\n                ] += 1  # TP\n            else:\n                result_matrix[true_class_value, num_classes] += 1  # FN\n\n        for i, detection_class_value in enumerate(detection_classes):\n            if not any(matched_detection_idx == i):\n                result_matrix[num_classes, detection_class_value] += 1  # FP\n\n        return result_matrix\n\n    @staticmethod\n    def _drop_extra_matches(matches: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Deduplicate matches. If there are multiple matches for the same true or\n        predicted box, only the one with the highest IoU is kept.\n        \"\"\"\n        if matches.shape[0] &gt; 0:\n            matches = matches[matches[:, 2].argsort()[::-1]]\n            matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n            matches = matches[matches[:, 2].argsort()[::-1]]\n            matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n        return matches\n\n    @classmethod\n    def benchmark(\n        cls,\n        dataset: DetectionDataset,\n        callback: Callable[[np.ndarray], Detections],\n        conf_threshold: float = 0.3,\n        iou_threshold: float = 0.5,\n    ) -&gt; ConfusionMatrix:\n        \"\"\"\n        Calculate confusion matrix from dataset and callback function.\n\n        Args:\n            dataset (DetectionDataset): Object detection dataset used for evaluation.\n            callback (Callable[[np.ndarray], Detections]): Function that takes an image\n                as input and returns Detections object.\n            conf_threshold (float): Detection confidence threshold between `0` and `1`.\n                Detections with lower confidence will be excluded.\n            iou_threshold (float): Detection IoU threshold between `0` and `1`.\n                Detections with lower IoU will be classified as `FP`.\n\n        Returns:\n            ConfusionMatrix: New instance of ConfusionMatrix.\n\n        Example:\n            ```python\n            import supervision as sv\n            from ultralytics import YOLO\n\n            dataset = sv.DetectionDataset.from_yolo(...)\n\n            model = YOLO(...)\n            def callback(image: np.ndarray) -&gt; sv.Detections:\n                result = model(image)[0]\n                return sv.Detections.from_ultralytics(result)\n\n            confusion_matrix = sv.ConfusionMatrix.benchmark(\n                dataset = dataset,\n                callback = callback\n            )\n\n            print(confusion_matrix.matrix)\n            # np.array([\n            #     [0., 0., 0., 0.],\n            #     [0., 1., 0., 1.],\n            #     [0., 1., 1., 0.],\n            #     [1., 1., 0., 0.]\n            # ])\n            ```\n        \"\"\"\n        predictions, targets = [], []\n        for _, image, annotation in dataset:\n            predictions_batch = callback(image)\n            predictions.append(predictions_batch)\n            targets.append(annotation)\n        return cls.from_detections(\n            predictions=predictions,\n            targets=targets,\n            classes=dataset.classes,\n            conf_threshold=conf_threshold,\n            iou_threshold=iou_threshold,\n        )\n\n    def plot(\n        self,\n        save_path: Optional[str] = None,\n        title: Optional[str] = None,\n        classes: Optional[List[str]] = None,\n        normalize: bool = False,\n        fig_size: Tuple[int, int] = (12, 10),\n    ) -&gt; matplotlib.figure.Figure:\n        \"\"\"\n        Create confusion matrix plot and save it at selected location.\n\n        Args:\n            save_path (Optional[str]): Path to save the plot. If not provided,\n                plot will be displayed.\n            title (Optional[str]): Title of the plot.\n            classes (Optional[List[str]]): List of classes to be displayed on the plot.\n                If not provided, all classes will be displayed.\n            normalize (bool): If True, normalize the confusion matrix.\n            fig_size (Tuple[int, int]): Size of the plot.\n\n        Returns:\n            matplotlib.figure.Figure: Confusion matrix plot.\n        \"\"\"\n\n        array = self.matrix.copy()\n\n        if normalize:\n            eps = 1e-8\n            array = array / (array.sum(0).reshape(1, -1) + eps)\n\n        array[array &lt; 0.005] = np.nan\n\n        fig, ax = plt.subplots(figsize=fig_size, tight_layout=True, facecolor=\"white\")\n\n        class_names = classes if classes is not None else self.classes\n        use_labels_for_ticks = class_names is not None and (0 &lt; len(class_names) &lt; 99)\n        if use_labels_for_ticks:\n            x_tick_labels = class_names + [\"FN\"]\n            y_tick_labels = class_names + [\"FP\"]\n            num_ticks = len(x_tick_labels)\n        else:\n            x_tick_labels = None\n            y_tick_labels = None\n            num_ticks = len(array)\n        im = ax.imshow(array, cmap=\"Blues\")\n\n        cbar = ax.figure.colorbar(im, ax=ax)\n        cbar.mappable.set_clim(vmin=0, vmax=np.nanmax(array))\n\n        if x_tick_labels is None:\n            tick_interval = 2\n        else:\n            tick_interval = 1\n        ax.set_xticks(np.arange(0, num_ticks, tick_interval), labels=x_tick_labels)\n        ax.set_yticks(np.arange(0, num_ticks, tick_interval), labels=y_tick_labels)\n\n        plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\", rotation_mode=\"default\")\n\n        labelsize = 10 if num_ticks &lt; 50 else 8\n        ax.tick_params(axis=\"both\", which=\"both\", labelsize=labelsize)\n\n        if num_ticks &lt; 30:\n            for i in range(array.shape[0]):\n                for j in range(array.shape[1]):\n                    n_preds = array[i, j]\n                    if not np.isnan(n_preds):\n                        ax.text(\n                            j,\n                            i,\n                            f\"{n_preds:.2f}\" if normalize else f\"{n_preds:.0f}\",\n                            ha=\"center\",\n                            va=\"center\",\n                            color=\"black\"\n                            if n_preds &lt; 0.5 * np.nanmax(array)\n                            else \"white\",\n                        )\n\n        if title:\n            ax.set_title(title, fontsize=20)\n\n        ax.set_xlabel(\"Predicted\")\n        ax.set_ylabel(\"True\")\n        ax.set_facecolor(\"white\")\n        if save_path:\n            fig.savefig(\n                save_path, dpi=250, facecolor=fig.get_facecolor(), transparent=True\n            )\n        return fig\n</code></pre> MeanAveragePrecision <p>Mean Average Precision for object detection tasks.</p> <p>Attributes:</p> Name Type Description <code>map50_95</code> <code>float</code> <p>Mean Average Precision (mAP) calculated over IoU thresholds ranging from <code>0.50</code> to <code>0.95</code> with a step size of <code>0.05</code>.</p> <code>map50</code> <code>float</code> <p>Mean Average Precision (mAP) calculated specifically at an IoU threshold of <code>0.50</code>.</p> <code>map75</code> <code>float</code> <p>Mean Average Precision (mAP) calculated specifically at an IoU threshold of <code>0.75</code>.</p> <code>per_class_ap50_95</code> <code>ndarray</code> <p>Average Precision (AP) values calculated over IoU thresholds ranging from <code>0.50</code> to <code>0.95</code> with a step size of <code>0.05</code>, provided for each individual class.</p> Source code in <code>supervision/metrics/detection.py</code> <pre><code>@dataclass(frozen=True)\nclass MeanAveragePrecision:\n    \"\"\"\n    Mean Average Precision for object detection tasks.\n\n    Attributes:\n        map50_95 (float): Mean Average Precision (mAP) calculated over IoU thresholds\n            ranging from `0.50` to `0.95` with a step size of `0.05`.\n        map50 (float): Mean Average Precision (mAP) calculated specifically at\n            an IoU threshold of `0.50`.\n        map75 (float): Mean Average Precision (mAP) calculated specifically at\n            an IoU threshold of `0.75`.\n        per_class_ap50_95 (np.ndarray): Average Precision (AP) values calculated over\n            IoU thresholds ranging from `0.50` to `0.95` with a step size of `0.05`,\n            provided for each individual class.\n    \"\"\"\n\n    map50_95: float\n    map50: float\n    map75: float\n    per_class_ap50_95: np.ndarray\n\n    @classmethod\n    def from_detections(\n        cls,\n        predictions: List[Detections],\n        targets: List[Detections],\n    ) -&gt; MeanAveragePrecision:\n        \"\"\"\n        Calculate mean average precision based on predicted and ground-truth detections.\n\n        Args:\n            targets (List[Detections]): Detections objects from ground-truth.\n            predictions (List[Detections]): Detections objects predicted by the model.\n        Returns:\n            MeanAveragePrecision: New instance of ConfusionMatrix.\n\n        Example:\n            ```python\n            import supervision as sv\n\n            targets = [\n                sv.Detections(...),\n                sv.Detections(...)\n            ]\n\n            predictions = [\n                sv.Detections(...),\n                sv.Detections(...)\n            ]\n\n            mean_average_precision = sv.MeanAveragePrecision.from_detections(\n                predictions=predictions,\n                targets=target,\n            )\n\n            print(mean_average_precison.map50_95)\n            # 0.2899\n            ```\n        \"\"\"\n        prediction_tensors = []\n        target_tensors = []\n        for prediction, target in zip(predictions, targets):\n            prediction_tensors.append(\n                detections_to_tensor(prediction, with_confidence=True)\n            )\n            target_tensors.append(detections_to_tensor(target, with_confidence=False))\n        return cls.from_tensors(\n            predictions=prediction_tensors,\n            targets=target_tensors,\n        )\n\n    @classmethod\n    def benchmark(\n        cls,\n        dataset: DetectionDataset,\n        callback: Callable[[np.ndarray], Detections],\n    ) -&gt; MeanAveragePrecision:\n        \"\"\"\n        Calculate mean average precision from dataset and callback function.\n\n        Args:\n            dataset (DetectionDataset): Object detection dataset used for evaluation.\n            callback (Callable[[np.ndarray], Detections]): Function that takes\n                an image as input and returns Detections object.\n        Returns:\n            MeanAveragePrecision: New instance of MeanAveragePrecision.\n\n        Example:\n            ```python\n            import supervision as sv\n            from ultralytics import YOLO\n\n            dataset = sv.DetectionDataset.from_yolo(...)\n\n            model = YOLO(...)\n            def callback(image: np.ndarray) -&gt; sv.Detections:\n                result = model(image)[0]\n                return sv.Detections.from_ultralytics(result)\n\n            mean_average_precision = sv.MeanAveragePrecision.benchmark(\n                dataset = dataset,\n                callback = callback\n            )\n\n            print(mean_average_precision.map50_95)\n            # 0.433\n            ```\n        \"\"\"\n        predictions, targets = [], []\n        for _, image, annotation in dataset:\n            predictions_batch = callback(image)\n            predictions.append(predictions_batch)\n            targets.append(annotation)\n        return cls.from_detections(\n            predictions=predictions,\n            targets=targets,\n        )\n\n    @classmethod\n    def from_tensors(\n        cls,\n        predictions: List[np.ndarray],\n        targets: List[np.ndarray],\n    ) -&gt; MeanAveragePrecision:\n        \"\"\"\n        Calculate Mean Average Precision based on predicted and ground-truth\n            detections at different threshold.\n\n        Args:\n            predictions (List[np.ndarray]): Each element of the list describes\n                a single image and has `shape = (M, 6)` where `M` is\n                the number of detected objects. Each row is expected to be\n                in `(x_min, y_min, x_max, y_max, class, conf)` format.\n            targets (List[np.ndarray]): Each element of the list describes a single\n                image and has `shape = (N, 5)` where `N` is the\n                number of ground-truth objects. Each row is expected to be in\n                `(x_min, y_min, x_max, y_max, class)` format.\n        Returns:\n            MeanAveragePrecision: New instance of MeanAveragePrecision.\n\n        Example:\n            ```python\n            import supervision as sv\n            import numpy as np\n\n            targets = (\n                [\n                    np.array(\n                        [\n                            [0.0, 0.0, 3.0, 3.0, 1],\n                            [2.0, 2.0, 5.0, 5.0, 1],\n                            [6.0, 1.0, 8.0, 3.0, 2],\n                        ]\n                    ),\n                    np.array([[1.0, 1.0, 2.0, 2.0, 2]]),\n                ]\n            )\n\n            predictions = [\n                np.array(\n                    [\n                        [0.0, 0.0, 3.0, 3.0, 1, 0.9],\n                        [0.1, 0.1, 3.0, 3.0, 0, 0.9],\n                        [6.0, 1.0, 8.0, 3.0, 1, 0.8],\n                        [1.0, 6.0, 2.0, 7.0, 1, 0.8],\n                    ]\n                ),\n                np.array([[1.0, 1.0, 2.0, 2.0, 2, 0.8]])\n            ]\n\n            mean_average_precison = sv.MeanAveragePrecision.from_tensors(\n                predictions=predictions,\n                targets=targets,\n            )\n\n            print(mean_average_precison.map50_95)\n            # 0.6649\n            ```\n        \"\"\"\n        validate_input_tensors(predictions, targets)\n        iou_thresholds = np.linspace(0.5, 0.95, 10)\n        stats = []\n\n        # Gather matching stats for predictions and targets\n        for true_objs, predicted_objs in zip(targets, predictions):\n            if predicted_objs.shape[0] == 0:\n                if true_objs.shape[0]:\n                    stats.append(\n                        (\n                            np.zeros((0, iou_thresholds.size), dtype=bool),\n                            *np.zeros((2, 0)),\n                            true_objs[:, 4],\n                        )\n                    )\n                continue\n\n            if true_objs.shape[0]:\n                matches = cls._match_detection_batch(\n                    predicted_objs, true_objs, iou_thresholds\n                )\n                stats.append(\n                    (\n                        matches,\n                        predicted_objs[:, 5],\n                        predicted_objs[:, 4],\n                        true_objs[:, 4],\n                    )\n                )\n\n        # Compute average precisions if any matches exist\n        if stats:\n            concatenated_stats = [np.concatenate(items, 0) for items in zip(*stats)]\n            average_precisions = cls._average_precisions_per_class(*concatenated_stats)\n            map50 = average_precisions[:, 0].mean()\n            map75 = average_precisions[:, 5].mean()\n            map50_95 = average_precisions.mean()\n        else:\n            map50, map75, map50_95 = 0, 0, 0\n            average_precisions = []\n\n        return cls(\n            map50_95=map50_95,\n            map50=map50,\n            map75=map75,\n            per_class_ap50_95=average_precisions,\n        )\n\n    @staticmethod\n    def compute_average_precision(recall: np.ndarray, precision: np.ndarray) -&gt; float:\n        \"\"\"\n        Compute the average precision using 101-point interpolation (COCO), given\n            the recall and precision curves.\n\n        Args:\n            recall (np.ndarray): The recall curve.\n            precision (np.ndarray): The precision curve.\n\n        Returns:\n            float: Average precision.\n        \"\"\"\n        extended_recall = np.concatenate(([0.0], recall, [1.0]))\n        extended_precision = np.concatenate(([1.0], precision, [0.0]))\n        max_accumulated_precision = np.flip(\n            np.maximum.accumulate(np.flip(extended_precision))\n        )\n        interpolated_recall_levels = np.linspace(0, 1, 101)\n        interpolated_precision = np.interp(\n            interpolated_recall_levels, extended_recall, max_accumulated_precision\n        )\n        average_precision = np.trapz(interpolated_precision, interpolated_recall_levels)\n        return average_precision\n\n    @staticmethod\n    def _match_detection_batch(\n        predictions: np.ndarray, targets: np.ndarray, iou_thresholds: np.ndarray\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Match predictions with target labels based on IoU levels.\n\n        Args:\n            predictions (np.ndarray): Batch prediction. Describes a single image and\n                has `shape = (M, 6)` where `M` is the number of detected objects.\n                Each row is expected to be in\n                `(x_min, y_min, x_max, y_max, class, conf)` format.\n            targets (np.ndarray): Batch target labels. Describes a single image and\n                has `shape = (N, 5)` where `N` is the number of ground-truth objects.\n                Each row is expected to be in\n                `(x_min, y_min, x_max, y_max, class)` format.\n            iou_thresholds (np.ndarray): Array contains different IoU thresholds.\n\n        Returns:\n            np.ndarray: Matched prediction with target labels result.\n        \"\"\"\n        num_predictions, num_iou_levels = predictions.shape[0], iou_thresholds.shape[0]\n        correct = np.zeros((num_predictions, num_iou_levels), dtype=bool)\n        iou = box_iou_batch(targets[:, :4], predictions[:, :4])\n        correct_class = targets[:, 4:5] == predictions[:, 4]\n\n        for i, iou_level in enumerate(iou_thresholds):\n            matched_indices = np.where((iou &gt;= iou_level) &amp; correct_class)\n\n            if matched_indices[0].shape[0]:\n                combined_indices = np.stack(matched_indices, axis=1)\n                iou_values = iou[matched_indices][:, None]\n                matches = np.hstack([combined_indices, iou_values])\n\n                if matched_indices[0].shape[0] &gt; 1:\n                    matches = matches[matches[:, 2].argsort()[::-1]]\n                    matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n                    matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n\n                correct[matches[:, 1].astype(int), i] = True\n\n        return correct\n\n    @staticmethod\n    def _average_precisions_per_class(\n        matches: np.ndarray,\n        prediction_confidence: np.ndarray,\n        prediction_class_ids: np.ndarray,\n        true_class_ids: np.ndarray,\n        eps: float = 1e-16,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Compute the average precision, given the recall and precision curves.\n        Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n\n        Args:\n            matches (np.ndarray): True positives.\n            prediction_confidence (np.ndarray): Objectness value from 0-1.\n            prediction_class_ids (np.ndarray): Predicted object classes.\n            true_class_ids (np.ndarray): True object classes.\n            eps (float, optional): Small value to prevent division by zero.\n\n        Returns:\n            np.ndarray: Average precision for different IoU levels.\n        \"\"\"\n        sorted_indices = np.argsort(-prediction_confidence)\n        matches = matches[sorted_indices]\n        prediction_class_ids = prediction_class_ids[sorted_indices]\n\n        unique_classes, class_counts = np.unique(true_class_ids, return_counts=True)\n        num_classes = unique_classes.shape[0]\n\n        average_precisions = np.zeros((num_classes, matches.shape[1]))\n\n        for class_idx, class_id in enumerate(unique_classes):\n            is_class = prediction_class_ids == class_id\n            total_true = class_counts[class_idx]\n            total_prediction = is_class.sum()\n\n            if total_prediction == 0 or total_true == 0:\n                continue\n\n            false_positives = (1 - matches[is_class]).cumsum(0)\n            true_positives = matches[is_class].cumsum(0)\n            recall = true_positives / (total_true + eps)\n            precision = true_positives / (true_positives + false_positives)\n\n            for iou_level_idx in range(matches.shape[1]):\n                average_precisions[class_idx, iou_level_idx] = (\n                    MeanAveragePrecision.compute_average_precision(\n                        recall[:, iou_level_idx], precision[:, iou_level_idx]\n                    )\n                )\n\n        return average_precisions\n</code></pre>"},{"location":"detection/metrics/#supervision.metrics.detection.ConfusionMatrix-functions","title":"Functions","text":""},{"location":"detection/metrics/#supervision.metrics.detection.ConfusionMatrix.benchmark","title":"<code>benchmark(dataset, callback, conf_threshold=0.3, iou_threshold=0.5)</code>  <code>classmethod</code>","text":"<p>Calculate confusion matrix from dataset and callback function.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DetectionDataset</code> <p>Object detection dataset used for evaluation.</p> required <code>callback</code> <code>Callable[[ndarray], Detections]</code> <p>Function that takes an image as input and returns Detections object.</p> required <code>conf_threshold</code> <code>float</code> <p>Detection confidence threshold between <code>0</code> and <code>1</code>. Detections with lower confidence will be excluded.</p> <code>0.3</code> <code>iou_threshold</code> <code>float</code> <p>Detection IoU threshold between <code>0</code> and <code>1</code>. Detections with lower IoU will be classified as <code>FP</code>.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>ConfusionMatrix</code> <code>ConfusionMatrix</code> <p>New instance of ConfusionMatrix.</p> Example <pre><code>import supervision as sv\nfrom ultralytics import YOLO\n\ndataset = sv.DetectionDataset.from_yolo(...)\n\nmodel = YOLO(...)\ndef callback(image: np.ndarray) -&gt; sv.Detections:\n    result = model(image)[0]\n    return sv.Detections.from_ultralytics(result)\n\nconfusion_matrix = sv.ConfusionMatrix.benchmark(\n    dataset = dataset,\n    callback = callback\n)\n\nprint(confusion_matrix.matrix)\n# np.array([\n#     [0., 0., 0., 0.],\n#     [0., 1., 0., 1.],\n#     [0., 1., 1., 0.],\n#     [1., 1., 0., 0.]\n# ])\n</code></pre> Source code in <code>supervision/metrics/detection.py</code> <pre><code>@classmethod\ndef benchmark(\n    cls,\n    dataset: DetectionDataset,\n    callback: Callable[[np.ndarray], Detections],\n    conf_threshold: float = 0.3,\n    iou_threshold: float = 0.5,\n) -&gt; ConfusionMatrix:\n    \"\"\"\n    Calculate confusion matrix from dataset and callback function.\n\n    Args:\n        dataset (DetectionDataset): Object detection dataset used for evaluation.\n        callback (Callable[[np.ndarray], Detections]): Function that takes an image\n            as input and returns Detections object.\n        conf_threshold (float): Detection confidence threshold between `0` and `1`.\n            Detections with lower confidence will be excluded.\n        iou_threshold (float): Detection IoU threshold between `0` and `1`.\n            Detections with lower IoU will be classified as `FP`.\n\n    Returns:\n        ConfusionMatrix: New instance of ConfusionMatrix.\n\n    Example:\n        ```python\n        import supervision as sv\n        from ultralytics import YOLO\n\n        dataset = sv.DetectionDataset.from_yolo(...)\n\n        model = YOLO(...)\n        def callback(image: np.ndarray) -&gt; sv.Detections:\n            result = model(image)[0]\n            return sv.Detections.from_ultralytics(result)\n\n        confusion_matrix = sv.ConfusionMatrix.benchmark(\n            dataset = dataset,\n            callback = callback\n        )\n\n        print(confusion_matrix.matrix)\n        # np.array([\n        #     [0., 0., 0., 0.],\n        #     [0., 1., 0., 1.],\n        #     [0., 1., 1., 0.],\n        #     [1., 1., 0., 0.]\n        # ])\n        ```\n    \"\"\"\n    predictions, targets = [], []\n    for _, image, annotation in dataset:\n        predictions_batch = callback(image)\n        predictions.append(predictions_batch)\n        targets.append(annotation)\n    return cls.from_detections(\n        predictions=predictions,\n        targets=targets,\n        classes=dataset.classes,\n        conf_threshold=conf_threshold,\n        iou_threshold=iou_threshold,\n    )\n</code></pre>"},{"location":"detection/metrics/#supervision.metrics.detection.ConfusionMatrix.evaluate_detection_batch","title":"<code>evaluate_detection_batch(predictions, targets, num_classes, conf_threshold, iou_threshold)</code>  <code>staticmethod</code>","text":"<p>Calculate confusion matrix for a batch of detections for a single image.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>ndarray</code> <p>Batch prediction. Describes a single image and has <code>shape = (M, 6)</code> where <code>M</code> is the number of detected objects. Each row is expected to be in <code>(x_min, y_min, x_max, y_max, class, conf)</code> format.</p> required <code>targets</code> <code>ndarray</code> <p>Batch target labels. Describes a single image and has <code>shape = (N, 5)</code> where <code>N</code> is the number of ground-truth objects. Each row is expected to be in <code>(x_min, y_min, x_max, y_max, class)</code> format.</p> required <code>num_classes</code> <code>int</code> <p>Number of classes.</p> required <code>conf_threshold</code> <code>float</code> <p>Detection confidence threshold between <code>0</code> and <code>1</code>. Detections with lower confidence will be excluded.</p> required <code>iou_threshold</code> <code>float</code> <p>Detection iou  threshold between <code>0</code> and <code>1</code>. Detections with lower iou will be classified as <code>FP</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Confusion matrix based on a single image.</p> Source code in <code>supervision/metrics/detection.py</code> <pre><code>@staticmethod\ndef evaluate_detection_batch(\n    predictions: np.ndarray,\n    targets: np.ndarray,\n    num_classes: int,\n    conf_threshold: float,\n    iou_threshold: float,\n) -&gt; np.ndarray:\n    \"\"\"\n    Calculate confusion matrix for a batch of detections for a single image.\n\n    Args:\n        predictions (np.ndarray): Batch prediction. Describes a single image and\n            has `shape = (M, 6)` where `M` is the number of detected objects.\n            Each row is expected to be in\n            `(x_min, y_min, x_max, y_max, class, conf)` format.\n        targets (np.ndarray): Batch target labels. Describes a single image and\n            has `shape = (N, 5)` where `N` is the number of ground-truth objects.\n            Each row is expected to be in\n            `(x_min, y_min, x_max, y_max, class)` format.\n        num_classes (int): Number of classes.\n        conf_threshold (float): Detection confidence threshold between `0` and `1`.\n            Detections with lower confidence will be excluded.\n        iou_threshold (float): Detection iou  threshold between `0` and `1`.\n            Detections with lower iou will be classified as `FP`.\n\n    Returns:\n        np.ndarray: Confusion matrix based on a single image.\n    \"\"\"\n    result_matrix = np.zeros((num_classes + 1, num_classes + 1))\n\n    conf_idx = 5\n    confidence = predictions[:, conf_idx]\n    detection_batch_filtered = predictions[confidence &gt; conf_threshold]\n\n    class_id_idx = 4\n    true_classes = np.array(targets[:, class_id_idx], dtype=np.int16)\n    detection_classes = np.array(\n        detection_batch_filtered[:, class_id_idx], dtype=np.int16\n    )\n    true_boxes = targets[:, :class_id_idx]\n    detection_boxes = detection_batch_filtered[:, :class_id_idx]\n\n    iou_batch = box_iou_batch(\n        boxes_true=true_boxes, boxes_detection=detection_boxes\n    )\n    matched_idx = np.asarray(iou_batch &gt; iou_threshold).nonzero()\n\n    if matched_idx[0].shape[0]:\n        matches = np.stack(\n            (matched_idx[0], matched_idx[1], iou_batch[matched_idx]), axis=1\n        )\n        matches = ConfusionMatrix._drop_extra_matches(matches=matches)\n    else:\n        matches = np.zeros((0, 3))\n\n    matched_true_idx, matched_detection_idx, _ = matches.transpose().astype(\n        np.int16\n    )\n\n    for i, true_class_value in enumerate(true_classes):\n        j = matched_true_idx == i\n        if matches.shape[0] &gt; 0 and sum(j) == 1:\n            result_matrix[\n                true_class_value, detection_classes[matched_detection_idx[j]]\n            ] += 1  # TP\n        else:\n            result_matrix[true_class_value, num_classes] += 1  # FN\n\n    for i, detection_class_value in enumerate(detection_classes):\n        if not any(matched_detection_idx == i):\n            result_matrix[num_classes, detection_class_value] += 1  # FP\n\n    return result_matrix\n</code></pre>"},{"location":"detection/metrics/#supervision.metrics.detection.ConfusionMatrix.from_detections","title":"<code>from_detections(predictions, targets, classes, conf_threshold=0.3, iou_threshold=0.5)</code>  <code>classmethod</code>","text":"<p>Calculate confusion matrix based on predicted and ground-truth detections.</p> <p>Parameters:</p> Name Type Description Default <code>targets</code> <code>List[Detections]</code> <p>Detections objects from ground-truth.</p> required <code>predictions</code> <code>List[Detections]</code> <p>Detections objects predicted by the model.</p> required <code>classes</code> <code>List[str]</code> <p>Model class names.</p> required <code>conf_threshold</code> <code>float</code> <p>Detection confidence threshold between <code>0</code> and <code>1</code>. Detections with lower confidence will be excluded.</p> <code>0.3</code> <code>iou_threshold</code> <code>float</code> <p>Detection IoU threshold between <code>0</code> and <code>1</code>. Detections with lower IoU will be classified as <code>FP</code>.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>ConfusionMatrix</code> <code>ConfusionMatrix</code> <p>New instance of ConfusionMatrix.</p> Example <pre><code>import supervision as sv\n\ntargets = [\n    sv.Detections(...),\n    sv.Detections(...)\n]\n\npredictions = [\n    sv.Detections(...),\n    sv.Detections(...)\n]\n\nconfusion_matrix = sv.ConfusionMatrix.from_detections(\n    predictions=predictions,\n    targets=target,\n    classes=['person', ...]\n)\n\nprint(confusion_matrix.matrix)\n# np.array([\n#    [0., 0., 0., 0.],\n#    [0., 1., 0., 1.],\n#    [0., 1., 1., 0.],\n#    [1., 1., 0., 0.]\n# ])\n</code></pre> Source code in <code>supervision/metrics/detection.py</code> <pre><code>@classmethod\ndef from_detections(\n    cls,\n    predictions: List[Detections],\n    targets: List[Detections],\n    classes: List[str],\n    conf_threshold: float = 0.3,\n    iou_threshold: float = 0.5,\n) -&gt; ConfusionMatrix:\n    \"\"\"\n    Calculate confusion matrix based on predicted and ground-truth detections.\n\n    Args:\n        targets (List[Detections]): Detections objects from ground-truth.\n        predictions (List[Detections]): Detections objects predicted by the model.\n        classes (List[str]): Model class names.\n        conf_threshold (float): Detection confidence threshold between `0` and `1`.\n            Detections with lower confidence will be excluded.\n        iou_threshold (float): Detection IoU threshold between `0` and `1`.\n            Detections with lower IoU will be classified as `FP`.\n\n    Returns:\n        ConfusionMatrix: New instance of ConfusionMatrix.\n\n    Example:\n        ```python\n        import supervision as sv\n\n        targets = [\n            sv.Detections(...),\n            sv.Detections(...)\n        ]\n\n        predictions = [\n            sv.Detections(...),\n            sv.Detections(...)\n        ]\n\n        confusion_matrix = sv.ConfusionMatrix.from_detections(\n            predictions=predictions,\n            targets=target,\n            classes=['person', ...]\n        )\n\n        print(confusion_matrix.matrix)\n        # np.array([\n        #    [0., 0., 0., 0.],\n        #    [0., 1., 0., 1.],\n        #    [0., 1., 1., 0.],\n        #    [1., 1., 0., 0.]\n        # ])\n        ```\n    \"\"\"\n\n    prediction_tensors = []\n    target_tensors = []\n    for prediction, target in zip(predictions, targets):\n        prediction_tensors.append(\n            detections_to_tensor(prediction, with_confidence=True)\n        )\n        target_tensors.append(detections_to_tensor(target, with_confidence=False))\n    return cls.from_tensors(\n        predictions=prediction_tensors,\n        targets=target_tensors,\n        classes=classes,\n        conf_threshold=conf_threshold,\n        iou_threshold=iou_threshold,\n    )\n</code></pre>"},{"location":"detection/metrics/#supervision.metrics.detection.ConfusionMatrix.from_tensors","title":"<code>from_tensors(predictions, targets, classes, conf_threshold=0.3, iou_threshold=0.5)</code>  <code>classmethod</code>","text":"<p>Calculate confusion matrix based on predicted and ground-truth detections.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[ndarray]</code> <p>Each element of the list describes a single image and has <code>shape = (M, 6)</code> where <code>M</code> is the number of detected objects. Each row is expected to be in <code>(x_min, y_min, x_max, y_max, class, conf)</code> format.</p> required <code>targets</code> <code>List[ndarray]</code> <p>Each element of the list describes a single image and has <code>shape = (N, 5)</code> where <code>N</code> is the number of ground-truth objects. Each row is expected to be in <code>(x_min, y_min, x_max, y_max, class)</code> format.</p> required <code>classes</code> <code>List[str]</code> <p>Model class names.</p> required <code>conf_threshold</code> <code>float</code> <p>Detection confidence threshold between <code>0</code> and <code>1</code>. Detections with lower confidence will be excluded.</p> <code>0.3</code> <code>iou_threshold</code> <code>float</code> <p>Detection iou  threshold between <code>0</code> and <code>1</code>. Detections with lower iou will be classified as <code>FP</code>.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>ConfusionMatrix</code> <code>ConfusionMatrix</code> <p>New instance of ConfusionMatrix.</p> Example <pre><code>import supervision as sv\nimport numpy as np\n\ntargets = (\n    [\n        np.array(\n            [\n                [0.0, 0.0, 3.0, 3.0, 1],\n                [2.0, 2.0, 5.0, 5.0, 1],\n                [6.0, 1.0, 8.0, 3.0, 2],\n            ]\n        ),\n        np.array([1.0, 1.0, 2.0, 2.0, 2]),\n    ]\n)\n\npredictions = [\n    np.array(\n        [\n            [0.0, 0.0, 3.0, 3.0, 1, 0.9],\n            [0.1, 0.1, 3.0, 3.0, 0, 0.9],\n            [6.0, 1.0, 8.0, 3.0, 1, 0.8],\n            [1.0, 6.0, 2.0, 7.0, 1, 0.8],\n        ]\n    ),\n    np.array([[1.0, 1.0, 2.0, 2.0, 2, 0.8]])\n]\n\nconfusion_matrix = sv.ConfusionMatrix.from_tensors(\n    predictions=predictions,\n    targets=targets,\n    classes=['person', ...]\n)\n\nprint(confusion_matrix.matrix)\n# np.array([\n#     [0., 0., 0., 0.],\n#     [0., 1., 0., 1.],\n#     [0., 1., 1., 0.],\n#     [1., 1., 0., 0.]\n# ])\n</code></pre> Source code in <code>supervision/metrics/detection.py</code> <pre><code>@classmethod\ndef from_tensors(\n    cls,\n    predictions: List[np.ndarray],\n    targets: List[np.ndarray],\n    classes: List[str],\n    conf_threshold: float = 0.3,\n    iou_threshold: float = 0.5,\n) -&gt; ConfusionMatrix:\n    \"\"\"\n    Calculate confusion matrix based on predicted and ground-truth detections.\n\n    Args:\n        predictions (List[np.ndarray]): Each element of the list describes a single\n            image and has `shape = (M, 6)` where `M` is the number of detected\n            objects. Each row is expected to be in\n            `(x_min, y_min, x_max, y_max, class, conf)` format.\n        targets (List[np.ndarray]): Each element of the list describes a single\n            image and has `shape = (N, 5)` where `N` is the number of\n            ground-truth objects. Each row is expected to be in\n            `(x_min, y_min, x_max, y_max, class)` format.\n        classes (List[str]): Model class names.\n        conf_threshold (float): Detection confidence threshold between `0` and `1`.\n            Detections with lower confidence will be excluded.\n        iou_threshold (float): Detection iou  threshold between `0` and `1`.\n            Detections with lower iou will be classified as `FP`.\n\n    Returns:\n        ConfusionMatrix: New instance of ConfusionMatrix.\n\n    Example:\n        ```python\n        import supervision as sv\n        import numpy as np\n\n        targets = (\n            [\n                np.array(\n                    [\n                        [0.0, 0.0, 3.0, 3.0, 1],\n                        [2.0, 2.0, 5.0, 5.0, 1],\n                        [6.0, 1.0, 8.0, 3.0, 2],\n                    ]\n                ),\n                np.array([1.0, 1.0, 2.0, 2.0, 2]),\n            ]\n        )\n\n        predictions = [\n            np.array(\n                [\n                    [0.0, 0.0, 3.0, 3.0, 1, 0.9],\n                    [0.1, 0.1, 3.0, 3.0, 0, 0.9],\n                    [6.0, 1.0, 8.0, 3.0, 1, 0.8],\n                    [1.0, 6.0, 2.0, 7.0, 1, 0.8],\n                ]\n            ),\n            np.array([[1.0, 1.0, 2.0, 2.0, 2, 0.8]])\n        ]\n\n        confusion_matrix = sv.ConfusionMatrix.from_tensors(\n            predictions=predictions,\n            targets=targets,\n            classes=['person', ...]\n        )\n\n        print(confusion_matrix.matrix)\n        # np.array([\n        #     [0., 0., 0., 0.],\n        #     [0., 1., 0., 1.],\n        #     [0., 1., 1., 0.],\n        #     [1., 1., 0., 0.]\n        # ])\n        ```\n    \"\"\"\n    validate_input_tensors(predictions, targets)\n\n    num_classes = len(classes)\n    matrix = np.zeros((num_classes + 1, num_classes + 1))\n    for true_batch, detection_batch in zip(targets, predictions):\n        matrix += cls.evaluate_detection_batch(\n            predictions=detection_batch,\n            targets=true_batch,\n            num_classes=num_classes,\n            conf_threshold=conf_threshold,\n            iou_threshold=iou_threshold,\n        )\n    return cls(\n        matrix=matrix,\n        classes=classes,\n        conf_threshold=conf_threshold,\n        iou_threshold=iou_threshold,\n    )\n</code></pre>"},{"location":"detection/metrics/#supervision.metrics.detection.ConfusionMatrix.plot","title":"<code>plot(save_path=None, title=None, classes=None, normalize=False, fig_size=(12, 10))</code>","text":"<p>Create confusion matrix plot and save it at selected location.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>Optional[str]</code> <p>Path to save the plot. If not provided, plot will be displayed.</p> <code>None</code> <code>title</code> <code>Optional[str]</code> <p>Title of the plot.</p> <code>None</code> <code>classes</code> <code>Optional[List[str]]</code> <p>List of classes to be displayed on the plot. If not provided, all classes will be displayed.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>If True, normalize the confusion matrix.</p> <code>False</code> <code>fig_size</code> <code>Tuple[int, int]</code> <p>Size of the plot.</p> <code>(12, 10)</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: Confusion matrix plot.</p> Source code in <code>supervision/metrics/detection.py</code> <pre><code>def plot(\n    self,\n    save_path: Optional[str] = None,\n    title: Optional[str] = None,\n    classes: Optional[List[str]] = None,\n    normalize: bool = False,\n    fig_size: Tuple[int, int] = (12, 10),\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"\n    Create confusion matrix plot and save it at selected location.\n\n    Args:\n        save_path (Optional[str]): Path to save the plot. If not provided,\n            plot will be displayed.\n        title (Optional[str]): Title of the plot.\n        classes (Optional[List[str]]): List of classes to be displayed on the plot.\n            If not provided, all classes will be displayed.\n        normalize (bool): If True, normalize the confusion matrix.\n        fig_size (Tuple[int, int]): Size of the plot.\n\n    Returns:\n        matplotlib.figure.Figure: Confusion matrix plot.\n    \"\"\"\n\n    array = self.matrix.copy()\n\n    if normalize:\n        eps = 1e-8\n        array = array / (array.sum(0).reshape(1, -1) + eps)\n\n    array[array &lt; 0.005] = np.nan\n\n    fig, ax = plt.subplots(figsize=fig_size, tight_layout=True, facecolor=\"white\")\n\n    class_names = classes if classes is not None else self.classes\n    use_labels_for_ticks = class_names is not None and (0 &lt; len(class_names) &lt; 99)\n    if use_labels_for_ticks:\n        x_tick_labels = class_names + [\"FN\"]\n        y_tick_labels = class_names + [\"FP\"]\n        num_ticks = len(x_tick_labels)\n    else:\n        x_tick_labels = None\n        y_tick_labels = None\n        num_ticks = len(array)\n    im = ax.imshow(array, cmap=\"Blues\")\n\n    cbar = ax.figure.colorbar(im, ax=ax)\n    cbar.mappable.set_clim(vmin=0, vmax=np.nanmax(array))\n\n    if x_tick_labels is None:\n        tick_interval = 2\n    else:\n        tick_interval = 1\n    ax.set_xticks(np.arange(0, num_ticks, tick_interval), labels=x_tick_labels)\n    ax.set_yticks(np.arange(0, num_ticks, tick_interval), labels=y_tick_labels)\n\n    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\", rotation_mode=\"default\")\n\n    labelsize = 10 if num_ticks &lt; 50 else 8\n    ax.tick_params(axis=\"both\", which=\"both\", labelsize=labelsize)\n\n    if num_ticks &lt; 30:\n        for i in range(array.shape[0]):\n            for j in range(array.shape[1]):\n                n_preds = array[i, j]\n                if not np.isnan(n_preds):\n                    ax.text(\n                        j,\n                        i,\n                        f\"{n_preds:.2f}\" if normalize else f\"{n_preds:.0f}\",\n                        ha=\"center\",\n                        va=\"center\",\n                        color=\"black\"\n                        if n_preds &lt; 0.5 * np.nanmax(array)\n                        else \"white\",\n                    )\n\n    if title:\n        ax.set_title(title, fontsize=20)\n\n    ax.set_xlabel(\"Predicted\")\n    ax.set_ylabel(\"True\")\n    ax.set_facecolor(\"white\")\n    if save_path:\n        fig.savefig(\n            save_path, dpi=250, facecolor=fig.get_facecolor(), transparent=True\n        )\n    return fig\n</code></pre>"},{"location":"detection/metrics/#supervision.metrics.detection.MeanAveragePrecision-functions","title":"Functions","text":""},{"location":"detection/metrics/#supervision.metrics.detection.MeanAveragePrecision.benchmark","title":"<code>benchmark(dataset, callback)</code>  <code>classmethod</code>","text":"<p>Calculate mean average precision from dataset and callback function.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DetectionDataset</code> <p>Object detection dataset used for evaluation.</p> required <code>callback</code> <code>Callable[[ndarray], Detections]</code> <p>Function that takes an image as input and returns Detections object.</p> required <p>Returns:     MeanAveragePrecision: New instance of MeanAveragePrecision.</p> Example <pre><code>import supervision as sv\nfrom ultralytics import YOLO\n\ndataset = sv.DetectionDataset.from_yolo(...)\n\nmodel = YOLO(...)\ndef callback(image: np.ndarray) -&gt; sv.Detections:\n    result = model(image)[0]\n    return sv.Detections.from_ultralytics(result)\n\nmean_average_precision = sv.MeanAveragePrecision.benchmark(\n    dataset = dataset,\n    callback = callback\n)\n\nprint(mean_average_precision.map50_95)\n# 0.433\n</code></pre> Source code in <code>supervision/metrics/detection.py</code> <pre><code>@classmethod\ndef benchmark(\n    cls,\n    dataset: DetectionDataset,\n    callback: Callable[[np.ndarray], Detections],\n) -&gt; MeanAveragePrecision:\n    \"\"\"\n    Calculate mean average precision from dataset and callback function.\n\n    Args:\n        dataset (DetectionDataset): Object detection dataset used for evaluation.\n        callback (Callable[[np.ndarray], Detections]): Function that takes\n            an image as input and returns Detections object.\n    Returns:\n        MeanAveragePrecision: New instance of MeanAveragePrecision.\n\n    Example:\n        ```python\n        import supervision as sv\n        from ultralytics import YOLO\n\n        dataset = sv.DetectionDataset.from_yolo(...)\n\n        model = YOLO(...)\n        def callback(image: np.ndarray) -&gt; sv.Detections:\n            result = model(image)[0]\n            return sv.Detections.from_ultralytics(result)\n\n        mean_average_precision = sv.MeanAveragePrecision.benchmark(\n            dataset = dataset,\n            callback = callback\n        )\n\n        print(mean_average_precision.map50_95)\n        # 0.433\n        ```\n    \"\"\"\n    predictions, targets = [], []\n    for _, image, annotation in dataset:\n        predictions_batch = callback(image)\n        predictions.append(predictions_batch)\n        targets.append(annotation)\n    return cls.from_detections(\n        predictions=predictions,\n        targets=targets,\n    )\n</code></pre>"},{"location":"detection/metrics/#supervision.metrics.detection.MeanAveragePrecision.compute_average_precision","title":"<code>compute_average_precision(recall, precision)</code>  <code>staticmethod</code>","text":"<p>Compute the average precision using 101-point interpolation (COCO), given     the recall and precision curves.</p> <p>Parameters:</p> Name Type Description Default <code>recall</code> <code>ndarray</code> <p>The recall curve.</p> required <code>precision</code> <code>ndarray</code> <p>The precision curve.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Average precision.</p> Source code in <code>supervision/metrics/detection.py</code> <pre><code>@staticmethod\ndef compute_average_precision(recall: np.ndarray, precision: np.ndarray) -&gt; float:\n    \"\"\"\n    Compute the average precision using 101-point interpolation (COCO), given\n        the recall and precision curves.\n\n    Args:\n        recall (np.ndarray): The recall curve.\n        precision (np.ndarray): The precision curve.\n\n    Returns:\n        float: Average precision.\n    \"\"\"\n    extended_recall = np.concatenate(([0.0], recall, [1.0]))\n    extended_precision = np.concatenate(([1.0], precision, [0.0]))\n    max_accumulated_precision = np.flip(\n        np.maximum.accumulate(np.flip(extended_precision))\n    )\n    interpolated_recall_levels = np.linspace(0, 1, 101)\n    interpolated_precision = np.interp(\n        interpolated_recall_levels, extended_recall, max_accumulated_precision\n    )\n    average_precision = np.trapz(interpolated_precision, interpolated_recall_levels)\n    return average_precision\n</code></pre>"},{"location":"detection/metrics/#supervision.metrics.detection.MeanAveragePrecision.from_detections","title":"<code>from_detections(predictions, targets)</code>  <code>classmethod</code>","text":"<p>Calculate mean average precision based on predicted and ground-truth detections.</p> <p>Parameters:</p> Name Type Description Default <code>targets</code> <code>List[Detections]</code> <p>Detections objects from ground-truth.</p> required <code>predictions</code> <code>List[Detections]</code> <p>Detections objects predicted by the model.</p> required <p>Returns:     MeanAveragePrecision: New instance of ConfusionMatrix.</p> Example <pre><code>import supervision as sv\n\ntargets = [\n    sv.Detections(...),\n    sv.Detections(...)\n]\n\npredictions = [\n    sv.Detections(...),\n    sv.Detections(...)\n]\n\nmean_average_precision = sv.MeanAveragePrecision.from_detections(\n    predictions=predictions,\n    targets=target,\n)\n\nprint(mean_average_precison.map50_95)\n# 0.2899\n</code></pre> Source code in <code>supervision/metrics/detection.py</code> <pre><code>@classmethod\ndef from_detections(\n    cls,\n    predictions: List[Detections],\n    targets: List[Detections],\n) -&gt; MeanAveragePrecision:\n    \"\"\"\n    Calculate mean average precision based on predicted and ground-truth detections.\n\n    Args:\n        targets (List[Detections]): Detections objects from ground-truth.\n        predictions (List[Detections]): Detections objects predicted by the model.\n    Returns:\n        MeanAveragePrecision: New instance of ConfusionMatrix.\n\n    Example:\n        ```python\n        import supervision as sv\n\n        targets = [\n            sv.Detections(...),\n            sv.Detections(...)\n        ]\n\n        predictions = [\n            sv.Detections(...),\n            sv.Detections(...)\n        ]\n\n        mean_average_precision = sv.MeanAveragePrecision.from_detections(\n            predictions=predictions,\n            targets=target,\n        )\n\n        print(mean_average_precison.map50_95)\n        # 0.2899\n        ```\n    \"\"\"\n    prediction_tensors = []\n    target_tensors = []\n    for prediction, target in zip(predictions, targets):\n        prediction_tensors.append(\n            detections_to_tensor(prediction, with_confidence=True)\n        )\n        target_tensors.append(detections_to_tensor(target, with_confidence=False))\n    return cls.from_tensors(\n        predictions=prediction_tensors,\n        targets=target_tensors,\n    )\n</code></pre>"},{"location":"detection/metrics/#supervision.metrics.detection.MeanAveragePrecision.from_tensors","title":"<code>from_tensors(predictions, targets)</code>  <code>classmethod</code>","text":"<p>Calculate Mean Average Precision based on predicted and ground-truth     detections at different threshold.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>List[ndarray]</code> <p>Each element of the list describes a single image and has <code>shape = (M, 6)</code> where <code>M</code> is the number of detected objects. Each row is expected to be in <code>(x_min, y_min, x_max, y_max, class, conf)</code> format.</p> required <code>targets</code> <code>List[ndarray]</code> <p>Each element of the list describes a single image and has <code>shape = (N, 5)</code> where <code>N</code> is the number of ground-truth objects. Each row is expected to be in <code>(x_min, y_min, x_max, y_max, class)</code> format.</p> required <p>Returns:     MeanAveragePrecision: New instance of MeanAveragePrecision.</p> Example <pre><code>import supervision as sv\nimport numpy as np\n\ntargets = (\n    [\n        np.array(\n            [\n                [0.0, 0.0, 3.0, 3.0, 1],\n                [2.0, 2.0, 5.0, 5.0, 1],\n                [6.0, 1.0, 8.0, 3.0, 2],\n            ]\n        ),\n        np.array([[1.0, 1.0, 2.0, 2.0, 2]]),\n    ]\n)\n\npredictions = [\n    np.array(\n        [\n            [0.0, 0.0, 3.0, 3.0, 1, 0.9],\n            [0.1, 0.1, 3.0, 3.0, 0, 0.9],\n            [6.0, 1.0, 8.0, 3.0, 1, 0.8],\n            [1.0, 6.0, 2.0, 7.0, 1, 0.8],\n        ]\n    ),\n    np.array([[1.0, 1.0, 2.0, 2.0, 2, 0.8]])\n]\n\nmean_average_precison = sv.MeanAveragePrecision.from_tensors(\n    predictions=predictions,\n    targets=targets,\n)\n\nprint(mean_average_precison.map50_95)\n# 0.6649\n</code></pre> Source code in <code>supervision/metrics/detection.py</code> <pre><code>@classmethod\ndef from_tensors(\n    cls,\n    predictions: List[np.ndarray],\n    targets: List[np.ndarray],\n) -&gt; MeanAveragePrecision:\n    \"\"\"\n    Calculate Mean Average Precision based on predicted and ground-truth\n        detections at different threshold.\n\n    Args:\n        predictions (List[np.ndarray]): Each element of the list describes\n            a single image and has `shape = (M, 6)` where `M` is\n            the number of detected objects. Each row is expected to be\n            in `(x_min, y_min, x_max, y_max, class, conf)` format.\n        targets (List[np.ndarray]): Each element of the list describes a single\n            image and has `shape = (N, 5)` where `N` is the\n            number of ground-truth objects. Each row is expected to be in\n            `(x_min, y_min, x_max, y_max, class)` format.\n    Returns:\n        MeanAveragePrecision: New instance of MeanAveragePrecision.\n\n    Example:\n        ```python\n        import supervision as sv\n        import numpy as np\n\n        targets = (\n            [\n                np.array(\n                    [\n                        [0.0, 0.0, 3.0, 3.0, 1],\n                        [2.0, 2.0, 5.0, 5.0, 1],\n                        [6.0, 1.0, 8.0, 3.0, 2],\n                    ]\n                ),\n                np.array([[1.0, 1.0, 2.0, 2.0, 2]]),\n            ]\n        )\n\n        predictions = [\n            np.array(\n                [\n                    [0.0, 0.0, 3.0, 3.0, 1, 0.9],\n                    [0.1, 0.1, 3.0, 3.0, 0, 0.9],\n                    [6.0, 1.0, 8.0, 3.0, 1, 0.8],\n                    [1.0, 6.0, 2.0, 7.0, 1, 0.8],\n                ]\n            ),\n            np.array([[1.0, 1.0, 2.0, 2.0, 2, 0.8]])\n        ]\n\n        mean_average_precison = sv.MeanAveragePrecision.from_tensors(\n            predictions=predictions,\n            targets=targets,\n        )\n\n        print(mean_average_precison.map50_95)\n        # 0.6649\n        ```\n    \"\"\"\n    validate_input_tensors(predictions, targets)\n    iou_thresholds = np.linspace(0.5, 0.95, 10)\n    stats = []\n\n    # Gather matching stats for predictions and targets\n    for true_objs, predicted_objs in zip(targets, predictions):\n        if predicted_objs.shape[0] == 0:\n            if true_objs.shape[0]:\n                stats.append(\n                    (\n                        np.zeros((0, iou_thresholds.size), dtype=bool),\n                        *np.zeros((2, 0)),\n                        true_objs[:, 4],\n                    )\n                )\n            continue\n\n        if true_objs.shape[0]:\n            matches = cls._match_detection_batch(\n                predicted_objs, true_objs, iou_thresholds\n            )\n            stats.append(\n                (\n                    matches,\n                    predicted_objs[:, 5],\n                    predicted_objs[:, 4],\n                    true_objs[:, 4],\n                )\n            )\n\n    # Compute average precisions if any matches exist\n    if stats:\n        concatenated_stats = [np.concatenate(items, 0) for items in zip(*stats)]\n        average_precisions = cls._average_precisions_per_class(*concatenated_stats)\n        map50 = average_precisions[:, 0].mean()\n        map75 = average_precisions[:, 5].mean()\n        map50_95 = average_precisions.mean()\n    else:\n        map50, map75, map50_95 = 0, 0, 0\n        average_precisions = []\n\n    return cls(\n        map50_95=map50_95,\n        map50=map50,\n        map75=map75,\n        per_class_ap50_95=average_precisions,\n    )\n</code></pre>"},{"location":"detection/utils/","title":"Detection Utils","text":"box_iou_batch <p>Compute Intersection over Union (IoU) of two sets of bounding boxes -     <code>boxes_true</code> and <code>boxes_detection</code>. Both sets     of boxes are expected to be in <code>(x_min, y_min, x_max, y_max)</code> format.</p> <p>Parameters:</p> Name Type Description Default <code>boxes_true</code> <code>ndarray</code> <p>2D <code>np.ndarray</code> representing ground-truth boxes. <code>shape = (N, 4)</code> where <code>N</code> is number of true objects.</p> required <code>boxes_detection</code> <code>ndarray</code> <p>2D <code>np.ndarray</code> representing detection boxes. <code>shape = (M, 4)</code> where <code>M</code> is number of detected objects.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Pairwise IoU of boxes from <code>boxes_true</code> and <code>boxes_detection</code>. <code>shape = (N, M)</code> where <code>N</code> is number of true objects and <code>M</code> is number of detected objects.</p> Source code in <code>supervision/detection/utils.py</code> <pre><code>def box_iou_batch(boxes_true: np.ndarray, boxes_detection: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Compute Intersection over Union (IoU) of two sets of bounding boxes -\n        `boxes_true` and `boxes_detection`. Both sets\n        of boxes are expected to be in `(x_min, y_min, x_max, y_max)` format.\n\n    Args:\n        boxes_true (np.ndarray): 2D `np.ndarray` representing ground-truth boxes.\n            `shape = (N, 4)` where `N` is number of true objects.\n        boxes_detection (np.ndarray): 2D `np.ndarray` representing detection boxes.\n            `shape = (M, 4)` where `M` is number of detected objects.\n\n    Returns:\n        np.ndarray: Pairwise IoU of boxes from `boxes_true` and `boxes_detection`.\n            `shape = (N, M)` where `N` is number of true objects and\n            `M` is number of detected objects.\n    \"\"\"\n\n    def box_area(box):\n        return (box[2] - box[0]) * (box[3] - box[1])\n\n    area_true = box_area(boxes_true.T)\n    area_detection = box_area(boxes_detection.T)\n\n    top_left = np.maximum(boxes_true[:, None, :2], boxes_detection[:, :2])\n    bottom_right = np.minimum(boxes_true[:, None, 2:], boxes_detection[:, 2:])\n\n    area_inter = np.prod(np.clip(bottom_right - top_left, a_min=0, a_max=None), 2)\n    ious = area_inter / (area_true[:, None] + area_detection - area_inter)\n    ious = np.nan_to_num(ious)\n    return ious\n</code></pre> mask_iou_batch <p>Compute Intersection over Union (IoU) of two sets of masks -     <code>masks_true</code> and <code>masks_detection</code>.</p> <p>Parameters:</p> Name Type Description Default <code>masks_true</code> <code>ndarray</code> <p>3D <code>np.ndarray</code> representing ground-truth masks.</p> required <code>masks_detection</code> <code>ndarray</code> <p>3D <code>np.ndarray</code> representing detection masks.</p> required <code>memory_limit</code> <code>int</code> <p>memory limit in MB, default is 1024 * 5 MB (5GB).</p> <code>1024 * 5</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Pairwise IoU of masks from <code>masks_true</code> and <code>masks_detection</code>.</p> Source code in <code>supervision/detection/utils.py</code> <pre><code>def mask_iou_batch(\n    masks_true: np.ndarray,\n    masks_detection: np.ndarray,\n    memory_limit: int = 1024 * 5,\n) -&gt; np.ndarray:\n    \"\"\"\n    Compute Intersection over Union (IoU) of two sets of masks -\n        `masks_true` and `masks_detection`.\n\n    Args:\n        masks_true (np.ndarray): 3D `np.ndarray` representing ground-truth masks.\n        masks_detection (np.ndarray): 3D `np.ndarray` representing detection masks.\n        memory_limit (int, optional): memory limit in MB, default is 1024 * 5 MB (5GB).\n\n    Returns:\n        np.ndarray: Pairwise IoU of masks from `masks_true` and `masks_detection`.\n    \"\"\"\n    memory = (\n        masks_true.shape[0]\n        * masks_true.shape[1]\n        * masks_true.shape[2]\n        * masks_detection.shape[0]\n        / 1024\n        / 1024\n    )\n    if memory &lt;= memory_limit:\n        return _mask_iou_batch_split(masks_true, masks_detection)\n\n    ious = []\n    step = max(\n        memory_limit\n        * 1024\n        * 1024\n        // (\n            masks_detection.shape[0]\n            * masks_detection.shape[1]\n            * masks_detection.shape[2]\n        ),\n        1,\n    )\n    for i in range(0, masks_true.shape[0], step):\n        ious.append(_mask_iou_batch_split(masks_true[i : i + step], masks_detection))\n\n    return np.vstack(ious)\n</code></pre> polygon_to_mask <p>Generate a mask from a polygon.</p> <p>Parameters:</p> Name Type Description Default <code>polygon</code> <code>ndarray</code> <p>The polygon for which the mask should be generated, given as a list of vertices.</p> required <code>resolution_wh</code> <code>Tuple[int, int]</code> <p>The width and height of the desired resolution.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The generated 2D mask, where the polygon is marked with <code>1</code>'s and the rest is filled with <code>0</code>'s.</p> Source code in <code>supervision/detection/utils.py</code> <pre><code>def polygon_to_mask(polygon: np.ndarray, resolution_wh: Tuple[int, int]) -&gt; np.ndarray:\n    \"\"\"Generate a mask from a polygon.\n\n    Args:\n        polygon (np.ndarray): The polygon for which the mask should be generated,\n            given as a list of vertices.\n        resolution_wh (Tuple[int, int]): The width and height of the desired resolution.\n\n    Returns:\n        np.ndarray: The generated 2D mask, where the polygon is marked with\n            `1`'s and the rest is filled with `0`'s.\n    \"\"\"\n    width, height = resolution_wh\n    mask = np.zeros((height, width))\n\n    cv2.fillPoly(mask, [polygon], color=1)\n    return mask\n</code></pre> mask_to_xyxy <p>Converts a 3D <code>np.array</code> of 2D bool masks into a 2D <code>np.array</code> of bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>ndarray</code> <p>A 3D <code>np.array</code> of shape <code>(N, W, H)</code> containing 2D bool masks</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 2D <code>np.array</code> of shape <code>(N, 4)</code> containing the bounding boxes <code>(x_min, y_min, x_max, y_max)</code> for each mask</p> Source code in <code>supervision/detection/utils.py</code> <pre><code>def mask_to_xyxy(masks: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Converts a 3D `np.array` of 2D bool masks into a 2D `np.array` of bounding boxes.\n\n    Parameters:\n        masks (np.ndarray): A 3D `np.array` of shape `(N, W, H)`\n            containing 2D bool masks\n\n    Returns:\n        np.ndarray: A 2D `np.array` of shape `(N, 4)` containing the bounding boxes\n            `(x_min, y_min, x_max, y_max)` for each mask\n    \"\"\"\n    n = masks.shape[0]\n    xyxy = np.zeros((n, 4), dtype=int)\n\n    for i, mask in enumerate(masks):\n        rows, cols = np.where(mask)\n\n        if len(rows) &gt; 0 and len(cols) &gt; 0:\n            x_min, x_max = np.min(cols), np.max(cols)\n            y_min, y_max = np.min(rows), np.max(rows)\n            xyxy[i, :] = [x_min, y_min, x_max, y_max]\n\n    return xyxy\n</code></pre> mask_to_polygons <p>Converts a binary mask to a list of polygons.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ndarray</code> <p>A binary mask represented as a 2D NumPy array of shape <code>(H, W)</code>, where H and W are the height and width of the mask, respectively.</p> required <p>Returns:</p> Type Description <code>List[ndarray]</code> <p>List[np.ndarray]: A list of polygons, where each polygon is represented by a NumPy array of shape <code>(N, 2)</code>, containing the <code>x</code>, <code>y</code> coordinates of the points. Polygons with fewer points than <code>MIN_POLYGON_POINT_COUNT = 3</code> are excluded from the output.</p> Source code in <code>supervision/detection/utils.py</code> <pre><code>def mask_to_polygons(mask: np.ndarray) -&gt; List[np.ndarray]:\n    \"\"\"\n    Converts a binary mask to a list of polygons.\n\n    Parameters:\n        mask (np.ndarray): A binary mask represented as a 2D NumPy array of\n            shape `(H, W)`, where H and W are the height and width of\n            the mask, respectively.\n\n    Returns:\n        List[np.ndarray]: A list of polygons, where each polygon is represented by a\n            NumPy array of shape `(N, 2)`, containing the `x`, `y` coordinates\n            of the points. Polygons with fewer points than `MIN_POLYGON_POINT_COUNT = 3`\n            are excluded from the output.\n    \"\"\"\n\n    contours, _ = cv2.findContours(\n        mask.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n    )\n    return [\n        np.squeeze(contour, axis=1)\n        for contour in contours\n        if contour.shape[0] &gt;= MIN_POLYGON_POINT_COUNT\n    ]\n</code></pre> polygon_to_xyxy <p>Converts a polygon represented by a NumPy array into a bounding box.</p> <p>Parameters:</p> Name Type Description Default <code>polygon</code> <code>ndarray</code> <p>A polygon represented by a NumPy array of shape <code>(N, 2)</code>, containing the <code>x</code>, <code>y</code> coordinates of the points.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A 1D NumPy array containing the bounding box <code>(x_min, y_min, x_max, y_max)</code> of the input polygon.</p> Source code in <code>supervision/detection/utils.py</code> <pre><code>def polygon_to_xyxy(polygon: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Converts a polygon represented by a NumPy array into a bounding box.\n\n    Parameters:\n        polygon (np.ndarray): A polygon represented by a NumPy array of shape `(N, 2)`,\n            containing the `x`, `y` coordinates of the points.\n\n    Returns:\n        np.ndarray: A 1D NumPy array containing the bounding box\n            `(x_min, y_min, x_max, y_max)` of the input polygon.\n    \"\"\"\n    x_min, y_min = np.min(polygon, axis=0)\n    x_max, y_max = np.max(polygon, axis=0)\n    return np.array([x_min, y_min, x_max, y_max])\n</code></pre> filter_polygons_by_area <p>Filters a list of polygons based on their area.</p> <p>Parameters:</p> Name Type Description Default <code>polygons</code> <code>List[ndarray]</code> <p>A list of polygons, where each polygon is represented by a NumPy array of shape <code>(N, 2)</code>, containing the <code>x</code>, <code>y</code> coordinates of the points.</p> required <code>min_area</code> <code>Optional[float]</code> <p>The minimum area threshold. Only polygons with an area greater than or equal to this value will be included in the output. If set to None, no minimum area constraint will be applied.</p> <code>None</code> <code>max_area</code> <code>Optional[float]</code> <p>The maximum area threshold. Only polygons with an area less than or equal to this value will be included in the output. If set to None, no maximum area constraint will be applied.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[ndarray]</code> <p>List[np.ndarray]: A new list of polygons containing only those with areas within the specified thresholds.</p> Source code in <code>supervision/detection/utils.py</code> <pre><code>def filter_polygons_by_area(\n    polygons: List[np.ndarray],\n    min_area: Optional[float] = None,\n    max_area: Optional[float] = None,\n) -&gt; List[np.ndarray]:\n    \"\"\"\n    Filters a list of polygons based on their area.\n\n    Parameters:\n        polygons (List[np.ndarray]): A list of polygons, where each polygon is\n            represented by a NumPy array of shape `(N, 2)`,\n            containing the `x`, `y` coordinates of the points.\n        min_area (Optional[float]): The minimum area threshold.\n            Only polygons with an area greater than or equal to this value\n            will be included in the output. If set to None,\n            no minimum area constraint will be applied.\n        max_area (Optional[float]): The maximum area threshold.\n            Only polygons with an area less than or equal to this value\n            will be included in the output. If set to None,\n            no maximum area constraint will be applied.\n\n    Returns:\n        List[np.ndarray]: A new list of polygons containing only those with\n            areas within the specified thresholds.\n    \"\"\"\n    if min_area is None and max_area is None:\n        return polygons\n    ares = [cv2.contourArea(polygon) for polygon in polygons]\n    return [\n        polygon\n        for polygon, area in zip(polygons, ares)\n        if (min_area is None or area &gt;= min_area)\n        and (max_area is None or area &lt;= max_area)\n    ]\n</code></pre> move_boxes <p>Parameters:</p> Name Type Description Default <code>xyxy</code> <code>NDArray[float64]</code> <p>An array of shape <code>(n, 4)</code> containing the bounding boxes coordinates in format <code>[x1, y1, x2, y2]</code></p> required <code>offset</code> <code>array</code> <p>An array of shape <code>(2,)</code> containing offset values in format is <code>[dx, dy]</code>.</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>npt.NDArray[np.float64]: Repositioned bounding boxes.</p> <p>Examples:</p> <pre><code>import numpy as np\nimport supervision as sv\n\nxyxy = np.array([\n    [10, 10, 20, 20],\n    [30, 30, 40, 40]\n])\noffset = np.array([5, 5])\n\nsv.move_boxes(xyxy=xyxy, offset=offset)\n# array([\n#    [15, 15, 25, 25],\n#    [35, 35, 45, 45]\n# ])\n</code></pre> Source code in <code>supervision/detection/utils.py</code> <pre><code>def move_boxes(\n    xyxy: npt.NDArray[np.float64], offset: npt.NDArray[np.int32]\n) -&gt; npt.NDArray[np.float64]:\n    \"\"\"\n    Parameters:\n        xyxy (npt.NDArray[np.float64]): An array of shape `(n, 4)` containing the\n            bounding boxes coordinates in format `[x1, y1, x2, y2]`\n        offset (np.array): An array of shape `(2,)` containing offset values in format\n            is `[dx, dy]`.\n\n    Returns:\n        npt.NDArray[np.float64]: Repositioned bounding boxes.\n\n    Examples:\n        ```python\n        import numpy as np\n        import supervision as sv\n\n        xyxy = np.array([\n            [10, 10, 20, 20],\n            [30, 30, 40, 40]\n        ])\n        offset = np.array([5, 5])\n\n        sv.move_boxes(xyxy=xyxy, offset=offset)\n        # array([\n        #    [15, 15, 25, 25],\n        #    [35, 35, 45, 45]\n        # ])\n        ```\n    \"\"\"\n    return xyxy + np.hstack([offset, offset])\n</code></pre> move_masks <p>Offset the masks in an array by the specified (x, y) amount.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>NDArray[bool_]</code> <p>A 3D array of binary masks corresponding to the predictions. Shape: <code>(N, H, W)</code>, where N is the number of predictions, and H, W are the dimensions of each mask.</p> required <code>offset</code> <code>NDArray[int32]</code> <p>An array of shape <code>(2,)</code> containing non-negative int values <code>[dx, dy]</code>.</p> required <code>resolution_wh</code> <code>Tuple[int, int]</code> <p>The width and height of the desired mask resolution.</p> required <p>Returns:</p> Type Description <code>NDArray[bool_]</code> <p>(npt.NDArray[np.bool_]) repositioned masks, optionally padded to the specified shape.</p> Source code in <code>supervision/detection/utils.py</code> <pre><code>def move_masks(\n    masks: npt.NDArray[np.bool_],\n    offset: npt.NDArray[np.int32],\n    resolution_wh: Tuple[int, int],\n) -&gt; npt.NDArray[np.bool_]:\n    \"\"\"\n    Offset the masks in an array by the specified (x, y) amount.\n\n    Args:\n        masks (npt.NDArray[np.bool_]): A 3D array of binary masks corresponding to the\n            predictions. Shape: `(N, H, W)`, where N is the number of predictions, and\n            H, W are the dimensions of each mask.\n        offset (npt.NDArray[np.int32]): An array of shape `(2,)` containing non-negative\n            int values `[dx, dy]`.\n        resolution_wh (Tuple[int, int]): The width and height of the desired mask\n            resolution.\n\n    Returns:\n        (npt.NDArray[np.bool_]) repositioned masks, optionally padded to the specified\n            shape.\n    \"\"\"\n\n    if offset[0] &lt; 0 or offset[1] &lt; 0:\n        raise ValueError(f\"Offset values must be non-negative integers. Got: {offset}\")\n\n    mask_array = np.full((masks.shape[0], resolution_wh[1], resolution_wh[0]), False)\n    mask_array[\n        :,\n        offset[1] : masks.shape[1] + offset[1],\n        offset[0] : masks.shape[2] + offset[0],\n    ] = masks\n\n    return mask_array\n</code></pre> scale_boxes <p>Scale the dimensions of bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>xyxy</code> <code>NDArray[float64]</code> <p>An array of shape <code>(n, 4)</code> containing the bounding boxes coordinates in format <code>[x1, y1, x2, y2]</code></p> required <code>factor</code> <code>float</code> <p>A float value representing the factor by which the box dimensions are scaled. A factor greater than 1 enlarges the boxes, while a factor less than 1 shrinks them.</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>npt.NDArray[np.float64]: Scaled bounding boxes.</p> <p>Examples:</p> <pre><code>import numpy as np\nimport supervision as sv\n\nxyxy = np.array([\n    [10, 10, 20, 20],\n    [30, 30, 40, 40]\n])\n\nsv.scale_boxes(xyxy=xyxy, factor=1.5)\n# array([\n#    [ 7.5,  7.5, 22.5, 22.5],\n#    [27.5, 27.5, 42.5, 42.5]\n# ])\n</code></pre> Source code in <code>supervision/detection/utils.py</code> <pre><code>def scale_boxes(\n    xyxy: npt.NDArray[np.float64], factor: float\n) -&gt; npt.NDArray[np.float64]:\n    \"\"\"\n    Scale the dimensions of bounding boxes.\n\n    Parameters:\n        xyxy (npt.NDArray[np.float64]): An array of shape `(n, 4)` containing the\n            bounding boxes coordinates in format `[x1, y1, x2, y2]`\n        factor (float): A float value representing the factor by which the box\n            dimensions are scaled. A factor greater than 1 enlarges the boxes, while a\n            factor less than 1 shrinks them.\n\n    Returns:\n        npt.NDArray[np.float64]: Scaled bounding boxes.\n\n    Examples:\n        ```python\n        import numpy as np\n        import supervision as sv\n\n        xyxy = np.array([\n            [10, 10, 20, 20],\n            [30, 30, 40, 40]\n        ])\n\n        sv.scale_boxes(xyxy=xyxy, factor=1.5)\n        # array([\n        #    [ 7.5,  7.5, 22.5, 22.5],\n        #    [27.5, 27.5, 42.5, 42.5]\n        # ])\n        ```\n    \"\"\"\n    centers = (xyxy[:, :2] + xyxy[:, 2:]) / 2\n    new_sizes = (xyxy[:, 2:] - xyxy[:, :2]) * factor\n    return np.concatenate((centers - new_sizes / 2, centers + new_sizes / 2), axis=1)\n</code></pre> clip_boxes <p>Clips bounding boxes coordinates to fit within the frame resolution.</p> <p>Parameters:</p> Name Type Description Default <code>xyxy</code> <code>ndarray</code> <p>A numpy array of shape <code>(N, 4)</code> where each row corresponds to a bounding box in</p> required <code>resolution_wh</code> <code>Tuple[int, int]</code> <p>A tuple of the form <code>(width, height)</code> representing the resolution of the frame.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A numpy array of shape <code>(N, 4)</code> where each row corresponds to a bounding box with coordinates clipped to fit within the frame resolution.</p> <p>Examples:</p> <pre><code>import numpy as np\nimport supervision as sv\n\nxyxy = np.array([\n    [10, 20, 300, 200],\n    [15, 25, 350, 450],\n    [-10, -20, 30, 40]\n])\n\nsv.clip_boxes(xyxy=xyxy, resolution_wh=(320, 240))\n# array([\n#     [ 10,  20, 300, 200],\n#     [ 15,  25, 320, 240],\n#     [  0,   0,  30,  40]\n# ])\n</code></pre> Source code in <code>supervision/detection/utils.py</code> <pre><code>def clip_boxes(xyxy: np.ndarray, resolution_wh: Tuple[int, int]) -&gt; np.ndarray:\n    \"\"\"\n    Clips bounding boxes coordinates to fit within the frame resolution.\n\n    Args:\n        xyxy (np.ndarray): A numpy array of shape `(N, 4)` where each\n            row corresponds to a bounding box in\n        the format `(x_min, y_min, x_max, y_max)`.\n        resolution_wh (Tuple[int, int]): A tuple of the form `(width, height)`\n            representing the resolution of the frame.\n\n    Returns:\n        np.ndarray: A numpy array of shape `(N, 4)` where each row\n            corresponds to a bounding box with coordinates clipped to fit\n            within the frame resolution.\n\n    Examples:\n        ```python\n        import numpy as np\n        import supervision as sv\n\n        xyxy = np.array([\n            [10, 20, 300, 200],\n            [15, 25, 350, 450],\n            [-10, -20, 30, 40]\n        ])\n\n        sv.clip_boxes(xyxy=xyxy, resolution_wh=(320, 240))\n        # array([\n        #     [ 10,  20, 300, 200],\n        #     [ 15,  25, 320, 240],\n        #     [  0,   0,  30,  40]\n        # ])\n        ```\n    \"\"\"\n    result = np.copy(xyxy)\n    width, height = resolution_wh\n    result[:, [0, 2]] = result[:, [0, 2]].clip(0, width)\n    result[:, [1, 3]] = result[:, [1, 3]].clip(0, height)\n    return result\n</code></pre> pad_boxes <p>Pads bounding boxes coordinates with a constant padding.</p> <p>Parameters:</p> Name Type Description Default <code>xyxy</code> <code>ndarray</code> <p>A numpy array of shape <code>(N, 4)</code> where each row corresponds to a bounding box in the format <code>(x_min, y_min, x_max, y_max)</code>.</p> required <code>px</code> <code>int</code> <p>The padding value to be added to both the left and right sides of each bounding box.</p> required <code>py</code> <code>Optional[int]</code> <p>The padding value to be added to both the top and bottom sides of each bounding box. If not provided, <code>px</code> will be used for both dimensions.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A numpy array of shape <code>(N, 4)</code> where each row corresponds to a bounding box with coordinates padded according to the provided padding values.</p> <p>Examples:</p> <pre><code>import numpy as np\nimport supervision as sv\n\nxyxy = np.array([\n    [10, 20, 30, 40],\n    [15, 25, 35, 45]\n])\n\nsv.pad_boxes(xyxy=xyxy, px=5, py=10)\n# array([\n#     [ 5, 10, 35, 50],\n#     [10, 15, 40, 55]\n# ])\n</code></pre> Source code in <code>supervision/detection/utils.py</code> <pre><code>def pad_boxes(xyxy: np.ndarray, px: int, py: Optional[int] = None) -&gt; np.ndarray:\n    \"\"\"\n    Pads bounding boxes coordinates with a constant padding.\n\n    Args:\n        xyxy (np.ndarray): A numpy array of shape `(N, 4)` where each\n            row corresponds to a bounding box in the format\n            `(x_min, y_min, x_max, y_max)`.\n        px (int): The padding value to be added to both the left and right sides of\n            each bounding box.\n        py (Optional[int]): The padding value to be added to both the top and bottom\n            sides of each bounding box. If not provided, `px` will be used for both\n            dimensions.\n\n    Returns:\n        np.ndarray: A numpy array of shape `(N, 4)` where each row corresponds to a\n            bounding box with coordinates padded according to the provided padding\n            values.\n\n    Examples:\n        ```python\n        import numpy as np\n        import supervision as sv\n\n        xyxy = np.array([\n            [10, 20, 30, 40],\n            [15, 25, 35, 45]\n        ])\n\n        sv.pad_boxes(xyxy=xyxy, px=5, py=10)\n        # array([\n        #     [ 5, 10, 35, 50],\n        #     [10, 15, 40, 55]\n        # ])\n        ```\n    \"\"\"\n    if py is None:\n        py = px\n\n    result = xyxy.copy()\n    result[:, [0, 1]] -= [px, py]\n    result[:, [2, 3]] += [px, py]\n\n    return result\n</code></pre> contains_holes <p>Converts bounding box coordinates from <code>(x, y, width, height)</code> format to <code>(x_min, y_min, x_max, y_max)</code> format.</p> <p>Parameters:</p> Name Type Description Default <code>xywh</code> <code>ndarray</code> <p>A numpy array of shape <code>(N, 4)</code> where each row corresponds to a bounding box in the format <code>(x, y, width, height)</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A numpy array of shape <code>(N, 4)</code> where each row corresponds to a bounding box in the format <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>Examples:</p> <pre><code>import numpy as np\nimport supervision as sv\n\nxywh = np.array([\n    [10, 20, 30, 40],\n    [15, 25, 35, 45]\n])\n\nsv.xywh_to_xyxy(xywh=xywh)\n# array([\n#     [10, 20, 40, 60],\n#     [15, 25, 50, 70]\n# ])\n</code></pre> Source code in <code>supervision/detection/utils.py</code> <pre><code>def xywh_to_xyxy(xywh: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Converts bounding box coordinates from `(x, y, width, height)`\n    format to `(x_min, y_min, x_max, y_max)` format.\n\n    Args:\n        xywh (np.ndarray): A numpy array of shape `(N, 4)` where each row\n            corresponds to a bounding box in the format `(x, y, width, height)`.\n\n    Returns:\n        np.ndarray: A numpy array of shape `(N, 4)` where each row corresponds\n            to a bounding box in the format `(x_min, y_min, x_max, y_max)`.\n\n    Examples:\n        ```python\n        import numpy as np\n        import supervision as sv\n\n        xywh = np.array([\n            [10, 20, 30, 40],\n            [15, 25, 35, 45]\n        ])\n\n        sv.xywh_to_xyxy(xywh=xywh)\n        # array([\n        #     [10, 20, 40, 60],\n        #     [15, 25, 50, 70]\n        # ])\n        ```\n    \"\"\"\n    xyxy = xywh.copy()\n    xyxy[:, 2] = xywh[:, 0] + xywh[:, 2]\n    xyxy[:, 3] = xywh[:, 1] + xywh[:, 3]\n    return xyxy\n</code></pre> xywh_to_xyxy <p>Converts bounding box coordinates from <code>(center_x, center_y, width, height)</code> format to <code>(x_min, y_min, x_max, y_max)</code> format.</p> <p>Parameters:</p> Name Type Description Default <code>xcycwh</code> <code>ndarray</code> <p>A numpy array of shape <code>(N, 4)</code> where each row corresponds to a bounding box in the format <code>(center_x, center_y, width, height)</code>.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: A numpy array of shape <code>(N, 4)</code> where each row corresponds to a bounding box in the format <code>(x_min, y_min, x_max, y_max)</code>.</p> <p>Examples:</p> <pre><code>import numpy as np\nimport supervision as sv\n\nxcycwh = np.array([\n    [50, 50, 20, 30],\n    [30, 40, 10, 15]\n])\n\nsv.xcycwh_to_xyxy(xcycwh=xcycwh)\n# array([\n#     [40, 35, 60, 65],\n#     [25, 32.5, 35, 47.5]\n# ])\n</code></pre> Source code in <code>supervision/detection/utils.py</code> <pre><code>def xcycwh_to_xyxy(xcycwh: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Converts bounding box coordinates from `(center_x, center_y, width, height)`\n    format to `(x_min, y_min, x_max, y_max)` format.\n\n    Args:\n        xcycwh (np.ndarray): A numpy array of shape `(N, 4)` where each row\n            corresponds to a bounding box in the format `(center_x, center_y, width,\n            height)`.\n\n    Returns:\n        np.ndarray: A numpy array of shape `(N, 4)` where each row corresponds\n            to a bounding box in the format `(x_min, y_min, x_max, y_max)`.\n\n    Examples:\n        ```python\n        import numpy as np\n        import supervision as sv\n\n        xcycwh = np.array([\n            [50, 50, 20, 30],\n            [30, 40, 10, 15]\n        ])\n\n        sv.xcycwh_to_xyxy(xcycwh=xcycwh)\n        # array([\n        #     [40, 35, 60, 65],\n        #     [25, 32.5, 35, 47.5]\n        # ])\n        ```\n    \"\"\"\n    xyxy = xcycwh.copy()\n    xyxy[:, 0] = xcycwh[:, 0] - xcycwh[:, 2] / 2\n    xyxy[:, 1] = xcycwh[:, 1] - xcycwh[:, 3] / 2\n    xyxy[:, 2] = xcycwh[:, 0] + xcycwh[:, 2] / 2\n    xyxy[:, 3] = xcycwh[:, 1] + xcycwh[:, 3] / 2\n    return xyxy\n</code></pre> xcycwh_to_xyxy <p>Checks if the binary mask contains holes (background pixels fully enclosed by foreground pixels).</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>NDArray[bool_]</code> <p>2D binary mask where <code>True</code> indicates foreground object and <code>False</code> indicates background.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if holes are detected, False otherwise.</p> <p>Examples:</p> <pre><code>import numpy as np\nimport supervision as sv\n\nmask = np.array([\n    [0, 0, 0, 0, 0],\n    [0, 1, 1, 1, 0],\n    [0, 1, 0, 1, 0],\n    [0, 1, 1, 1, 0],\n    [0, 0, 0, 0, 0]\n]).astype(bool)\n\nsv.contains_holes(mask=mask)\n# True\n\nmask = np.array([\n    [0, 0, 0, 0, 0],\n    [0, 1, 1, 1, 0],\n    [0, 1, 1, 1, 0],\n    [0, 1, 1, 1, 0],\n    [0, 0, 0, 0, 0]\n]).astype(bool)\n\nsv.contains_holes(mask=mask)\n# False\n</code></pre> <p></p> Source code in <code>supervision/detection/utils.py</code> <pre><code>def contains_holes(mask: npt.NDArray[np.bool_]) -&gt; bool:\n    \"\"\"\n    Checks if the binary mask contains holes (background pixels fully enclosed by\n    foreground pixels).\n\n    Args:\n        mask (npt.NDArray[np.bool_]): 2D binary mask where `True` indicates foreground\n            object and `False` indicates background.\n\n    Returns:\n        True if holes are detected, False otherwise.\n\n    Examples:\n        ```python\n        import numpy as np\n        import supervision as sv\n\n        mask = np.array([\n            [0, 0, 0, 0, 0],\n            [0, 1, 1, 1, 0],\n            [0, 1, 0, 1, 0],\n            [0, 1, 1, 1, 0],\n            [0, 0, 0, 0, 0]\n        ]).astype(bool)\n\n        sv.contains_holes(mask=mask)\n        # True\n\n        mask = np.array([\n            [0, 0, 0, 0, 0],\n            [0, 1, 1, 1, 0],\n            [0, 1, 1, 1, 0],\n            [0, 1, 1, 1, 0],\n            [0, 0, 0, 0, 0]\n        ]).astype(bool)\n\n        sv.contains_holes(mask=mask)\n        # False\n        ```\n\n    ![contains_holes](https://media.roboflow.com/supervision-docs/contains-holes.png){ align=center width=\"800\" }\n    \"\"\"  # noqa E501 // docs\n    mask_uint8 = mask.astype(np.uint8)\n    _, hierarchy = cv2.findContours(mask_uint8, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n\n    if hierarchy is not None:\n        parent_contour_index = 3\n        for h in hierarchy[0]:\n            if h[parent_contour_index] != -1:\n                return True\n    return False\n</code></pre> contains_multiple_segments <p>Checks if the binary mask contains multiple unconnected foreground segments.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>NDArray[bool_]</code> <p>2D binary mask where <code>True</code> indicates foreground object and <code>False</code> indicates background.</p> required <code>connectivity</code> <code>int) </code> <p>Default: 4 is 4-way connectivity, which means that foreground pixels are the part of the same segment/component if their edges touch. Alternatively: 8 for 8-way connectivity, when foreground pixels are connected by their edges or corners touch.</p> <code>4</code> <p>Returns:</p> Type Description <code>bool</code> <p>True when the mask contains multiple not connected components, False otherwise.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If connectivity(int) parameter value is not 4 or 8.</p> <p>Examples:</p> <pre><code>import numpy as np\nimport supervision as sv\n\nmask = np.array([\n    [0, 0, 0, 0, 0, 0],\n    [0, 1, 1, 0, 1, 1],\n    [0, 1, 1, 0, 1, 1],\n    [0, 0, 0, 0, 0, 0],\n    [0, 1, 1, 1, 0, 0],\n    [0, 1, 1, 1, 0, 0]\n]).astype(bool)\n\nsv.contains_multiple_segments(mask=mask, connectivity=4)\n# True\n\nmask = np.array([\n    [0, 0, 0, 0, 0, 0],\n    [0, 1, 1, 1, 1, 1],\n    [0, 1, 1, 1, 1, 1],\n    [0, 1, 1, 1, 1, 1],\n    [0, 1, 1, 1, 1, 1],\n    [0, 0, 0, 0, 0, 0]\n]).astype(bool)\n\nsv.contains_multiple_segments(mask=mask, connectivity=4)\n# False\n</code></pre> <p></p> Source code in <code>supervision/detection/utils.py</code> <pre><code>def contains_multiple_segments(\n    mask: npt.NDArray[np.bool_], connectivity: int = 4\n) -&gt; bool:\n    \"\"\"\n    Checks if the binary mask contains multiple unconnected foreground segments.\n\n    Args:\n        mask (npt.NDArray[np.bool_]): 2D binary mask where `True` indicates foreground\n            object and `False` indicates background.\n        connectivity (int) : Default: 4 is 4-way connectivity, which means that\n            foreground pixels are the part of the same segment/component\n            if their edges touch.\n            Alternatively: 8 for 8-way connectivity, when foreground pixels are\n            connected by their edges or corners touch.\n\n    Returns:\n        True when the mask contains multiple not connected components, False otherwise.\n\n    Raises:\n        ValueError: If connectivity(int) parameter value is not 4 or 8.\n\n    Examples:\n        ```python\n        import numpy as np\n        import supervision as sv\n\n        mask = np.array([\n            [0, 0, 0, 0, 0, 0],\n            [0, 1, 1, 0, 1, 1],\n            [0, 1, 1, 0, 1, 1],\n            [0, 0, 0, 0, 0, 0],\n            [0, 1, 1, 1, 0, 0],\n            [0, 1, 1, 1, 0, 0]\n        ]).astype(bool)\n\n        sv.contains_multiple_segments(mask=mask, connectivity=4)\n        # True\n\n        mask = np.array([\n            [0, 0, 0, 0, 0, 0],\n            [0, 1, 1, 1, 1, 1],\n            [0, 1, 1, 1, 1, 1],\n            [0, 1, 1, 1, 1, 1],\n            [0, 1, 1, 1, 1, 1],\n            [0, 0, 0, 0, 0, 0]\n        ]).astype(bool)\n\n        sv.contains_multiple_segments(mask=mask, connectivity=4)\n        # False\n        ```\n\n    ![contains_multiple_segments](https://media.roboflow.com/supervision-docs/contains-multiple-segments.png){ align=center width=\"800\" }\n    \"\"\"  # noqa E501 // docs\n    if connectivity != 4 and connectivity != 8:\n        raise ValueError(\n            \"Incorrect connectivity value. Possible connectivity values: 4 or 8.\"\n        )\n    mask_uint8 = mask.astype(np.uint8)\n    labels = np.zeros_like(mask_uint8, dtype=np.int32)\n    number_of_labels, _ = cv2.connectedComponents(\n        mask_uint8, labels, connectivity=connectivity\n    )\n    return number_of_labels &gt; 2\n</code></pre>"},{"location":"detection/tools/inference_slicer/","title":"InferenceSlicer","text":"<p>InferenceSlicer performs slicing-based inference for small target detection. This method, often referred to as Slicing Adaptive Inference (SAHI), involves dividing a larger image into smaller slices, performing inference on each slice, and then merging the detections.</p> <p>Parameters:</p> Name Type Description Default <code>slice_wh</code> <code>Tuple[int, int]</code> <p>Dimensions of each slice measured in pixels. The tuple should be in the format <code>(width, height)</code>.</p> <code>(320, 320)</code> <code>overlap_ratio_wh</code> <code>Optional[Tuple[float, float]]</code> <p>A tuple representing the desired overlap ratio for width and height between consecutive slices. Each value should be in the range [0, 1), where 0 means no overlap and a value close to 1 means high overlap.</p> <code>(0.2, 0.2)</code> <code>overlap_wh</code> <code>Optional[Tuple[int, int]]</code> <p>A tuple representing the desired overlap for width and height between consecutive slices measured in pixels. Each value should be greater than or equal to 0.</p> <code>None</code> <code>overlap_filter</code> <code>Union[OverlapFilter, str]</code> <p>Strategy for filtering or merging overlapping detections in slices.</p> <code>NON_MAX_SUPPRESSION</code> <code>iou_threshold</code> <code>float</code> <p>Intersection over Union (IoU) threshold used when filtering by overlap.</p> <code>0.5</code> <code>callback</code> <code>Callable</code> <p>A function that performs inference on a given image slice and returns detections.</p> required <code>thread_workers</code> <code>int</code> <p>Number of threads for parallel execution.</p> <code>1</code> Note <p>The class ensures that slices do not exceed the boundaries of the original image. As a result, the final slices in the row and column dimensions might be smaller than the specified slice dimensions if the image's width or height is not a multiple of the slice's width or height minus the overlap.</p> Source code in <code>supervision/detection/tools/inference_slicer.py</code> <pre><code>class InferenceSlicer:\n    \"\"\"\n    InferenceSlicer performs slicing-based inference for small target detection. This\n    method, often referred to as\n    [Slicing Adaptive Inference (SAHI)](https://ieeexplore.ieee.org/document/9897990),\n    involves dividing a larger image into smaller slices, performing inference on each\n    slice, and then merging the detections.\n\n    Args:\n        slice_wh (Tuple[int, int]): Dimensions of each slice measured in pixels. The\n            tuple should be in the format `(width, height)`.\n        overlap_ratio_wh (Optional[Tuple[float, float]]): A tuple representing the\n            desired overlap ratio for width and height between consecutive slices.\n            Each value should be in the range [0, 1), where 0 means no overlap and\n            a value close to 1 means high overlap.\n        overlap_wh (Optional[Tuple[int, int]]): A tuple representing the desired\n            overlap for width and height between consecutive slices measured in pixels.\n            Each value should be greater than or equal to 0.\n        overlap_filter (Union[OverlapFilter, str]): Strategy for\n            filtering or merging overlapping detections in slices.\n        iou_threshold (float): Intersection over Union (IoU) threshold\n            used when filtering by overlap.\n        callback (Callable): A function that performs inference on a given image\n            slice and returns detections.\n        thread_workers (int): Number of threads for parallel execution.\n\n    Note:\n        The class ensures that slices do not exceed the boundaries of the original\n        image. As a result, the final slices in the row and column dimensions might be\n        smaller than the specified slice dimensions if the image's width or height is\n        not a multiple of the slice's width or height minus the overlap.\n    \"\"\"\n\n    @deprecated_parameter(\n        old_parameter=\"overlap_filter_strategy\",\n        new_parameter=\"overlap_filter\",\n        map_function=lambda x: x,\n        warning_message=\"`{old_parameter}` in `{function_name}` is deprecated and will \"\n        \"be removed in `supervision-0.27.0`. Use '{new_parameter}' \"\n        \"instead.\",\n    )\n    def __init__(\n        self,\n        callback: Callable[[np.ndarray], Detections],\n        slice_wh: Tuple[int, int] = (320, 320),\n        overlap_ratio_wh: Optional[Tuple[float, float]] = (0.2, 0.2),\n        overlap_wh: Optional[Tuple[int, int]] = None,\n        overlap_filter: Union[OverlapFilter, str] = OverlapFilter.NON_MAX_SUPPRESSION,\n        iou_threshold: float = 0.5,\n        thread_workers: int = 1,\n    ):\n        if overlap_ratio_wh is not None:\n            warn_deprecated(\n                \"`overlap_ratio_wh` in `InferenceSlicer.__init__` is deprecated and \"\n                \"will be removed in `supervision-0.27.0`. Use `overlap_wh` instead.\"\n            )\n\n        self._validate_overlap(overlap_ratio_wh, overlap_wh)\n        self.overlap_ratio_wh = overlap_ratio_wh\n        self.overlap_wh = overlap_wh\n\n        self.slice_wh = slice_wh\n        self.iou_threshold = iou_threshold\n        self.overlap_filter = OverlapFilter.from_value(overlap_filter)\n        self.callback = callback\n        self.thread_workers = thread_workers\n\n    def __call__(self, image: np.ndarray) -&gt; Detections:\n        \"\"\"\n        Performs slicing-based inference on the provided image using the specified\n            callback.\n\n        Args:\n            image (np.ndarray): The input image on which inference needs to be\n                performed. The image should be in the format\n                `(height, width, channels)`.\n\n        Returns:\n            Detections: A collection of detections for the entire image after merging\n                results from all slices and applying NMS.\n\n        Example:\n            ```python\n            import cv2\n            import supervision as sv\n            from ultralytics import YOLO\n\n            image = cv2.imread(SOURCE_IMAGE_PATH)\n            model = YOLO(...)\n\n            def callback(image_slice: np.ndarray) -&gt; sv.Detections:\n                result = model(image_slice)[0]\n                return sv.Detections.from_ultralytics(result)\n\n            slicer = sv.InferenceSlicer(\n                callback=callback,\n                overlap_filter_strategy=sv.OverlapFilter.NON_MAX_SUPPRESSION,\n            )\n\n            detections = slicer(image)\n            ```\n        \"\"\"\n        detections_list = []\n        resolution_wh = (image.shape[1], image.shape[0])\n        offsets = self._generate_offset(\n            resolution_wh=resolution_wh,\n            slice_wh=self.slice_wh,\n            overlap_ratio_wh=self.overlap_ratio_wh,\n            overlap_wh=self.overlap_wh,\n        )\n\n        with ThreadPoolExecutor(max_workers=self.thread_workers) as executor:\n            futures = [\n                executor.submit(self._run_callback, image, offset) for offset in offsets\n            ]\n            for future in as_completed(futures):\n                detections_list.append(future.result())\n\n        merged = Detections.merge(detections_list=detections_list)\n        if self.overlap_filter == OverlapFilter.NONE:\n            return merged\n        elif self.overlap_filter == OverlapFilter.NON_MAX_SUPPRESSION:\n            return merged.with_nms(threshold=self.iou_threshold)\n        elif self.overlap_filter == OverlapFilter.NON_MAX_MERGE:\n            return merged.with_nmm(threshold=self.iou_threshold)\n        else:\n            warnings.warn(\n                f\"Invalid overlap filter strategy: {self.overlap_filter}\",\n                category=SupervisionWarnings,\n            )\n            return merged\n\n    def _run_callback(self, image, offset) -&gt; Detections:\n        \"\"\"\n        Run the provided callback on a slice of an image.\n\n        Args:\n            image (np.ndarray): The input image on which inference needs to run\n            offset (np.ndarray): An array of shape `(4,)` containing coordinates\n                for the slice.\n\n        Returns:\n            Detections: A collection of detections for the slice.\n        \"\"\"\n        image_slice = crop_image(image=image, xyxy=offset)\n        detections = self.callback(image_slice)\n        resolution_wh = (image.shape[1], image.shape[0])\n        detections = move_detections(\n            detections=detections, offset=offset[:2], resolution_wh=resolution_wh\n        )\n\n        return detections\n\n    @staticmethod\n    def _generate_offset(\n        resolution_wh: Tuple[int, int],\n        slice_wh: Tuple[int, int],\n        overlap_ratio_wh: Optional[Tuple[float, float]],\n        overlap_wh: Optional[Tuple[int, int]],\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Generate offset coordinates for slicing an image based on the given resolution,\n        slice dimensions, and overlap ratios.\n\n        Args:\n            resolution_wh (Tuple[int, int]): A tuple representing the width and height\n                of the image to be sliced.\n            slice_wh (Tuple[int, int]): Dimensions of each slice measured in pixels. The\n            tuple should be in the format `(width, height)`.\n            overlap_ratio_wh (Optional[Tuple[float, float]]): A tuple representing the\n                desired overlap ratio for width and height between consecutive slices.\n                Each value should be in the range [0, 1), where 0 means no overlap and\n                a value close to 1 means high overlap.\n            overlap_wh (Optional[Tuple[int, int]]): A tuple representing the desired\n                overlap for width and height between consecutive slices measured in\n                pixels. Each value should be greater than or equal to 0.\n\n        Returns:\n            np.ndarray: An array of shape `(n, 4)` containing coordinates for each\n                slice in the format `[xmin, ymin, xmax, ymax]`.\n\n        Note:\n            The function ensures that slices do not exceed the boundaries of the\n                original image. As a result, the final slices in the row and column\n                dimensions might be smaller than the specified slice dimensions if the\n                image's width or height is not a multiple of the slice's width or\n                height minus the overlap.\n        \"\"\"\n        slice_width, slice_height = slice_wh\n        image_width, image_height = resolution_wh\n        overlap_width = (\n            overlap_wh[0]\n            if overlap_wh is not None\n            else int(overlap_ratio_wh[0] * slice_width)\n        )\n        overlap_height = (\n            overlap_wh[1]\n            if overlap_wh is not None\n            else int(overlap_ratio_wh[1] * slice_height)\n        )\n\n        width_stride = slice_width - overlap_width\n        height_stride = slice_height - overlap_height\n\n        ws = np.arange(0, image_width, width_stride)\n        hs = np.arange(0, image_height, height_stride)\n\n        xmin, ymin = np.meshgrid(ws, hs)\n        xmax = np.clip(xmin + slice_width, 0, image_width)\n        ymax = np.clip(ymin + slice_height, 0, image_height)\n\n        offsets = np.stack([xmin, ymin, xmax, ymax], axis=-1).reshape(-1, 4)\n\n        return offsets\n\n    @staticmethod\n    def _validate_overlap(\n        overlap_ratio_wh: Optional[Tuple[float, float]],\n        overlap_wh: Optional[Tuple[int, int]],\n    ) -&gt; None:\n        if overlap_ratio_wh is not None and overlap_wh is not None:\n            raise ValueError(\n                \"Both `overlap_ratio_wh` and `overlap_wh` cannot be provided. \"\n                \"Please provide only one of them.\"\n            )\n        if overlap_ratio_wh is None and overlap_wh is None:\n            raise ValueError(\n                \"Either `overlap_ratio_wh` or `overlap_wh` must be provided. \"\n                \"Please provide one of them.\"\n            )\n\n        if overlap_ratio_wh is not None:\n            if not (0 &lt;= overlap_ratio_wh[0] &lt; 1 and 0 &lt;= overlap_ratio_wh[1] &lt; 1):\n                raise ValueError(\n                    \"Overlap ratios must be in the range [0, 1). \"\n                    f\"Received: {overlap_ratio_wh}\"\n                )\n        if overlap_wh is not None:\n            if not (overlap_wh[0] &gt;= 0 and overlap_wh[1] &gt;= 0):\n                raise ValueError(\n                    \"Overlap values must be greater than or equal to 0. \"\n                    f\"Received: {overlap_wh}\"\n                )\n</code></pre>"},{"location":"detection/tools/inference_slicer/#supervision.detection.tools.inference_slicer.InferenceSlicer-functions","title":"Functions","text":""},{"location":"detection/tools/inference_slicer/#supervision.detection.tools.inference_slicer.InferenceSlicer.__call__","title":"<code>__call__(image)</code>","text":"<p>Performs slicing-based inference on the provided image using the specified     callback.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The input image on which inference needs to be performed. The image should be in the format <code>(height, width, channels)</code>.</p> required <p>Returns:</p> Name Type Description <code>Detections</code> <code>Detections</code> <p>A collection of detections for the entire image after merging results from all slices and applying NMS.</p> Example <pre><code>import cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nimage = cv2.imread(SOURCE_IMAGE_PATH)\nmodel = YOLO(...)\n\ndef callback(image_slice: np.ndarray) -&gt; sv.Detections:\n    result = model(image_slice)[0]\n    return sv.Detections.from_ultralytics(result)\n\nslicer = sv.InferenceSlicer(\n    callback=callback,\n    overlap_filter_strategy=sv.OverlapFilter.NON_MAX_SUPPRESSION,\n)\n\ndetections = slicer(image)\n</code></pre> Source code in <code>supervision/detection/tools/inference_slicer.py</code> <pre><code>def __call__(self, image: np.ndarray) -&gt; Detections:\n    \"\"\"\n    Performs slicing-based inference on the provided image using the specified\n        callback.\n\n    Args:\n        image (np.ndarray): The input image on which inference needs to be\n            performed. The image should be in the format\n            `(height, width, channels)`.\n\n    Returns:\n        Detections: A collection of detections for the entire image after merging\n            results from all slices and applying NMS.\n\n    Example:\n        ```python\n        import cv2\n        import supervision as sv\n        from ultralytics import YOLO\n\n        image = cv2.imread(SOURCE_IMAGE_PATH)\n        model = YOLO(...)\n\n        def callback(image_slice: np.ndarray) -&gt; sv.Detections:\n            result = model(image_slice)[0]\n            return sv.Detections.from_ultralytics(result)\n\n        slicer = sv.InferenceSlicer(\n            callback=callback,\n            overlap_filter_strategy=sv.OverlapFilter.NON_MAX_SUPPRESSION,\n        )\n\n        detections = slicer(image)\n        ```\n    \"\"\"\n    detections_list = []\n    resolution_wh = (image.shape[1], image.shape[0])\n    offsets = self._generate_offset(\n        resolution_wh=resolution_wh,\n        slice_wh=self.slice_wh,\n        overlap_ratio_wh=self.overlap_ratio_wh,\n        overlap_wh=self.overlap_wh,\n    )\n\n    with ThreadPoolExecutor(max_workers=self.thread_workers) as executor:\n        futures = [\n            executor.submit(self._run_callback, image, offset) for offset in offsets\n        ]\n        for future in as_completed(futures):\n            detections_list.append(future.result())\n\n    merged = Detections.merge(detections_list=detections_list)\n    if self.overlap_filter == OverlapFilter.NONE:\n        return merged\n    elif self.overlap_filter == OverlapFilter.NON_MAX_SUPPRESSION:\n        return merged.with_nms(threshold=self.iou_threshold)\n    elif self.overlap_filter == OverlapFilter.NON_MAX_MERGE:\n        return merged.with_nmm(threshold=self.iou_threshold)\n    else:\n        warnings.warn(\n            f\"Invalid overlap filter strategy: {self.overlap_filter}\",\n            category=SupervisionWarnings,\n        )\n        return merged\n</code></pre>"},{"location":"detection/tools/line_zone/","title":"Line Zone","text":"LineZone <p>This class is responsible for counting the number of objects that cross a predefined line.</p> <p>Warning</p> <p>LineZone uses the <code>tracker_id</code>. Read here to learn how to plug tracking into your inference pipeline.</p> <p>Attributes:</p> Name Type Description <code>in_count</code> <code>int</code> <p>The number of objects that have crossed the line from outside to inside.</p> <code>out_count</code> <code>int</code> <p>The number of objects that have crossed the line from inside to outside.</p> Example <pre><code>import supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(&lt;SOURCE_MODEL_PATH&gt;)\ntracker = sv.ByteTrack()\nframes_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;)\nstart, end = sv.Point(x=0, y=1080), sv.Point(x=3840, y=1080)\nline_zone = sv.LineZone(start=start, end=end)\n\nfor frame in frames_generator:\n    result = model(frame)[0]\n    detections = sv.Detections.from_ultralytics(result)\n    detections = tracker.update_with_detections(detections)\n    crossed_in, crossed_out = line_zone.trigger(detections)\n\nline_zone.in_count, line_zone.out_count\n# 7, 2\n</code></pre> Source code in <code>supervision/detection/line_zone.py</code> <pre><code>class LineZone:\n    \"\"\"\n    This class is responsible for counting the number of objects that cross a\n    predefined line.\n\n    &lt;video controls&gt;\n        &lt;source\n            src=\"https://media.roboflow.com/supervision/cookbooks/count-objects-crossing-the-line-result-1280x720.mp4\"\n            type=\"video/mp4\"&gt;\n    &lt;/video&gt;\n\n    !!! warning\n\n        LineZone uses the `tracker_id`. Read\n        [here](/latest/trackers/) to learn how to plug\n        tracking into your inference pipeline.\n\n    Attributes:\n        in_count (int): The number of objects that have crossed the line from outside\n            to inside.\n        out_count (int): The number of objects that have crossed the line from inside\n            to outside.\n\n    Example:\n        ```python\n        import supervision as sv\n        from ultralytics import YOLO\n\n        model = YOLO(&lt;SOURCE_MODEL_PATH&gt;)\n        tracker = sv.ByteTrack()\n        frames_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;)\n        start, end = sv.Point(x=0, y=1080), sv.Point(x=3840, y=1080)\n        line_zone = sv.LineZone(start=start, end=end)\n\n        for frame in frames_generator:\n            result = model(frame)[0]\n            detections = sv.Detections.from_ultralytics(result)\n            detections = tracker.update_with_detections(detections)\n            crossed_in, crossed_out = line_zone.trigger(detections)\n\n        line_zone.in_count, line_zone.out_count\n        # 7, 2\n        ```\n    \"\"\"  # noqa: E501 // docs\n\n    def __init__(\n        self,\n        start: Point,\n        end: Point,\n        triggering_anchors: Iterable[Position] = (\n            Position.TOP_LEFT,\n            Position.TOP_RIGHT,\n            Position.BOTTOM_LEFT,\n            Position.BOTTOM_RIGHT,\n        ),\n    ):\n        \"\"\"\n        Args:\n            start (Point): The starting point of the line.\n            end (Point): The ending point of the line.\n            triggering_anchors (List[sv.Position]): A list of positions\n                specifying which anchors of the detections bounding box\n                to consider when deciding on whether the detection\n                has passed the line counter or not. By default, this\n                contains the four corners of the detection's bounding box\n        \"\"\"\n        self.vector = Vector(start=start, end=end)\n        self.limits = self.calculate_region_of_interest_limits(vector=self.vector)\n        self.tracker_state: Dict[str, bool] = {}\n        self.in_count: int = 0\n        self.out_count: int = 0\n        self.triggering_anchors = triggering_anchors\n        if not list(self.triggering_anchors):\n            raise ValueError(\"Triggering anchors cannot be empty.\")\n\n    @staticmethod\n    def calculate_region_of_interest_limits(vector: Vector) -&gt; Tuple[Vector, Vector]:\n        magnitude = vector.magnitude\n\n        if magnitude == 0:\n            raise ValueError(\"The magnitude of the vector cannot be zero.\")\n\n        delta_x = vector.end.x - vector.start.x\n        delta_y = vector.end.y - vector.start.y\n\n        unit_vector_x = delta_x / magnitude\n        unit_vector_y = delta_y / magnitude\n\n        perpendicular_vector_x = -unit_vector_y\n        perpendicular_vector_y = unit_vector_x\n\n        start_region_limit = Vector(\n            start=vector.start,\n            end=Point(\n                x=vector.start.x + perpendicular_vector_x,\n                y=vector.start.y + perpendicular_vector_y,\n            ),\n        )\n        end_region_limit = Vector(\n            start=vector.end,\n            end=Point(\n                x=vector.end.x - perpendicular_vector_x,\n                y=vector.end.y - perpendicular_vector_y,\n            ),\n        )\n        return start_region_limit, end_region_limit\n\n    @staticmethod\n    def is_point_in_limits(point: Point, limits: Tuple[Vector, Vector]) -&gt; bool:\n        cross_product_1 = limits[0].cross_product(point)\n        cross_product_2 = limits[1].cross_product(point)\n        return (cross_product_1 &gt; 0) == (cross_product_2 &gt; 0)\n\n    def trigger(self, detections: Detections) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Update the `in_count` and `out_count` based on the objects that cross the line.\n\n        Args:\n            detections (Detections): A list of detections for which to update the\n                counts.\n\n        Returns:\n            A tuple of two boolean NumPy arrays. The first array indicates which\n                detections have crossed the line from outside to inside. The second\n                array indicates which detections have crossed the line from inside to\n                outside.\n        \"\"\"\n        crossed_in = np.full(len(detections), False)\n        crossed_out = np.full(len(detections), False)\n\n        if len(detections) == 0:\n            return crossed_in, crossed_out\n\n        if detections.tracker_id is None:\n            warnings.warn(\n                \"Line zone counting skipped. LineZone requires tracker_id. Refer to \"\n                \"https://supervision.roboflow.com/latest/trackers for more \"\n                \"information.\",\n                category=SupervisionWarnings,\n            )\n            return crossed_in, crossed_out\n\n        all_anchors = np.array(\n            [\n                detections.get_anchors_coordinates(anchor)\n                for anchor in self.triggering_anchors\n            ]\n        )\n\n        cross_products_1 = cross_product(all_anchors, self.limits[0])\n        cross_products_2 = cross_product(all_anchors, self.limits[1])\n        in_limits = (cross_products_1 &gt; 0) == (cross_products_2 &gt; 0)\n        in_limits = np.all(in_limits, axis=0)\n\n        triggers = cross_product(all_anchors, self.vector) &lt; 0\n        has_any_left_trigger = np.any(triggers, axis=0)\n        has_any_right_trigger = np.any(~triggers, axis=0)\n        is_uniformly_triggered = ~(has_any_left_trigger &amp; has_any_right_trigger)\n        for i, tracker_id in enumerate(detections.tracker_id):\n            if not in_limits[i]:\n                continue\n\n            if not is_uniformly_triggered[i]:\n                continue\n\n            tracker_state = has_any_left_trigger[i]\n            if tracker_id not in self.tracker_state:\n                self.tracker_state[tracker_id] = tracker_state\n                continue\n\n            if self.tracker_state.get(tracker_id) == tracker_state:\n                continue\n\n            self.tracker_state[tracker_id] = tracker_state\n            if tracker_state:\n                self.in_count += 1\n                crossed_in[i] = True\n            else:\n                self.out_count += 1\n                crossed_out[i] = True\n\n        return crossed_in, crossed_out\n</code></pre> LineZoneAnnotator Source code in <code>supervision/detection/line_zone.py</code> <pre><code>class LineZoneAnnotator:\n    def __init__(\n        self,\n        thickness: float = 2,\n        color: Color = Color.WHITE,\n        text_thickness: float = 2,\n        text_color: Color = Color.BLACK,\n        text_scale: float = 0.5,\n        text_offset: float = 1.5,\n        text_padding: int = 10,\n        custom_in_text: Optional[str] = None,\n        custom_out_text: Optional[str] = None,\n        display_in_count: bool = True,\n        display_out_count: bool = True,\n    ):\n        \"\"\"\n        Initialize the LineCounterAnnotator object with default values.\n\n        Attributes:\n            thickness (float): The thickness of the line that will be drawn.\n            color (Color): The color of the line that will be drawn.\n            text_thickness (float): The thickness of the text that will be drawn.\n            text_color (Color): The color of the text that will be drawn.\n            text_scale (float): The scale of the text that will be drawn.\n            text_offset (float): The offset of the text that will be drawn.\n            text_padding (int): The padding of the text that will be drawn.\n            display_in_count (bool): Whether to display the in count or not.\n            display_out_count (bool): Whether to display the out count or not.\n\n        \"\"\"\n        self.thickness: float = thickness\n        self.color: Color = color\n        self.text_thickness: float = text_thickness\n        self.text_color: Color = text_color\n        self.text_scale: float = text_scale\n        self.text_offset: float = text_offset\n        self.text_padding: int = text_padding\n        self.custom_in_text: str = custom_in_text\n        self.custom_out_text: str = custom_out_text\n        self.display_in_count: bool = display_in_count\n        self.display_out_count: bool = display_out_count\n\n    def _annotate_count(\n        self,\n        frame: np.ndarray,\n        center_text_anchor: Point,\n        text: str,\n        is_in_count: bool,\n    ) -&gt; None:\n        \"\"\"This method is drawing the text on the frame.\n\n        Args:\n            frame (np.ndarray): The image on which the text will be drawn.\n            center_text_anchor: The center point that the text will be drawn.\n            text (str): The text that will be drawn.\n            is_in_count (bool): Whether to display the in count or out count.\n        \"\"\"\n        _, text_height = cv2.getTextSize(\n            text, cv2.FONT_HERSHEY_SIMPLEX, self.text_scale, self.text_thickness\n        )[0]\n\n        if is_in_count:\n            center_text_anchor.y -= int(self.text_offset * text_height)\n        else:\n            center_text_anchor.y += int(self.text_offset * text_height)\n\n        draw_text(\n            scene=frame,\n            text=text,\n            text_anchor=center_text_anchor,\n            text_color=self.text_color,\n            text_scale=self.text_scale,\n            text_thickness=self.text_thickness,\n            text_padding=self.text_padding,\n            background_color=self.color,\n        )\n\n    def annotate(self, frame: np.ndarray, line_counter: LineZone) -&gt; np.ndarray:\n        \"\"\"\n        Draws the line on the frame using the line_counter provided.\n\n        Attributes:\n            frame (np.ndarray): The image on which the line will be drawn.\n            line_counter (LineCounter): The line counter\n                that will be used to draw the line.\n\n        Returns:\n            np.ndarray: The image with the line drawn on it.\n\n        \"\"\"\n        cv2.line(\n            frame,\n            line_counter.vector.start.as_xy_int_tuple(),\n            line_counter.vector.end.as_xy_int_tuple(),\n            self.color.as_bgr(),\n            self.thickness,\n            lineType=cv2.LINE_AA,\n            shift=0,\n        )\n        cv2.circle(\n            frame,\n            line_counter.vector.start.as_xy_int_tuple(),\n            radius=5,\n            color=self.text_color.as_bgr(),\n            thickness=-1,\n            lineType=cv2.LINE_AA,\n        )\n        cv2.circle(\n            frame,\n            line_counter.vector.end.as_xy_int_tuple(),\n            radius=5,\n            color=self.text_color.as_bgr(),\n            thickness=-1,\n            lineType=cv2.LINE_AA,\n        )\n\n        text_anchor = Vector(\n            start=line_counter.vector.start, end=line_counter.vector.end\n        )\n\n        if self.display_in_count:\n            in_text = (\n                f\"{self.custom_in_text}: {line_counter.in_count}\"\n                if self.custom_in_text is not None\n                else f\"in: {line_counter.in_count}\"\n            )\n            self._annotate_count(\n                frame=frame,\n                center_text_anchor=text_anchor.center,\n                text=in_text,\n                is_in_count=True,\n            )\n\n        if self.display_out_count:\n            out_text = (\n                f\"{self.custom_out_text}: {line_counter.out_count}\"\n                if self.custom_out_text is not None\n                else f\"out: {line_counter.out_count}\"\n            )\n            self._annotate_count(\n                frame=frame,\n                center_text_anchor=text_anchor.center,\n                text=out_text,\n                is_in_count=False,\n            )\n        return frame\n</code></pre>"},{"location":"detection/tools/line_zone/#supervision.detection.line_zone.LineZone-functions","title":"Functions","text":""},{"location":"detection/tools/line_zone/#supervision.detection.line_zone.LineZone.__init__","title":"<code>__init__(start, end, triggering_anchors=(Position.TOP_LEFT, Position.TOP_RIGHT, Position.BOTTOM_LEFT, Position.BOTTOM_RIGHT))</code>","text":"<p>Parameters:</p> Name Type Description Default <code>start</code> <code>Point</code> <p>The starting point of the line.</p> required <code>end</code> <code>Point</code> <p>The ending point of the line.</p> required <code>triggering_anchors</code> <code>List[Position]</code> <p>A list of positions specifying which anchors of the detections bounding box to consider when deciding on whether the detection has passed the line counter or not. By default, this contains the four corners of the detection's bounding box</p> <code>(TOP_LEFT, TOP_RIGHT, BOTTOM_LEFT, BOTTOM_RIGHT)</code> Source code in <code>supervision/detection/line_zone.py</code> <pre><code>def __init__(\n    self,\n    start: Point,\n    end: Point,\n    triggering_anchors: Iterable[Position] = (\n        Position.TOP_LEFT,\n        Position.TOP_RIGHT,\n        Position.BOTTOM_LEFT,\n        Position.BOTTOM_RIGHT,\n    ),\n):\n    \"\"\"\n    Args:\n        start (Point): The starting point of the line.\n        end (Point): The ending point of the line.\n        triggering_anchors (List[sv.Position]): A list of positions\n            specifying which anchors of the detections bounding box\n            to consider when deciding on whether the detection\n            has passed the line counter or not. By default, this\n            contains the four corners of the detection's bounding box\n    \"\"\"\n    self.vector = Vector(start=start, end=end)\n    self.limits = self.calculate_region_of_interest_limits(vector=self.vector)\n    self.tracker_state: Dict[str, bool] = {}\n    self.in_count: int = 0\n    self.out_count: int = 0\n    self.triggering_anchors = triggering_anchors\n    if not list(self.triggering_anchors):\n        raise ValueError(\"Triggering anchors cannot be empty.\")\n</code></pre>"},{"location":"detection/tools/line_zone/#supervision.detection.line_zone.LineZone.trigger","title":"<code>trigger(detections)</code>","text":"<p>Update the <code>in_count</code> and <code>out_count</code> based on the objects that cross the line.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p>A list of detections for which to update the counts.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>A tuple of two boolean NumPy arrays. The first array indicates which detections have crossed the line from outside to inside. The second array indicates which detections have crossed the line from inside to outside.</p> Source code in <code>supervision/detection/line_zone.py</code> <pre><code>def trigger(self, detections: Detections) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Update the `in_count` and `out_count` based on the objects that cross the line.\n\n    Args:\n        detections (Detections): A list of detections for which to update the\n            counts.\n\n    Returns:\n        A tuple of two boolean NumPy arrays. The first array indicates which\n            detections have crossed the line from outside to inside. The second\n            array indicates which detections have crossed the line from inside to\n            outside.\n    \"\"\"\n    crossed_in = np.full(len(detections), False)\n    crossed_out = np.full(len(detections), False)\n\n    if len(detections) == 0:\n        return crossed_in, crossed_out\n\n    if detections.tracker_id is None:\n        warnings.warn(\n            \"Line zone counting skipped. LineZone requires tracker_id. Refer to \"\n            \"https://supervision.roboflow.com/latest/trackers for more \"\n            \"information.\",\n            category=SupervisionWarnings,\n        )\n        return crossed_in, crossed_out\n\n    all_anchors = np.array(\n        [\n            detections.get_anchors_coordinates(anchor)\n            for anchor in self.triggering_anchors\n        ]\n    )\n\n    cross_products_1 = cross_product(all_anchors, self.limits[0])\n    cross_products_2 = cross_product(all_anchors, self.limits[1])\n    in_limits = (cross_products_1 &gt; 0) == (cross_products_2 &gt; 0)\n    in_limits = np.all(in_limits, axis=0)\n\n    triggers = cross_product(all_anchors, self.vector) &lt; 0\n    has_any_left_trigger = np.any(triggers, axis=0)\n    has_any_right_trigger = np.any(~triggers, axis=0)\n    is_uniformly_triggered = ~(has_any_left_trigger &amp; has_any_right_trigger)\n    for i, tracker_id in enumerate(detections.tracker_id):\n        if not in_limits[i]:\n            continue\n\n        if not is_uniformly_triggered[i]:\n            continue\n\n        tracker_state = has_any_left_trigger[i]\n        if tracker_id not in self.tracker_state:\n            self.tracker_state[tracker_id] = tracker_state\n            continue\n\n        if self.tracker_state.get(tracker_id) == tracker_state:\n            continue\n\n        self.tracker_state[tracker_id] = tracker_state\n        if tracker_state:\n            self.in_count += 1\n            crossed_in[i] = True\n        else:\n            self.out_count += 1\n            crossed_out[i] = True\n\n    return crossed_in, crossed_out\n</code></pre>"},{"location":"detection/tools/line_zone/#supervision.detection.line_zone.LineZoneAnnotator-functions","title":"Functions","text":""},{"location":"detection/tools/line_zone/#supervision.detection.line_zone.LineZoneAnnotator.__init__","title":"<code>__init__(thickness=2, color=Color.WHITE, text_thickness=2, text_color=Color.BLACK, text_scale=0.5, text_offset=1.5, text_padding=10, custom_in_text=None, custom_out_text=None, display_in_count=True, display_out_count=True)</code>","text":"<p>Initialize the LineCounterAnnotator object with default values.</p> <p>Attributes:</p> Name Type Description <code>thickness</code> <code>float</code> <p>The thickness of the line that will be drawn.</p> <code>color</code> <code>Color</code> <p>The color of the line that will be drawn.</p> <code>text_thickness</code> <code>float</code> <p>The thickness of the text that will be drawn.</p> <code>text_color</code> <code>Color</code> <p>The color of the text that will be drawn.</p> <code>text_scale</code> <code>float</code> <p>The scale of the text that will be drawn.</p> <code>text_offset</code> <code>float</code> <p>The offset of the text that will be drawn.</p> <code>text_padding</code> <code>int</code> <p>The padding of the text that will be drawn.</p> <code>display_in_count</code> <code>bool</code> <p>Whether to display the in count or not.</p> <code>display_out_count</code> <code>bool</code> <p>Whether to display the out count or not.</p> Source code in <code>supervision/detection/line_zone.py</code> <pre><code>def __init__(\n    self,\n    thickness: float = 2,\n    color: Color = Color.WHITE,\n    text_thickness: float = 2,\n    text_color: Color = Color.BLACK,\n    text_scale: float = 0.5,\n    text_offset: float = 1.5,\n    text_padding: int = 10,\n    custom_in_text: Optional[str] = None,\n    custom_out_text: Optional[str] = None,\n    display_in_count: bool = True,\n    display_out_count: bool = True,\n):\n    \"\"\"\n    Initialize the LineCounterAnnotator object with default values.\n\n    Attributes:\n        thickness (float): The thickness of the line that will be drawn.\n        color (Color): The color of the line that will be drawn.\n        text_thickness (float): The thickness of the text that will be drawn.\n        text_color (Color): The color of the text that will be drawn.\n        text_scale (float): The scale of the text that will be drawn.\n        text_offset (float): The offset of the text that will be drawn.\n        text_padding (int): The padding of the text that will be drawn.\n        display_in_count (bool): Whether to display the in count or not.\n        display_out_count (bool): Whether to display the out count or not.\n\n    \"\"\"\n    self.thickness: float = thickness\n    self.color: Color = color\n    self.text_thickness: float = text_thickness\n    self.text_color: Color = text_color\n    self.text_scale: float = text_scale\n    self.text_offset: float = text_offset\n    self.text_padding: int = text_padding\n    self.custom_in_text: str = custom_in_text\n    self.custom_out_text: str = custom_out_text\n    self.display_in_count: bool = display_in_count\n    self.display_out_count: bool = display_out_count\n</code></pre>"},{"location":"detection/tools/line_zone/#supervision.detection.line_zone.LineZoneAnnotator.annotate","title":"<code>annotate(frame, line_counter)</code>","text":"<p>Draws the line on the frame using the line_counter provided.</p> <p>Attributes:</p> Name Type Description <code>frame</code> <code>ndarray</code> <p>The image on which the line will be drawn.</p> <code>line_counter</code> <code>LineCounter</code> <p>The line counter that will be used to draw the line.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The image with the line drawn on it.</p> Source code in <code>supervision/detection/line_zone.py</code> <pre><code>def annotate(self, frame: np.ndarray, line_counter: LineZone) -&gt; np.ndarray:\n    \"\"\"\n    Draws the line on the frame using the line_counter provided.\n\n    Attributes:\n        frame (np.ndarray): The image on which the line will be drawn.\n        line_counter (LineCounter): The line counter\n            that will be used to draw the line.\n\n    Returns:\n        np.ndarray: The image with the line drawn on it.\n\n    \"\"\"\n    cv2.line(\n        frame,\n        line_counter.vector.start.as_xy_int_tuple(),\n        line_counter.vector.end.as_xy_int_tuple(),\n        self.color.as_bgr(),\n        self.thickness,\n        lineType=cv2.LINE_AA,\n        shift=0,\n    )\n    cv2.circle(\n        frame,\n        line_counter.vector.start.as_xy_int_tuple(),\n        radius=5,\n        color=self.text_color.as_bgr(),\n        thickness=-1,\n        lineType=cv2.LINE_AA,\n    )\n    cv2.circle(\n        frame,\n        line_counter.vector.end.as_xy_int_tuple(),\n        radius=5,\n        color=self.text_color.as_bgr(),\n        thickness=-1,\n        lineType=cv2.LINE_AA,\n    )\n\n    text_anchor = Vector(\n        start=line_counter.vector.start, end=line_counter.vector.end\n    )\n\n    if self.display_in_count:\n        in_text = (\n            f\"{self.custom_in_text}: {line_counter.in_count}\"\n            if self.custom_in_text is not None\n            else f\"in: {line_counter.in_count}\"\n        )\n        self._annotate_count(\n            frame=frame,\n            center_text_anchor=text_anchor.center,\n            text=in_text,\n            is_in_count=True,\n        )\n\n    if self.display_out_count:\n        out_text = (\n            f\"{self.custom_out_text}: {line_counter.out_count}\"\n            if self.custom_out_text is not None\n            else f\"out: {line_counter.out_count}\"\n        )\n        self._annotate_count(\n            frame=frame,\n            center_text_anchor=text_anchor.center,\n            text=out_text,\n            is_in_count=False,\n        )\n    return frame\n</code></pre>"},{"location":"detection/tools/polygon_zone/","title":"Polygon Zone","text":"PolygonZone <p>A class for defining a polygon-shaped zone within a frame for detecting objects.</p> <p>Attributes:</p> Name Type Description <code>polygon</code> <code>ndarray</code> <p>A polygon represented by a numpy array of shape <code>(N, 2)</code>, containing the <code>x</code>, <code>y</code> coordinates of the points.</p> <code>triggering_anchors</code> <code>Iterable[Position]</code> <p>A list of positions specifying which anchors of the detections bounding box to consider when deciding on whether the detection fits within the PolygonZone (default: (sv.Position.BOTTOM_CENTER,)).</p> <code>current_count</code> <code>int</code> <p>The current count of detected objects within the zone</p> <code>mask</code> <code>ndarray</code> <p>The 2D bool mask for the polygon zone</p> Source code in <code>supervision/detection/tools/polygon_zone.py</code> <pre><code>class PolygonZone:\n    \"\"\"\n    A class for defining a polygon-shaped zone within a frame for detecting objects.\n\n    Attributes:\n        polygon (np.ndarray): A polygon represented by a numpy array of shape\n            `(N, 2)`, containing the `x`, `y` coordinates of the points.\n        triggering_anchors (Iterable[sv.Position]): A list of positions specifying\n            which anchors of the detections bounding box to consider when deciding on\n            whether the detection fits within the PolygonZone\n            (default: (sv.Position.BOTTOM_CENTER,)).\n        current_count (int): The current count of detected objects within the zone\n        mask (np.ndarray): The 2D bool mask for the polygon zone\n    \"\"\"\n\n    @deprecated_parameter(\n        old_parameter=\"triggering_position\",\n        new_parameter=\"triggering_anchors\",\n        map_function=lambda x: [x],\n        warning_message=\"`{old_parameter}` in `{function_name}` is deprecated and will \"\n        \"be remove in `supervision-0.23.0`. Use '{new_parameter}' \"\n        \"instead.\",\n    )\n    def __init__(\n        self,\n        polygon: npt.NDArray[np.int64],\n        frame_resolution_wh: Optional[Tuple[int, int]] = None,\n        triggering_anchors: Iterable[Position] = (Position.BOTTOM_CENTER,),\n    ):\n        if frame_resolution_wh is not None:\n            warnings.warn(\n                \"The `frame_resolution_wh` parameter is no longer required and will be \"\n                \"dropped in version supervision-0.24.0. The mask resolution is now \"\n                \"calculated automatically based on the polygon coordinates.\",\n                category=SupervisionWarnings,\n            )\n\n        self.polygon = polygon.astype(int)\n        self.triggering_anchors = triggering_anchors\n        if not list(self.triggering_anchors):\n            raise ValueError(\"Triggering anchors cannot be empty.\")\n\n        self.current_count = 0\n\n        x_max, y_max = np.max(polygon, axis=0)\n        self.frame_resolution_wh = (x_max + 1, y_max + 1)\n        self.mask = polygon_to_mask(\n            polygon=polygon, resolution_wh=(x_max + 2, y_max + 2)\n        )\n\n    def trigger(self, detections: Detections) -&gt; npt.NDArray[np.bool_]:\n        \"\"\"\n        Determines if the detections are within the polygon zone.\n\n        Parameters:\n            detections (Detections): The detections\n                to be checked against the polygon zone\n\n        Returns:\n            np.ndarray: A boolean numpy array indicating\n                if each detection is within the polygon zone\n        \"\"\"\n\n        clipped_xyxy = clip_boxes(\n            xyxy=detections.xyxy, resolution_wh=self.frame_resolution_wh\n        )\n        clipped_detections = replace(detections, xyxy=clipped_xyxy)\n        all_clipped_anchors = np.array(\n            [\n                np.ceil(clipped_detections.get_anchors_coordinates(anchor)).astype(int)\n                for anchor in self.triggering_anchors\n            ]\n        )\n\n        is_in_zone: npt.NDArray[np.bool_] = (\n            self.mask[all_clipped_anchors[:, :, 1], all_clipped_anchors[:, :, 0]]\n            .transpose()\n            .astype(bool)\n        )\n\n        is_in_zone: npt.NDArray[np.bool_] = np.all(is_in_zone, axis=1)\n        self.current_count = int(np.sum(is_in_zone))\n        return is_in_zone.astype(bool)\n</code></pre> PolygonZoneAnnotator <p>A class for annotating a polygon-shaped zone within a     frame with a count of detected objects.</p> <p>Attributes:</p> Name Type Description <code>zone</code> <code>PolygonZone</code> <p>The polygon zone to be annotated</p> <code>color</code> <code>Color</code> <p>The color to draw the polygon lines</p> <code>thickness</code> <code>int</code> <p>The thickness of the polygon lines, default is 2</p> <code>text_color</code> <code>Color</code> <p>The color of the text on the polygon, default is black</p> <code>text_scale</code> <code>float</code> <p>The scale of the text on the polygon, default is 0.5</p> <code>text_thickness</code> <code>int</code> <p>The thickness of the text on the polygon, default is 1</p> <code>text_padding</code> <code>int</code> <p>The padding around the text on the polygon, default is 10</p> <code>font</code> <code>int</code> <p>The font type for the text on the polygon, default is cv2.FONT_HERSHEY_SIMPLEX</p> <code>center</code> <code>Tuple[int, int]</code> <p>The center of the polygon for text placement</p> <code>display_in_zone_count</code> <code>bool</code> <p>Show the label of the zone or not. Default is True</p> Source code in <code>supervision/detection/tools/polygon_zone.py</code> <pre><code>class PolygonZoneAnnotator:\n    \"\"\"\n    A class for annotating a polygon-shaped zone within a\n        frame with a count of detected objects.\n\n    Attributes:\n        zone (PolygonZone): The polygon zone to be annotated\n        color (Color): The color to draw the polygon lines\n        thickness (int): The thickness of the polygon lines, default is 2\n        text_color (Color): The color of the text on the polygon, default is black\n        text_scale (float): The scale of the text on the polygon, default is 0.5\n        text_thickness (int): The thickness of the text on the polygon, default is 1\n        text_padding (int): The padding around the text on the polygon, default is 10\n        font (int): The font type for the text on the polygon,\n            default is cv2.FONT_HERSHEY_SIMPLEX\n        center (Tuple[int, int]): The center of the polygon for text placement\n        display_in_zone_count (bool): Show the label of the zone or not. Default is True\n    \"\"\"\n\n    def __init__(\n        self,\n        zone: PolygonZone,\n        color: Color,\n        thickness: int = 2,\n        text_color: Color = Color.BLACK,\n        text_scale: float = 0.5,\n        text_thickness: int = 1,\n        text_padding: int = 10,\n        display_in_zone_count: bool = True,\n    ):\n        self.zone = zone\n        self.color = color\n        self.thickness = thickness\n        self.text_color = text_color\n        self.text_scale = text_scale\n        self.text_thickness = text_thickness\n        self.text_padding = text_padding\n        self.font = cv2.FONT_HERSHEY_SIMPLEX\n        self.center = get_polygon_center(polygon=zone.polygon)\n        self.display_in_zone_count = display_in_zone_count\n\n    def annotate(self, scene: np.ndarray, label: Optional[str] = None) -&gt; np.ndarray:\n        \"\"\"\n        Annotates the polygon zone within a frame with a count of detected objects.\n\n        Parameters:\n            scene (np.ndarray): The image on which the polygon zone will be annotated\n            label (Optional[str]): An optional label for the count of detected objects\n                within the polygon zone (default: None)\n\n        Returns:\n            np.ndarray: The image with the polygon zone and count of detected objects\n        \"\"\"\n        annotated_frame = draw_polygon(\n            scene=scene,\n            polygon=self.zone.polygon,\n            color=self.color,\n            thickness=self.thickness,\n        )\n\n        if self.display_in_zone_count:\n            annotated_frame = draw_text(\n                scene=annotated_frame,\n                text=str(self.zone.current_count) if label is None else label,\n                text_anchor=self.center,\n                background_color=self.color,\n                text_color=self.text_color,\n                text_scale=self.text_scale,\n                text_thickness=self.text_thickness,\n                text_padding=self.text_padding,\n                text_font=self.font,\n            )\n\n        return annotated_frame\n</code></pre>"},{"location":"detection/tools/polygon_zone/#supervision.detection.tools.polygon_zone.PolygonZone-functions","title":"Functions","text":""},{"location":"detection/tools/polygon_zone/#supervision.detection.tools.polygon_zone.PolygonZone.trigger","title":"<code>trigger(detections)</code>","text":"<p>Determines if the detections are within the polygon zone.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p>The detections to be checked against the polygon zone</p> required <p>Returns:</p> Type Description <code>NDArray[bool_]</code> <p>np.ndarray: A boolean numpy array indicating if each detection is within the polygon zone</p> Source code in <code>supervision/detection/tools/polygon_zone.py</code> <pre><code>def trigger(self, detections: Detections) -&gt; npt.NDArray[np.bool_]:\n    \"\"\"\n    Determines if the detections are within the polygon zone.\n\n    Parameters:\n        detections (Detections): The detections\n            to be checked against the polygon zone\n\n    Returns:\n        np.ndarray: A boolean numpy array indicating\n            if each detection is within the polygon zone\n    \"\"\"\n\n    clipped_xyxy = clip_boxes(\n        xyxy=detections.xyxy, resolution_wh=self.frame_resolution_wh\n    )\n    clipped_detections = replace(detections, xyxy=clipped_xyxy)\n    all_clipped_anchors = np.array(\n        [\n            np.ceil(clipped_detections.get_anchors_coordinates(anchor)).astype(int)\n            for anchor in self.triggering_anchors\n        ]\n    )\n\n    is_in_zone: npt.NDArray[np.bool_] = (\n        self.mask[all_clipped_anchors[:, :, 1], all_clipped_anchors[:, :, 0]]\n        .transpose()\n        .astype(bool)\n    )\n\n    is_in_zone: npt.NDArray[np.bool_] = np.all(is_in_zone, axis=1)\n    self.current_count = int(np.sum(is_in_zone))\n    return is_in_zone.astype(bool)\n</code></pre>"},{"location":"detection/tools/polygon_zone/#supervision.detection.tools.polygon_zone.PolygonZoneAnnotator-functions","title":"Functions","text":""},{"location":"detection/tools/polygon_zone/#supervision.detection.tools.polygon_zone.PolygonZoneAnnotator.annotate","title":"<code>annotate(scene, label=None)</code>","text":"<p>Annotates the polygon zone within a frame with a count of detected objects.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The image on which the polygon zone will be annotated</p> required <code>label</code> <code>Optional[str]</code> <p>An optional label for the count of detected objects within the polygon zone (default: None)</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The image with the polygon zone and count of detected objects</p> Source code in <code>supervision/detection/tools/polygon_zone.py</code> <pre><code>def annotate(self, scene: np.ndarray, label: Optional[str] = None) -&gt; np.ndarray:\n    \"\"\"\n    Annotates the polygon zone within a frame with a count of detected objects.\n\n    Parameters:\n        scene (np.ndarray): The image on which the polygon zone will be annotated\n        label (Optional[str]): An optional label for the count of detected objects\n            within the polygon zone (default: None)\n\n    Returns:\n        np.ndarray: The image with the polygon zone and count of detected objects\n    \"\"\"\n    annotated_frame = draw_polygon(\n        scene=scene,\n        polygon=self.zone.polygon,\n        color=self.color,\n        thickness=self.thickness,\n    )\n\n    if self.display_in_zone_count:\n        annotated_frame = draw_text(\n            scene=annotated_frame,\n            text=str(self.zone.current_count) if label is None else label,\n            text_anchor=self.center,\n            background_color=self.color,\n            text_color=self.text_color,\n            text_scale=self.text_scale,\n            text_thickness=self.text_thickness,\n            text_padding=self.text_padding,\n            text_font=self.font,\n        )\n\n    return annotated_frame\n</code></pre>"},{"location":"detection/tools/save_detections/","title":"Save Detections","text":"CSV Sink <p>A utility class for saving detection data to a CSV file. This class is designed to efficiently serialize detection objects into a CSV format, allowing for the inclusion of bounding box coordinates and additional attributes like <code>confidence</code>, <code>class_id</code>, and <code>tracker_id</code>.</p> <p>Tip</p> <p>CSVSink allow to pass custom data alongside the detection fields, providing flexibility for logging various types of information.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the CSV file where the detections will be stored. Defaults to 'output.csv'.</p> <code>'output.csv'</code> Example <pre><code>import supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(&lt;SOURCE_MODEL_PATH&gt;)\ncsv_sink = sv.CSVSink(&lt;RESULT_CSV_FILE_PATH&gt;)\nframes_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;)\n\nwith csv_sink as sink:\n    for frame in frames_generator:\n        result = model(frame)[0]\n        detections = sv.Detections.from_ultralytics(result)\n        sink.append(detections, custom_data={'&lt;CUSTOM_LABEL&gt;':'&lt;CUSTOM_DATA&gt;'})\n</code></pre> Source code in <code>supervision/detection/tools/csv_sink.py</code> <pre><code>class CSVSink:\n    \"\"\"\n    A utility class for saving detection data to a CSV file. This class is designed to\n    efficiently serialize detection objects into a CSV format, allowing for the\n    inclusion of bounding box coordinates and additional attributes like `confidence`,\n    `class_id`, and `tracker_id`.\n\n    !!! tip\n\n        CSVSink allow to pass custom data alongside the detection fields, providing\n        flexibility for logging various types of information.\n\n    Args:\n        file_name (str): The name of the CSV file where the detections will be stored.\n            Defaults to 'output.csv'.\n\n    Example:\n        ```python\n        import supervision as sv\n        from ultralytics import YOLO\n\n        model = YOLO(&lt;SOURCE_MODEL_PATH&gt;)\n        csv_sink = sv.CSVSink(&lt;RESULT_CSV_FILE_PATH&gt;)\n        frames_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;)\n\n        with csv_sink as sink:\n            for frame in frames_generator:\n                result = model(frame)[0]\n                detections = sv.Detections.from_ultralytics(result)\n                sink.append(detections, custom_data={'&lt;CUSTOM_LABEL&gt;':'&lt;CUSTOM_DATA&gt;'})\n        ```\n    \"\"\"  # noqa: E501 // docs\n\n    def __init__(self, file_name: str = \"output.csv\") -&gt; None:\n        \"\"\"\n        Initialize the CSVSink instance.\n\n        Args:\n            file_name (str): The name of the CSV file.\n\n        Returns:\n            None\n        \"\"\"\n        self.file_name = file_name\n        self.file: Optional[open] = None\n        self.writer: Optional[csv.writer] = None\n        self.header_written = False\n        self.field_names = []\n\n    def __enter__(self) -&gt; CSVSink:\n        self.open()\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[type],\n        exc_val: Optional[Exception],\n        exc_tb: Optional[Any],\n    ) -&gt; None:\n        self.close()\n\n    def open(self) -&gt; None:\n        \"\"\"\n        Open the CSV file for writing.\n\n        Returns:\n            None\n        \"\"\"\n        parent_directory = os.path.dirname(self.file_name)\n        if parent_directory and not os.path.exists(parent_directory):\n            os.makedirs(parent_directory)\n\n        self.file = open(self.file_name, \"w\", newline=\"\")\n        self.writer = csv.writer(self.file)\n\n    def close(self) -&gt; None:\n        \"\"\"\n        Close the CSV file.\n\n        Returns:\n            None\n        \"\"\"\n        if self.file:\n            self.file.close()\n\n    @staticmethod\n    def parse_detection_data(\n        detections: Detections, custom_data: Dict[str, Any] = None\n    ) -&gt; List[Dict[str, Any]]:\n        parsed_rows = []\n        for i in range(len(detections.xyxy)):\n            row = {\n                \"x_min\": detections.xyxy[i][0],\n                \"y_min\": detections.xyxy[i][1],\n                \"x_max\": detections.xyxy[i][2],\n                \"y_max\": detections.xyxy[i][3],\n                \"class_id\": \"\"\n                if detections.class_id is None\n                else str(detections.class_id[i]),\n                \"confidence\": \"\"\n                if detections.confidence is None\n                else str(detections.confidence[i]),\n                \"tracker_id\": \"\"\n                if detections.tracker_id is None\n                else str(detections.tracker_id[i]),\n            }\n\n            if hasattr(detections, \"data\"):\n                for key, value in detections.data.items():\n                    if value.ndim == 0:\n                        row[key] = value\n                    else:\n                        row[key] = value[i]\n\n            if custom_data:\n                row.update(custom_data)\n            parsed_rows.append(row)\n        return parsed_rows\n\n    def append(\n        self, detections: Detections, custom_data: Dict[str, Any] = None\n    ) -&gt; None:\n        \"\"\"\n        Append detection data to the CSV file.\n\n        Args:\n            detections (Detections): The detection data.\n            custom_data (Dict[str, Any]): Custom data to include.\n\n        Returns:\n            None\n        \"\"\"\n        if not self.writer:\n            raise Exception(\n                f\"Cannot append to CSV: The file '{self.file_name}' is not open.\"\n            )\n        field_names = CSVSink.parse_field_names(detections, custom_data)\n        if not self.header_written:\n            self.field_names = field_names\n            self.writer.writerow(field_names)\n            self.header_written = True\n\n        if field_names != self.field_names:\n            print(\n                f\"Field names do not match the header. \"\n                f\"Expected: {self.field_names}, given: {field_names}\"\n            )\n\n        parsed_rows = CSVSink.parse_detection_data(detections, custom_data)\n        for row in parsed_rows:\n            self.writer.writerow(\n                [row.get(field_name, \"\") for field_name in self.field_names]\n            )\n\n    @staticmethod\n    def parse_field_names(\n        detections: Detections, custom_data: Dict[str, Any]\n    ) -&gt; List[str]:\n        dynamic_header = sorted(\n            set(custom_data.keys()) | set(getattr(detections, \"data\", {}).keys())\n        )\n        return BASE_HEADER + dynamic_header\n</code></pre> JSON Sink <p>A utility class for saving detection data to a JSON file. This class is designed to efficiently serialize detection objects into a JSON format, allowing for the inclusion of bounding box coordinates and additional attributes like <code>confidence</code>, <code>class_id</code>, and <code>tracker_id</code>.</p> <p>Tip</p> <p>JSONsink allow to pass custom data alongside the detection fields, providing flexibility for logging various types of information.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the JSON file where the detections will be stored. Defaults to 'output.json'.</p> <code>'output.json'</code> Example <pre><code>import supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(&lt;SOURCE_MODEL_PATH&gt;)\njson_sink = sv.JSONSink(&lt;RESULT_JSON_FILE_PATH&gt;)\nframes_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;)\n\nwith json_sink as sink:\n    for frame in frames_generator:\n        result = model(frame)[0]\n        detections = sv.Detections.from_ultralytics(result)\n        sink.append(detections, custom_data={'&lt;CUSTOM_LABEL&gt;':'&lt;CUSTOM_DATA&gt;'})\n</code></pre> Source code in <code>supervision/detection/tools/json_sink.py</code> <pre><code>class JSONSink:\n    \"\"\"\n    A utility class for saving detection data to a JSON file. This class is designed to\n    efficiently serialize detection objects into a JSON format, allowing for the\n    inclusion of bounding box coordinates and additional attributes like `confidence`,\n    `class_id`, and `tracker_id`.\n\n    !!! tip\n\n        JSONsink allow to pass custom data alongside the detection fields, providing\n        flexibility for logging various types of information.\n\n    Args:\n        file_name (str): The name of the JSON file where the detections will be stored.\n            Defaults to 'output.json'.\n\n    Example:\n        ```python\n        import supervision as sv\n        from ultralytics import YOLO\n\n        model = YOLO(&lt;SOURCE_MODEL_PATH&gt;)\n        json_sink = sv.JSONSink(&lt;RESULT_JSON_FILE_PATH&gt;)\n        frames_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;)\n\n        with json_sink as sink:\n            for frame in frames_generator:\n                result = model(frame)[0]\n                detections = sv.Detections.from_ultralytics(result)\n                sink.append(detections, custom_data={'&lt;CUSTOM_LABEL&gt;':'&lt;CUSTOM_DATA&gt;'})\n        ```\n    \"\"\"  # noqa: E501 // docs\n\n    def __init__(self, file_name: str = \"output.json\") -&gt; None:\n        \"\"\"\n        Initialize the JSONSink instance.\n\n        Args:\n            file_name (str): The name of the JSON file.\n\n        Returns:\n            None\n        \"\"\"\n        self.file_name = file_name\n        self.file: Optional[open] = None\n        self.data: List[Dict[str, Any]] = []\n\n    def __enter__(self) -&gt; JSONSink:\n        self.open()\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[type],\n        exc_val: Optional[Exception],\n        exc_tb: Optional[Any],\n    ) -&gt; None:\n        self.write_and_close()\n\n    def open(self) -&gt; None:\n        \"\"\"\n        Open the JSON file for writing.\n\n        Returns:\n            None\n        \"\"\"\n        parent_directory = os.path.dirname(self.file_name)\n        if parent_directory and not os.path.exists(parent_directory):\n            os.makedirs(parent_directory)\n\n        self.file = open(self.file_name, \"w\")\n\n    def write_and_close(self) -&gt; None:\n        \"\"\"\n        Write and close the JSON file.\n\n        Returns:\n            None\n        \"\"\"\n        if self.file:\n            json.dump(self.data, self.file, indent=4)\n            self.file.close()\n\n    @staticmethod\n    def parse_detection_data(\n        detections: Detections, custom_data: Dict[str, Any] = None\n    ) -&gt; List[Dict[str, Any]]:\n        parsed_rows = []\n        for i in range(len(detections.xyxy)):\n            row = {\n                \"x_min\": float(detections.xyxy[i][0]),\n                \"y_min\": float(detections.xyxy[i][1]),\n                \"x_max\": float(detections.xyxy[i][2]),\n                \"y_max\": float(detections.xyxy[i][3]),\n                \"class_id\": \"\"\n                if detections.class_id is None\n                else int(detections.class_id[i]),\n                \"confidence\": \"\"\n                if detections.confidence is None\n                else float(detections.confidence[i]),\n                \"tracker_id\": \"\"\n                if detections.tracker_id is None\n                else int(detections.tracker_id[i]),\n            }\n\n            if hasattr(detections, \"data\"):\n                for key, value in detections.data.items():\n                    row[key] = (\n                        str(value[i])\n                        if hasattr(value, \"__getitem__\") and value.ndim != 0\n                        else str(value)\n                    )\n\n            if custom_data:\n                row.update(custom_data)\n            parsed_rows.append(row)\n        return parsed_rows\n\n    def append(\n        self, detections: Detections, custom_data: Dict[str, Any] = None\n    ) -&gt; None:\n        \"\"\"\n        Append detection data to the JSON file.\n\n        Args:\n            detections (Detections): The detection data.\n            custom_data (Dict[str, Any]): Custom data to include.\n\n        Returns:\n            None\n        \"\"\"\n        parsed_rows = JSONSink.parse_detection_data(detections, custom_data)\n        self.data.extend(parsed_rows)\n</code></pre>"},{"location":"detection/tools/save_detections/#supervision.detection.tools.csv_sink.CSVSink-functions","title":"Functions","text":""},{"location":"detection/tools/save_detections/#supervision.detection.tools.csv_sink.CSVSink.__init__","title":"<code>__init__(file_name='output.csv')</code>","text":"<p>Initialize the CSVSink instance.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the CSV file.</p> <code>'output.csv'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>supervision/detection/tools/csv_sink.py</code> <pre><code>def __init__(self, file_name: str = \"output.csv\") -&gt; None:\n    \"\"\"\n    Initialize the CSVSink instance.\n\n    Args:\n        file_name (str): The name of the CSV file.\n\n    Returns:\n        None\n    \"\"\"\n    self.file_name = file_name\n    self.file: Optional[open] = None\n    self.writer: Optional[csv.writer] = None\n    self.header_written = False\n    self.field_names = []\n</code></pre>"},{"location":"detection/tools/save_detections/#supervision.detection.tools.csv_sink.CSVSink.append","title":"<code>append(detections, custom_data=None)</code>","text":"<p>Append detection data to the CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p>The detection data.</p> required <code>custom_data</code> <code>Dict[str, Any]</code> <p>Custom data to include.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>supervision/detection/tools/csv_sink.py</code> <pre><code>def append(\n    self, detections: Detections, custom_data: Dict[str, Any] = None\n) -&gt; None:\n    \"\"\"\n    Append detection data to the CSV file.\n\n    Args:\n        detections (Detections): The detection data.\n        custom_data (Dict[str, Any]): Custom data to include.\n\n    Returns:\n        None\n    \"\"\"\n    if not self.writer:\n        raise Exception(\n            f\"Cannot append to CSV: The file '{self.file_name}' is not open.\"\n        )\n    field_names = CSVSink.parse_field_names(detections, custom_data)\n    if not self.header_written:\n        self.field_names = field_names\n        self.writer.writerow(field_names)\n        self.header_written = True\n\n    if field_names != self.field_names:\n        print(\n            f\"Field names do not match the header. \"\n            f\"Expected: {self.field_names}, given: {field_names}\"\n        )\n\n    parsed_rows = CSVSink.parse_detection_data(detections, custom_data)\n    for row in parsed_rows:\n        self.writer.writerow(\n            [row.get(field_name, \"\") for field_name in self.field_names]\n        )\n</code></pre>"},{"location":"detection/tools/save_detections/#supervision.detection.tools.csv_sink.CSVSink.close","title":"<code>close()</code>","text":"<p>Close the CSV file.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>supervision/detection/tools/csv_sink.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"\n    Close the CSV file.\n\n    Returns:\n        None\n    \"\"\"\n    if self.file:\n        self.file.close()\n</code></pre>"},{"location":"detection/tools/save_detections/#supervision.detection.tools.csv_sink.CSVSink.open","title":"<code>open()</code>","text":"<p>Open the CSV file for writing.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>supervision/detection/tools/csv_sink.py</code> <pre><code>def open(self) -&gt; None:\n    \"\"\"\n    Open the CSV file for writing.\n\n    Returns:\n        None\n    \"\"\"\n    parent_directory = os.path.dirname(self.file_name)\n    if parent_directory and not os.path.exists(parent_directory):\n        os.makedirs(parent_directory)\n\n    self.file = open(self.file_name, \"w\", newline=\"\")\n    self.writer = csv.writer(self.file)\n</code></pre>"},{"location":"detection/tools/save_detections/#supervision.detection.tools.json_sink.JSONSink-functions","title":"Functions","text":""},{"location":"detection/tools/save_detections/#supervision.detection.tools.json_sink.JSONSink.__init__","title":"<code>__init__(file_name='output.json')</code>","text":"<p>Initialize the JSONSink instance.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The name of the JSON file.</p> <code>'output.json'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>supervision/detection/tools/json_sink.py</code> <pre><code>def __init__(self, file_name: str = \"output.json\") -&gt; None:\n    \"\"\"\n    Initialize the JSONSink instance.\n\n    Args:\n        file_name (str): The name of the JSON file.\n\n    Returns:\n        None\n    \"\"\"\n    self.file_name = file_name\n    self.file: Optional[open] = None\n    self.data: List[Dict[str, Any]] = []\n</code></pre>"},{"location":"detection/tools/save_detections/#supervision.detection.tools.json_sink.JSONSink.append","title":"<code>append(detections, custom_data=None)</code>","text":"<p>Append detection data to the JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p>The detection data.</p> required <code>custom_data</code> <code>Dict[str, Any]</code> <p>Custom data to include.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>supervision/detection/tools/json_sink.py</code> <pre><code>def append(\n    self, detections: Detections, custom_data: Dict[str, Any] = None\n) -&gt; None:\n    \"\"\"\n    Append detection data to the JSON file.\n\n    Args:\n        detections (Detections): The detection data.\n        custom_data (Dict[str, Any]): Custom data to include.\n\n    Returns:\n        None\n    \"\"\"\n    parsed_rows = JSONSink.parse_detection_data(detections, custom_data)\n    self.data.extend(parsed_rows)\n</code></pre>"},{"location":"detection/tools/save_detections/#supervision.detection.tools.json_sink.JSONSink.open","title":"<code>open()</code>","text":"<p>Open the JSON file for writing.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>supervision/detection/tools/json_sink.py</code> <pre><code>def open(self) -&gt; None:\n    \"\"\"\n    Open the JSON file for writing.\n\n    Returns:\n        None\n    \"\"\"\n    parent_directory = os.path.dirname(self.file_name)\n    if parent_directory and not os.path.exists(parent_directory):\n        os.makedirs(parent_directory)\n\n    self.file = open(self.file_name, \"w\")\n</code></pre>"},{"location":"detection/tools/save_detections/#supervision.detection.tools.json_sink.JSONSink.write_and_close","title":"<code>write_and_close()</code>","text":"<p>Write and close the JSON file.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>supervision/detection/tools/json_sink.py</code> <pre><code>def write_and_close(self) -&gt; None:\n    \"\"\"\n    Write and close the JSON file.\n\n    Returns:\n        None\n    \"\"\"\n    if self.file:\n        json.dump(self.data, self.file, indent=4)\n        self.file.close()\n</code></pre>"},{"location":"detection/tools/smoother/","title":"Detection Smoother","text":"<p>A utility class for smoothing detections over multiple frames in video tracking. It maintains a history of detections for each track and provides smoothed predictions based on these histories.</p> <p>Warning</p> <ul> <li><code>DetectionsSmoother</code> requires the <code>tracker_id</code> for each detection. Refer to   Roboflow Trackers for   information on integrating tracking into your inference pipeline.</li> <li>This class is not compatible with segmentation models.</li> </ul> Example <pre><code>import supervision as sv\n\nfrom ultralytics import YOLO\n\nvideo_info = sv.VideoInfo.from_video_path(video_path=&lt;SOURCE_FILE_PATH&gt;)\nframe_generator = sv.get_video_frames_generator(source_path=&lt;SOURCE_FILE_PATH&gt;)\n\nmodel = YOLO(&lt;MODEL_PATH&gt;)\ntracker = sv.ByteTrack(frame_rate=video_info.fps)\nsmoother = sv.DetectionsSmoother()\n\nbox_annotator = sv.BoxAnnotator()\n\nwith sv.VideoSink(&lt;TARGET_FILE_PATH&gt;, video_info=video_info) as sink:\n    for frame in frame_generator:\n        result = model(frame)[0]\n        detections = sv.Detections.from_ultralytics(result)\n        detections = tracker.update_with_detections(detections)\n        detections = smoother.update_with_detections(detections)\n\n        annotated_frame = box_annotator.annotate(frame.copy(), detections)\n        sink.write_frame(annotated_frame)\n</code></pre> Source code in <code>supervision/detection/tools/smoother.py</code> <pre><code>class DetectionsSmoother:\n    \"\"\"\n    A utility class for smoothing detections over multiple frames in video tracking.\n    It maintains a history of detections for each track and provides smoothed\n    predictions based on these histories.\n\n    &lt;video controls&gt;\n        &lt;source\n            src=\"https://media.roboflow.com/supervision-detection-smoothing.mp4\"\n            type=\"video/mp4\"&gt;\n    &lt;/video&gt;\n\n    !!! warning\n\n        - `DetectionsSmoother` requires the `tracker_id` for each detection. Refer to\n          [Roboflow Trackers](/latest/trackers/) for\n          information on integrating tracking into your inference pipeline.\n        - This class is not compatible with segmentation models.\n\n    Example:\n        ```python\n        import supervision as sv\n\n        from ultralytics import YOLO\n\n        video_info = sv.VideoInfo.from_video_path(video_path=&lt;SOURCE_FILE_PATH&gt;)\n        frame_generator = sv.get_video_frames_generator(source_path=&lt;SOURCE_FILE_PATH&gt;)\n\n        model = YOLO(&lt;MODEL_PATH&gt;)\n        tracker = sv.ByteTrack(frame_rate=video_info.fps)\n        smoother = sv.DetectionsSmoother()\n\n        box_annotator = sv.BoxAnnotator()\n\n        with sv.VideoSink(&lt;TARGET_FILE_PATH&gt;, video_info=video_info) as sink:\n            for frame in frame_generator:\n                result = model(frame)[0]\n                detections = sv.Detections.from_ultralytics(result)\n                detections = tracker.update_with_detections(detections)\n                detections = smoother.update_with_detections(detections)\n\n                annotated_frame = box_annotator.annotate(frame.copy(), detections)\n                sink.write_frame(annotated_frame)\n        ```\n    \"\"\"  # noqa: E501 // docs\n\n    def __init__(self, length: int = 5) -&gt; None:\n        \"\"\"\n        Args:\n            length (int): The maximum number of frames to consider for smoothing\n                detections. Defaults to 5.\n        \"\"\"\n        self.tracks = defaultdict(lambda: deque(maxlen=length))\n\n    def update_with_detections(self, detections: Detections) -&gt; Detections:\n        \"\"\"\n        Updates the smoother with a new set of detections from a frame.\n\n        Args:\n            detections (Detections): The detections to add to the smoother.\n        \"\"\"\n\n        if detections.tracker_id is None:\n            warnings.warn(\n                \"Smoothing skipped. DetectionsSmoother requires tracker_id. Refer to \"\n                \"https://supervision.roboflow.com/latest/trackers for more \"\n                \"information.\",\n                category=SupervisionWarnings,\n            )\n            return detections\n\n        for detection_idx in range(len(detections)):\n            tracker_id = detections.tracker_id[detection_idx]\n\n            self.tracks[tracker_id].append(detections[detection_idx])\n\n        for track_id in self.tracks.keys():\n            if track_id not in detections.tracker_id:\n                self.tracks[track_id].append(None)\n\n        for track_id in list(self.tracks.keys()):\n            if all([d is None for d in self.tracks[track_id]]):\n                del self.tracks[track_id]\n\n        return self.get_smoothed_detections()\n\n    def get_track(self, track_id: int) -&gt; Optional[Detections]:\n        track = self.tracks.get(track_id, None)\n        if track is None:\n            return None\n\n        track = [d for d in track if d is not None]\n        if len(track) == 0:\n            return None\n\n        ret = deepcopy(track[0])\n        ret.xyxy = np.mean([d.xyxy for d in track], axis=0)\n        ret.confidence = np.mean([d.confidence for d in track], axis=0)\n\n        return ret\n\n    def get_smoothed_detections(self) -&gt; Detections:\n        tracked_detections = []\n        for track_id in self.tracks:\n            track = self.get_track(track_id)\n            if track is not None:\n                tracked_detections.append(track)\n\n        detections = Detections.merge(tracked_detections)\n        if len(detections) == 0:\n            detections.tracker_id = np.array([], dtype=int)\n\n        return detections\n</code></pre>"},{"location":"detection/tools/smoother/#supervision.detection.tools.smoother.DetectionsSmoother-functions","title":"Functions","text":""},{"location":"detection/tools/smoother/#supervision.detection.tools.smoother.DetectionsSmoother.__init__","title":"<code>__init__(length=5)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>length</code> <code>int</code> <p>The maximum number of frames to consider for smoothing detections. Defaults to 5.</p> <code>5</code> Source code in <code>supervision/detection/tools/smoother.py</code> <pre><code>def __init__(self, length: int = 5) -&gt; None:\n    \"\"\"\n    Args:\n        length (int): The maximum number of frames to consider for smoothing\n            detections. Defaults to 5.\n    \"\"\"\n    self.tracks = defaultdict(lambda: deque(maxlen=length))\n</code></pre>"},{"location":"detection/tools/smoother/#supervision.detection.tools.smoother.DetectionsSmoother.update_with_detections","title":"<code>update_with_detections(detections)</code>","text":"<p>Updates the smoother with a new set of detections from a frame.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p>The detections to add to the smoother.</p> required Source code in <code>supervision/detection/tools/smoother.py</code> <pre><code>def update_with_detections(self, detections: Detections) -&gt; Detections:\n    \"\"\"\n    Updates the smoother with a new set of detections from a frame.\n\n    Args:\n        detections (Detections): The detections to add to the smoother.\n    \"\"\"\n\n    if detections.tracker_id is None:\n        warnings.warn(\n            \"Smoothing skipped. DetectionsSmoother requires tracker_id. Refer to \"\n            \"https://supervision.roboflow.com/latest/trackers for more \"\n            \"information.\",\n            category=SupervisionWarnings,\n        )\n        return detections\n\n    for detection_idx in range(len(detections)):\n        tracker_id = detections.tracker_id[detection_idx]\n\n        self.tracks[tracker_id].append(detections[detection_idx])\n\n    for track_id in self.tracks.keys():\n        if track_id not in detections.tracker_id:\n            self.tracks[track_id].append(None)\n\n    for track_id in list(self.tracks.keys()):\n        if all([d is None for d in self.tracks[track_id]]):\n            del self.tracks[track_id]\n\n    return self.get_smoothed_detections()\n</code></pre>"},{"location":"how_to/detect_and_annotate/","title":"Detect and Annotate","text":"<p>Supervision provides a seamless process for annotating predictions generated by various object detection and segmentation models. This guide shows how to perform inference with the  Inference, Ultralytics or Transformers packages. Following this, you'll learn how to import these predictions into Supervision and use them to annotate source image.</p> <p></p>"},{"location":"how_to/detect_and_annotate/#run-detection","title":"Run Detection","text":"<p>First, you'll need to obtain predictions from your object detection or segmentation model.</p> InferenceUltralyticsTransformers <pre><code>import cv2\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8n-640\")\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nresults = model.infer(image)[0]\n</code></pre> <pre><code>import cv2\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nresults = model(image)[0]\n</code></pre> <pre><code>import torch\nfrom PIL import Image\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n\nimage = Image.open(&lt;SOURCE_IMAGE_PATH&gt;)\ninputs = processor(images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nwidth, height = image.size\ntarget_size = torch.tensor([[height, width]])\nresults = processor.post_process_object_detection(\n    outputs=outputs, target_sizes=target_size)[0]\n</code></pre>"},{"location":"how_to/detect_and_annotate/#load-predictions-into-supervision","title":"Load Predictions into Supervision","text":"<p>Now that we have predictions from a model, we can load them into Supervision.</p> InferenceUltralyticsTransformers <p>We can do so using the <code>sv.Detections.from_inference</code> method, which accepts model results from both detection and segmentation models.</p> <pre><code>import cv2\nimport supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8n-640\")\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nresults = model.infer(image)[0]\ndetections = sv.Detections.from_inference(results)\n</code></pre> <p>We can do so using the <code>sv.Detections.from_ultralytics</code> method, which accepts model results from both detection and segmentation models.</p> <pre><code>import cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nresults = model(image)[0]\ndetections = sv.Detections.from_ultralytics(results)\n</code></pre> <p>We can do so using the <code>sv.Detections.from_transformers</code> method, which accepts model results from both detection and segmentation models.</p> <pre><code>import torch\nimport supervision as sv\nfrom PIL import Image\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n\nimage = Image.open(&lt;SOURCE_IMAGE_PATH&gt;)\ninputs = processor(images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nwidth, height = image.size\ntarget_size = torch.tensor([[height, width]])\nresults = processor.post_process_object_detection(\n    outputs=outputs, target_sizes=target_size)[0]\ndetections = sv.Detections.from_transformers(\n    transformers_results=results,\n    id2label=model.config.id2label)\n</code></pre> <p>You can load predictions from other computer vision frameworks and libraries using:</p> <ul> <li><code>from_deepsparse</code> (Deepsparse)</li> <li><code>from_detectron2</code> (Detectron2)</li> <li><code>from_mmdetection</code> (MMDetection)</li> <li><code>from_sam</code> (Segment Anything Model)</li> <li><code>from_yolo_nas</code> (YOLO-NAS)</li> </ul>"},{"location":"how_to/detect_and_annotate/#annotate-image-with-detections","title":"Annotate Image with Detections","text":"<p>Finally, we can annotate the image with the predictions. Since we are working with an object detection model, we will use the <code>sv.BoxAnnotator</code> and <code>sv.LabelAnnotator</code> classes.</p> InferenceUltralyticsTransformers <pre><code>import cv2\nimport supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8n-640\")\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nresults = model.infer(image)[0]\ndetections = sv.Detections.from_inference(results)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nresults = model(image)[0]\ndetections = sv.Detections.from_ultralytics(results)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n</code></pre> <pre><code>import torch\nimport supervision as sv\nfrom PIL import Image\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n\nimage = Image.open(&lt;SOURCE_IMAGE_PATH&gt;)\ninputs = processor(images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nwidth, height = image.size\ntarget_size = torch.tensor([[height, width]])\nresults = processor.post_process_object_detection(\n    outputs=outputs, target_sizes=target_size)[0]\ndetections = sv.Detections.from_transformers(\n    transformers_results=results,\n    id2label=model.config.id2label)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n</code></pre> <p></p>"},{"location":"how_to/detect_and_annotate/#display-custom-labels","title":"Display Custom Labels","text":"<p>By default, <code>sv.LabelAnnotator</code> will label each detection with its <code>class_name</code> (if possible) or <code>class_id</code>. You can override this behavior by passing a list of custom <code>labels</code> to the <code>annotate</code> method.</p> InferenceUltralyticsTransformers <pre><code>import cv2\nimport supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8n-640\")\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nresults = model.infer(image)[0]\ndetections = sv.Detections.from_inference(results)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nlabels = [\n    f\"{class_name} {confidence:.2f}\"\n    for class_name, confidence\n    in zip(detections['class_name'], detections.confidence)\n]\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels)\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nresults = model(image)[0]\ndetections = sv.Detections.from_ultralytics(results)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nlabels = [\n    f\"{class_name} {confidence:.2f}\"\n    for class_name, confidence\n    in zip(detections['class_name'], detections.confidence)\n]\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels)\n</code></pre> <pre><code>import torch\nimport supervision as sv\nfrom PIL import Image\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n\nimage = Image.open(&lt;SOURCE_IMAGE_PATH&gt;)\ninputs = processor(images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nwidth, height = image.size\ntarget_size = torch.tensor([[height, width]])\nresults = processor.post_process_object_detection(\n    outputs=outputs, target_sizes=target_size)[0]\ndetections = sv.Detections.from_transformers(\n    transformers_results=results,\n    id2label=model.config.id2label)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nlabels = [\n    f\"{class_name} {confidence:.2f}\"\n    for class_name, confidence\n    in zip(detections['class_name'], detections.confidence)\n]\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels)\n</code></pre> <p></p>"},{"location":"how_to/detect_and_annotate/#annotate-image-with-segmentations","title":"Annotate Image with Segmentations","text":"<p>If you are running the segmentation model <code>sv.MaskAnnotator</code> is a drop-in replacement for <code>sv.BoxAnnotator</code> that will allow you to draw masks instead of boxes.</p> InferenceUltralyticsTransformers <pre><code>import cv2\nimport supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8n-seg-640\")\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nresults = model.infer(image)[0]\ndetections = sv.Detections.from_inference(results)\n\nmask_annotator = sv.MaskAnnotator()\nlabel_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER_OF_MASS)\n\nannotated_image = mask_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n-seg.pt\")\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nresults = model(image)[0]\ndetections = sv.Detections.from_ultralytics(results)\n\nmask_annotator = sv.MaskAnnotator()\nlabel_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER_OF_MASS)\n\nannotated_image = mask_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n</code></pre> <pre><code>import torch\nimport supervision as sv\nfrom PIL import Image\nfrom transformers import DetrImageProcessor, DetrForSegmentation\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\nmodel = DetrForSegmentation.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n\nimage = Image.open(&lt;SOURCE_IMAGE_PATH&gt;)\ninputs = processor(images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nwidth, height = image.size\ntarget_size = torch.tensor([[height, width]])\nresults = processor.post_process_segmentation(\n    outputs=outputs, target_sizes=target_size)[0]\ndetections = sv.Detections.from_transformers(\n    transformers_results=results,\n    id2label=model.config.id2label)\n\nmask_annotator = sv.MaskAnnotator()\nlabel_annotator = sv.LabelAnnotator(text_position=sv.Position.CENTER_OF_MASS)\n\nlabels = [\n    f\"{class_name} {confidence:.2f}\"\n    for class_name, confidence\n    in zip(detections['class_name'], detections.confidence)\n]\n\nannotated_image = mask_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels)\n</code></pre> <p></p>"},{"location":"how_to/detect_small_objects/","title":"Detect Small Objects","text":"<p>This guide shows how to detect small objects with the Inference, Ultralytics or Transformers packages using <code>InferenceSlicer</code>.</p>"},{"location":"how_to/detect_small_objects/#baseline-detection","title":"Baseline Detection","text":"<p>Small object detection in high-resolution images presents challenges due to the objects' size relative to the image resolution.</p> InferenceUltralyticsTransformers <pre><code>import cv2\nimport supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8x-640\")\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nresults = model.infer(image)[0]\ndetections = sv.Detections.from_inference(results)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8x.pt\")\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nresults = model(image)[0]\ndetections = sv.Detections.from_ultralytics(results)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n</code></pre> <pre><code>import torch\nimport supervision as sv\nfrom PIL import Image\nfrom transformers import DetrImageProcessor, DetrForSegmentation\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForSegmentation.from_pretrained(\"facebook/detr-resnet-50\")\n\nimage = Image.open(&lt;SOURCE_IMAGE_PATH&gt;)\ninputs = processor(images=image, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\nwidth, height = image_slice.size\ntarget_size = torch.tensor([[width, height]])\nresults = processor.post_process_object_detection(\n    outputs=outputs, target_sizes=target_size)[0]\ndetections = sv.Detections.from_transformers(results)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nlabels = [\n    model.config.id2label[class_id]\n    for class_id\n    in detections.class_id\n]\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels)\n</code></pre> <p></p>"},{"location":"how_to/detect_small_objects/#input-resolution","title":"Input Resolution","text":"<p>Modifying the input resolution of images before detection can enhance small object identification at the cost of processing speed and increased memory usage. This method is less effective for ultra-high-resolution images (4K and above).</p> InferenceUltralytics <pre><code>import cv2\nimport supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8x-1280\")\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nresults = model.infer(image)[0]\ndetections = sv.Detections.from_inference(results)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8x.pt\")\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nresults = model(image, imgsz=1280)[0]\ndetections = sv.Detections.from_ultralytics(results)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n</code></pre> <p></p>"},{"location":"how_to/detect_small_objects/#inference-slicer","title":"Inference Slicer","text":"<p><code>InferenceSlicer</code> processes high-resolution images by dividing them into smaller segments, detecting objects within each, and aggregating the results.</p> InferenceUltralyticsTransformers <pre><code>import cv2\nimport numpy as np\nimport supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8x-640\")\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n\ndef callback(image_slice: np.ndarray) -&gt; sv.Detections:\n    results = model.infer(image_slice)[0]\n    return sv.Detections.from_inference(results)\n\nslicer = sv.InferenceSlicer(callback = callback)\ndetections = slicer(image)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n</code></pre> <pre><code>import cv2\nimport numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8x.pt\")\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n\ndef callback(image_slice: np.ndarray) -&gt; sv.Detections:\n    result = model(image_slice)[0]\n    return sv.Detections.from_ultralytics(result)\n\nslicer = sv.InferenceSlicer(callback = callback)\ndetections = slicer(image)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n</code></pre> <pre><code>import cv2\nimport torch\nimport numpy as np\nimport supervision as sv\nfrom PIL import Image\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n\ndef callback(image_slice: np.ndarray) -&gt; sv.Detections:\n    image_slice = cv2.cvtColor(image_slice, cv2.COLOR_BGR2RGB)\n    image_slice = Image.fromarray(image_slice)\n    inputs = processor(images=image_slice, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    width, height = image_slice.size\n    target_size = torch.tensor([[width, height]])\n    results = processor.post_process_object_detection(\n        outputs=outputs, target_sizes=target_size)[0]\n    return sv.Detections.from_transformers(results)\n\nslicer = sv.InferenceSlicer(callback = callback)\ndetections = slicer(image)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nlabels = [\n    model.config.id2label[class_id]\n    for class_id\n    in detections.class_id\n]\n\nannotated_image = box_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections, labels=labels)\n</code></pre> <p></p>"},{"location":"how_to/detect_small_objects/#small-object-segmentation","title":"Small Object Segmentation","text":"<p><code>InferenceSlicer</code> can perform segmentation tasks too.</p> InferenceUltralytics <pre><code>import cv2\nimport numpy as np\nimport supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8x-seg-640\")\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n\ndef callback(image_slice: np.ndarray) -&gt; sv.Detections:\n    results = model.infer(image_slice)[0]\n    return sv.Detections.from_inference(results)\n\nslicer = sv.InferenceSlicer(callback = callback)\ndetections = slicer(image)\n\nmask_annotator = sv.MaskAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = mask_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n</code></pre> <pre><code>import cv2\nimport numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8x-seg.pt\")\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n\ndef callback(image_slice: np.ndarray) -&gt; sv.Detections:\n    result = model(image_slice)[0]\n    return sv.Detections.from_ultralytics(result)\n\nslicer = sv.InferenceSlicer(callback = callback)\ndetections = slicer(image)\n\nmask_annotator = sv.MaskAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = mask_annotator.annotate(\n    scene=image, detections=detections)\nannotated_image = label_annotator.annotate(\n    scene=annotated_image, detections=detections)\n</code></pre> <p></p>"},{"location":"how_to/filter_detections/","title":"Filter Detections","text":"<p>The advanced filtering capabilities of the <code>Detections</code> class offer users a versatile and efficient way to narrow down and refine object detections. This section outlines various filtering methods, including filtering by specific class or a set of classes, confidence, object area, bounding box area, relative area, box dimensions, and designated zones. Each method is demonstrated with concise code examples to provide users with a clear understanding of how to implement the filters in their applications.</p>"},{"location":"how_to/filter_detections/#by-specific-class","title":"by specific class","text":"<p>Allows you to select detections that belong only to one selected class.</p> AfterBefore <pre><code>import supervision as sv\n\ndetections = sv.Detections(...)\ndetections = detections[detections.class_id == 0]\n</code></pre> <p></p> <pre><code>import supervision as sv\n\ndetections = sv.Detections(...)\ndetections = detections[detections.class_id == 0]\n</code></pre> <p></p>"},{"location":"how_to/filter_detections/#by-set-of-classes","title":"by set of classes","text":"<p>Allows you to select detections that belong only to selected set of classes.</p> AfterBefore <pre><code>import numpy as np\nimport supervision as sv\n\nselected_classes = [0, 2, 3]\ndetections = sv.Detections(...)\ndetections = detections[np.isin(detections.class_id, selected_classes)]\n</code></pre> <p></p> <pre><code>import numpy as np\nimport supervision as sv\n\nclass_id = [0, 2, 3]\ndetections = sv.Detections(...)\ndetections = detections[np.isin(detections.class_id, class_id)]\n</code></pre> <p></p>"},{"location":"how_to/filter_detections/#by-confidence","title":"by confidence","text":"<p>Allows you to select detections with specific confidence value, for example higher than selected threshold.</p> AfterBefore <pre><code>import supervision as sv\n\ndetections = sv.Detections(...)\ndetections = detections[detections.confidence &gt; 0.5]\n</code></pre> <p></p> <pre><code>import supervision as sv\n\ndetections = sv.Detections(...)\ndetections = detections[detections.confidence &gt; 0.5]\n</code></pre> <p></p>"},{"location":"how_to/filter_detections/#by-area","title":"by area","text":"<p>Allows you to select detections based on their size. We define the area as the number of pixels occupied by the detection in the image. In the example below, we have sifted out the detections that are too small.</p> AfterBefore <pre><code>import supervision as sv\n\ndetections = sv.Detections(...)\ndetections = detections[detections.area &gt; 1000]\n</code></pre> <p></p> <pre><code>import supervision as sv\n\ndetections = sv.Detections(...)\ndetections = detections[detections.area &gt; 1000]\n</code></pre> <p></p>"},{"location":"how_to/filter_detections/#by-relative-area","title":"by relative area","text":"<p>Allows you to select detections based on their size in relation to the size of whole image. Sometimes the concept of detection size changes depending on the image. Detection occupying 10000 square px can be large on a 1280x720 image but small on a 3840x2160 image. In such cases, we can filter out detections based on the percentage of the image area occupied by them. In the example below, we remove too large detections.</p> AfterBefore <pre><code>import supervision as sv\n\nimage = ...\nheight, width, channels = image.shape\nimage_area = height * width\n\ndetections = sv.Detections(...)\ndetections = detections[(detections.area / image_area) &lt; 0.8]\n</code></pre> <p></p> <pre><code>import supervision as sv\n\nimage = ...\nheight, width, channels = image.shape\nimage_area = height * width\n\ndetections = sv.Detections(...)\ndetections = detections[(detections.area / image_area) &lt; 0.8]\n</code></pre> <p></p>"},{"location":"how_to/filter_detections/#by-box-dimensions","title":"by box dimensions","text":"<p>Allows you to select detections based on their dimensions. The size of the bounding box, as well as its coordinates, can be criteria for rejecting detection. Implementing such filtering requires a bit of custom code but is relatively simple and fast.</p> AfterBefore <pre><code>import supervision as sv\n\ndetections = sv.Detections(...)\nw = detections.xyxy[:, 2] - detections.xyxy[:, 0]\nh = detections.xyxy[:, 3] - detections.xyxy[:, 1]\ndetections = detections[(w &gt; 200) &amp; (h &gt; 200)]\n</code></pre> <p></p> <pre><code>import supervision as sv\n\ndetections = sv.Detections(...)\nw = detections.xyxy[:, 2] - detections.xyxy[:, 0]\nh = detections.xyxy[:, 3] - detections.xyxy[:, 1]\ndetections = detections[(w &gt; 200) &amp; (h &gt; 200)]\n</code></pre> <p></p>"},{"location":"how_to/filter_detections/#by-polygonzone","title":"by <code>PolygonZone</code>","text":"<p>Allows you to use <code>Detections</code> in combination with <code>PolygonZone</code> to weed out bounding boxes that are in and out of the zone. In the example below you can see how to filter out all detections located in the lower part of the image.</p> AfterBefore <pre><code>import supervision as sv\n\nzone = sv.PolygonZone(...)\ndetections = sv.Detections(...)\nmask = zone.trigger(detections=detections)\ndetections = detections[mask]\n</code></pre> <p></p> <pre><code>import supervision as sv\n\nzone = sv.PolygonZone(...)\ndetections = sv.Detections(...)\nmask = zone.trigger(detections=detections)\ndetections = detections[mask]\n</code></pre> <p></p>"},{"location":"how_to/filter_detections/#by-mixed-conditions","title":"by mixed conditions","text":"<p><code>Detections</code>' greatest strength, however, is that you can build arbitrarily complex logical conditions by simply combining separate conditions using <code>&amp;</code> or <code>|</code>.</p> AfterBefore <pre><code>import supervision as sv\n\nzone = sv.PolygonZone(...)\ndetections = sv.Detections(...)\nmask = zone.trigger(detections=detections)\ndetections = detections[(detections.confidence &gt; 0.7) &amp; mask]\n</code></pre> <p></p> <pre><code>import supervision as sv\n\nzone = sv.PolygonZone(...)\ndetections = sv.Detections(...)\nmask = zone.trigger(detections=detections)\ndetections = detections[mask]\n</code></pre> <p></p>"},{"location":"how_to/process_datasets/","title":"Process Datasets","text":"<p>With Supervision, you can load and manipulate classification, object detection, and segmentation datasets. This tutorial will walk you through how to load, split, merge, visualize, and augment datasets in Supervision.</p>"},{"location":"how_to/process_datasets/#download-dataset","title":"Download Dataset","text":"<p>In this tutorial, we will use a dataset from Roboflow Universe, a public repository of thousands of computer vision datasets. If you already have your dataset in COCO, YOLO, or Pascal VOC format, you can skip this section.</p> <pre><code>pip install roboflow\n</code></pre> <p>Next, log into your Roboflow account and download the dataset of your choice in the COCO, YOLO, or Pascal VOC format. You can customize the following code snippet with your workspace ID, project ID, and version number.</p> COCOYOLOPascal VOC <pre><code>import roboflow\n\nroboflow.login()\n\nrf = roboflow.Roboflow()\nproject = rf.workspace('&lt;WORKSPACE_ID&gt;').project('&lt;PROJECT_ID&gt;')\ndataset = project.version('&lt;PROJECT_VERSION&gt;').download(\"coco\")\n</code></pre> <pre><code>import roboflow\n\nroboflow.login()\n\nrf = roboflow.Roboflow()\nproject = rf.workspace('&lt;WORKSPACE_ID&gt;').project('&lt;PROJECT_ID&gt;')\ndataset = project.version('&lt;PROJECT_VERSION&gt;').download(\"yolov8\")\n</code></pre> <pre><code>import roboflow\n\nroboflow.login()\n\nrf = roboflow.Roboflow()\nproject = rf.workspace('&lt;WORKSPACE_ID&gt;').project('&lt;PROJECT_ID&gt;')\ndataset = project.version('&lt;PROJECT_VERSION&gt;').download(\"voc\")\n</code></pre>"},{"location":"how_to/process_datasets/#load-dataset","title":"Load Dataset","text":"<p>The Supervision library provides convenient functions to load datasets in various formats. If your dataset is already split into train, test, and valid subsets, you can load each of those as separate <code>sv.DetectionDataset</code> instances.</p> COCOYOLOPascal VOC <p>We can do so using the <code>sv.DetectionDataset.from_coco</code> to load annotations in COCO format.</p> <pre><code>import supervision as sv\n\nds_train = sv.DetectionDataset.from_coco(\n    images_directory_path=f'{dataset.location}/train',\n    annotations_path=f'{dataset.location}/train/_annotations.coco.json',\n)\nds_valid = sv.DetectionDataset.from_coco(\n    images_directory_path=f'{dataset.location}/valid',\n    annotations_path=f'{dataset.location}/valid/_annotations.coco.json',\n)\nds_test = sv.DetectionDataset.from_coco(\n    images_directory_path=f'{dataset.location}/test',\n    annotations_path=f'{dataset.location}/test/_annotations.coco.json',\n)\n\nds_train.classes\n# ['person', 'bicycle', 'car', ...]\n\nlen(ds_train), len(ds_valid), len(ds_test)\n# 800, 100, 100\n</code></pre> <p>We can do so using the <code>sv.DetectionDataset.from_yolo</code> to load annotations in YOLO format.</p> <pre><code>import supervision as sv\n\nds_train = sv.DetectionDataset.from_yolo(\n    images_directory_path=f'{dataset.location}/train/images',\n    annotations_directory_path=f'{dataset.location}/train/labels',\n    data_yaml_path=f'{dataset.location}/data.yaml'\n)\nds_valid = sv.DetectionDataset.from_yolo(\n    images_directory_path=f'{dataset.location}/valid/images',\n    annotations_directory_path=f'{dataset.location}/valid/labels',\n    data_yaml_path=f'{dataset.location}/data.yaml'\n)\nds_test = sv.DetectionDataset.from_yolo(\n    images_directory_path=f'{dataset.location}/test/images',\n    annotations_directory_path=f'{dataset.location}/test/labels',\n    data_yaml_path=f'{dataset.location}/data.yaml'\n)\n\nds_train.classes\n# ['person', 'bicycle', 'car', ...]\n\nlen(ds_train), len(ds_valid), len(ds_test)\n# 800, 100, 100\n</code></pre> <p>We can do so using the <code>sv.DetectionDataset.from_pascal_voc</code> to load annotations in Pascal VOC format.</p> <pre><code>import supervision as sv\n\nds_train = sv.DetectionDataset.from_pascal_voc(\n    images_directory_path=f'{dataset.location}/train/images',\n    annotations_directory_path=f'{dataset.location}/train/labels'\n)\nds_valid = sv.DetectionDataset.from_pascal_voc(\n    images_directory_path=f'{dataset.location}/valid/images',\n    annotations_directory_path=f'{dataset.location}/valid/labels'\n)\nds_test = sv.DetectionDataset.from_pascal_voc(\n    images_directory_path=f'{dataset.location}/test/images',\n    annotations_directory_path=f'{dataset.location}/test/labels'\n)\n\nds_train.classes\n# ['person', 'bicycle', 'car', ...]\n\nlen(ds_train), len(ds_valid), len(ds_test)\n# 800, 100, 100\n</code></pre>"},{"location":"how_to/process_datasets/#split-dataset","title":"Split Dataset","text":"<p>If your dataset is not already split into train, test, and valid subsets, you can easily do so using the <code>sv.DetectionDataset.split</code> method. We can split it as follows, ensuring a random shuffle of the data.</p> <pre><code>import supervision as sv\n\nds = sv.DetectionDataset(...)\n\nlen(ds)\n# 1000\n\nds_train, ds = ds.split(split_ratio=0.8, shuffle=True)\nds_valid, ds_test = ds.split(split_ratio=0.5, shuffle=True)\n\nlen(ds_train), len(ds_valid), len(ds_test)\n# 800, 100, 100\n</code></pre>"},{"location":"how_to/process_datasets/#merge-dataset","title":"Merge Dataset","text":"<p>If you have multiple datasets that you would like to merge, you can do so using the <code>sv.DetectionDataset.merge</code> method.</p> COCOYOLOPascal VOC <pre><code>import supervision as sv\n\nds_train = sv.DetectionDataset.from_coco(\n    images_directory_path=f'{dataset.location}/train',\n    annotations_path=f'{dataset.location}/train/_annotations.coco.json',\n)\nds_valid = sv.DetectionDataset.from_coco(\n    images_directory_path=f'{dataset.location}/valid',\n    annotations_path=f'{dataset.location}/valid/_annotations.coco.json',\n)\nds_test = sv.DetectionDataset.from_coco(\n    images_directory_path=f'{dataset.location}/test',\n    annotations_path=f'{dataset.location}/test/_annotations.coco.json',\n)\n\nds_train.classes\n# ['person', 'bicycle', 'car', ...]\n\nlen(ds_train), len(ds_valid), len(ds_test)\n# 800, 100, 100\n\nds = sv.DetectionDataset.merge([ds_train, ds_valid, ds_test])\n\nds.classes\n# ['person', 'bicycle', 'car', ...]\n\nlen(ds)\n# 1000\n</code></pre> <pre><code>import supervision as sv\n\nds_train = sv.DetectionDataset.from_yolo(\n    images_directory_path=f'{dataset.location}/train/images',\n    annotations_directory_path=f'{dataset.location}/train/labels',\n    data_yaml_path=f'{dataset.location}/data.yaml'\n)\nds_valid = sv.DetectionDataset.from_yolo(\n    images_directory_path=f'{dataset.location}/valid/images',\n    annotations_directory_path=f'{dataset.location}/valid/labels',\n    data_yaml_path=f'{dataset.location}/data.yaml'\n)\nds_test = sv.DetectionDataset.from_yolo(\n    images_directory_path=f'{dataset.location}/test/images',\n    annotations_directory_path=f'{dataset.location}/test/labels',\n    data_yaml_path=f'{dataset.location}/data.yaml'\n)\n\nds_train.classes\n# ['person', 'bicycle', 'car', ...]\n\nlen(ds_train), len(ds_valid), len(ds_test)\n# 800, 100, 100\n\nds = sv.DetectionDataset.merge([ds_train, ds_valid, ds_test])\n\nds.classes\n# ['person', 'bicycle', 'car', ...]\n\nlen(ds)\n# 1000\n</code></pre> <pre><code>import supervision as sv\n\nds_train = sv.DetectionDataset.from_pascal_voc(\n    images_directory_path=f'{dataset.location}/train/images',\n    annotations_directory_path=f'{dataset.location}/train/labels'\n)\nds_valid = sv.DetectionDataset.from_pascal_voc(\n    images_directory_path=f'{dataset.location}/valid/images',\n    annotations_directory_path=f'{dataset.location}/valid/labels'\n)\nds_test = sv.DetectionDataset.from_pascal_voc(\n    images_directory_path=f'{dataset.location}/test/images',\n    annotations_directory_path=f'{dataset.location}/test/labels'\n)\n\nds_train.classes\n# ['person', 'bicycle', 'car', ...]\n\nlen(ds_train), len(ds_valid), len(ds_test)\n# 800, 100, 100\n\nds = sv.DetectionDataset.merge([ds_train, ds_valid, ds_test])\n\nds.classes\n# ['person', 'bicycle', 'car', ...]\n\nlen(ds)\n# 1000\n</code></pre>"},{"location":"how_to/process_datasets/#iterate-over-dataset","title":"Iterate over Dataset","text":"<p>There are two ways to loop over a <code>sv.DetectionDataset</code>: using a direct for loop called on the <code>sv.DetectionDataset</code> instance or loading <code>sv.DetectionDataset</code> entries by index.</p> <pre><code>import supervision as sv\n\nds = sv.DetectionDataset(...)\n\n# Option 1\nfor image_path, image, annotations in ds:\n    ... # Process each image and its annotations\n\n# Option 2\nfor idx in range(len(ds)):\n    image_path, image, annotations = ds[idx]\n    ... # Process the image and annotations at index `idx`\n</code></pre>"},{"location":"how_to/process_datasets/#visualize-dataset","title":"Visualize Dataset","text":"<p>The Supervision library provides tools for easily visualizing your detection dataset. You can create a grid of annotated images to quickly inspect your data and labels. First, initialize the <code>sv.BoxAnnotator</code> and <code>sv.LabelAnnotator</code>. Then, iterate through a subset of the dataset (e.g., the first 25 images), drawing bounding boxes and class labels on each image. Finally, combine the annotated images into a grid for display.</p> <pre><code>import supervision as sv\n\nds = sv.DetectionDataset(...)\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_images = []\nfor i in range(16):\n    _, image, annotations = ds[i]\n\n    labels = [ds.classes[class_id] for class_id in annotations.class_id]\n\n    annotated_image = image.copy()\n    annotated_image = box_annotator.annotate(annotated_image, annotations)\n    annotated_image = label_annotator.annotate(annotated_image, annotations, labels)\n    annotated_images.append(annotated_image)\n\ngrid = sv.create_tiles(\n    annotated_images,\n    grid_size=(4, 4),\n    single_tile_size=(400, 400),\n    tile_padding_color=sv.Color.WHITE,\n    tile_margin_color=sv.Color.WHITE\n)\n</code></pre> <p></p>"},{"location":"how_to/process_datasets/#save-dataset","title":"Save Dataset","text":"COCOYOLOPascal VOC <p>We can do so using the <code>sv.DetectionDataset.as_coco</code> method to save annotations in COCO format.</p> <pre><code>import supervision as sv\n\nds = sv.DetectionDataset(...)\n\nds.as_coco(\n    images_directory_path='&lt;IMAGE_DIRECTORY_PATH&gt;',\n    annotations_path='&lt;ANNOTATIONS_PATH&gt;'\n)\n</code></pre> <p>We can do so using the <code>sv.DetectionDataset.as_yolo</code> method to save annotations in YOLO format.</p> <pre><code>import supervision as sv\n\nds = sv.DetectionDataset(...)\n\nds.as_yolo(\n    images_directory_path='&lt;IMAGE_DIRECTORY_PATH&gt;',\n    annotations_directory_path='&lt;ANNOTATIONS_DIRECTORY_PATH&gt;',\n    data_yaml_path='&lt;DATA_YAML_PATH&gt;'\n)\n</code></pre> <p>We can do so using the <code>sv.DetectionDataset.as_pascal_voc</code> method to save annotations in Pascal VOC format.</p> <pre><code>import supervision as sv\n\nds = sv.DetectionDataset(...)\n\nds.as_pascal_voc(\n    images_directory_path='&lt;IMAGE_DIRECTORY_PATH&gt;',\n    annotations_directory_path='&lt;ANNOTATIONS_DIRECTORY_PATH&gt;'\n)\n</code></pre>"},{"location":"how_to/process_datasets/#augment-dataset","title":"Augment Dataset","text":"<p>In this section, we'll explore using Supervision in combination with Albumentations to augment our dataset. Data augmentation is a common technique in computer vision to increase the size and diversity of training datasets, leading to improved model performance and generalization.</p> <pre><code>pip install augmentation\n</code></pre> <p>Albumentations provides a flexible and powerful API for image augmentation. The core of the library is the <code>Compose</code> class, which allows you to chain multiple image transformations together. Each transformation is defined using a dedicated class, such as <code>HorizontalFlip</code>, <code>RandomBrightnessContrast</code>, or <code>Perspective</code>.</p> <pre><code>import albumentations as A\n\naugmentation = A.Compose(\n    transforms=[\n        A.Perspective(p=0.1),\n        A.HorizontalFlip(p=0.5),\n        A.RandomBrightnessContrast(p=0.5)\n    ],\n    bbox_params=A.BboxParams(\n        format='pascal_voc',\n        label_fields=['category']\n    ),\n)\n</code></pre> <p>The key is to set <code>format='pascal_voc'</code>, which corresponds to the <code>[x_min, y_min, x_max, y_max]</code> bounding box format used in Supervision.</p> <pre><code>import numpy as np\nimport supervision as sv\nfrom dataclasses import replace\n\nds = sv.DetectionDataset(...)\n\n_, original_image, original_annotations = ds[0]\n\noutput = augmentation(\n    image=original_image,\n    bboxes=original_annotations.xyxy,\n    category=original_annotations.class_id\n)\n\naugmented_image = output['image']\naugmented_annotations = replace(\n    original_annotations,\n    xyxy=np.array(output['bboxes']),\n    class_id=np.array(output['category'])\n)\n</code></pre> <p></p>"},{"location":"how_to/save_detections/","title":"Save Detections","text":"<p>Supervision enables an easy way to save detections in .CSV and .JSON files for offline processing. This guide demonstrates how to perform video inference using the Inference, Ultralytics or Transformers packages and save their results with <code>sv.CSVSink</code> and <code>sv.JSONSink</code>.</p>"},{"location":"how_to/save_detections/#run-detection","title":"Run Detection","text":"<p>First, you'll need to obtain predictions from your object detection or segmentation model. You can learn more on this topic in our How to Detect and Annotate guide.</p> InferenceUltralyticsTransformers <pre><code>import supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8n-640\")\nframes_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;)\n\nfor frame in frames_generator:\n\n    results = model.infer(image)[0]\n    detections = sv.Detections.from_inference(results)\n</code></pre> <pre><code>import supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\nframes_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;)\n\nfor frame in frames_generator:\n\n    results = model(frame)[0]\n    detections = sv.Detections.from_ultralytics(results)\n</code></pre> <pre><code>import torch\nimport supervision as sv\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\nframes_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;)\n\nfor frame in frames_generator:\n\n    frame = sv.cv2_to_pillow(frame)\n    inputs = processor(images=frame, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    width, height = frame.size\n    target_size = torch.tensor([[height, width]])\n    results = processor.post_process_object_detection(\n        outputs=outputs, target_sizes=target_size)[0]\n    detections = sv.Detections.from_transformers(results)\n</code></pre>"},{"location":"how_to/save_detections/#save-detections-as-csv","title":"Save Detections as CSV","text":"<p>To save detections to a <code>.CSV</code> file, open our <code>sv.CSVSink</code> and then pass the <code>sv.Detections</code> object resulting from the inference to it. Its fields are parsed and saved on disk.</p> InferenceUltralyticsTransformers <pre><code>import supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8n-640\")\nframes_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;)\n\nwith sv.CSVSink(&lt;TARGET_CSV_PATH&gt;) as sink:\n    for frame in frames_generator:\n\n        results = model.infer(image)[0]\n        detections = sv.Detections.from_inference(results)\n        sink.append(detections, {})\n</code></pre> <pre><code>import supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\nframes_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;)\n\nwith sv.CSVSink(&lt;TARGET_CSV_PATH&gt;) as sink:\n    for frame in frames_generator:\n\n        results = model(frame)[0]\n        detections = sv.Detections.from_ultralytics(results)\n        sink.append(detections, {})\n</code></pre> <pre><code>import torch\nimport supervision as sv\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\nframes_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;)\n\nwith sv.CSVSink(&lt;TARGET_CSV_PATH&gt;) as sink:\n    for frame in frames_generator:\n\n        frame = sv.cv2_to_pillow(frame)\n        inputs = processor(images=frame, return_tensors=\"pt\")\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        width, height = frame.size\n        target_size = torch.tensor([[height, width]])\n        results = processor.post_process_object_detection(\n            outputs=outputs, target_sizes=target_size)[0]\n        detections = sv.Detections.from_transformers(results)\n        sink.append(detections, {})\n</code></pre> x_min y_min x_max y_max class_id confidence tracker_id class_name 2941.14 1269.31 3220.77 1500.67 2 0.8517 car 944.889 899.641 1235.42 1308.80 7 0.6752 truck 1439.78 1077.79 1621.27 1231.40 2 0.6450 car"},{"location":"how_to/save_detections/#custom-fields","title":"Custom Fields","text":"<p>Besides regular fields in <code>sv.Detections</code>, <code>sv.CSVSink</code> also allows you to add custom information to each row, which can be passed via the <code>custom_data</code> dictionary. Let's utilize this feature to save information about the frame index from which the detections originate.</p> InferenceUltralyticsTransformers <pre><code>import supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8n-640\")\nframes_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;)\n\nwith sv.CSVSink(&lt;TARGET_CSV_PATH&gt;) as sink:\n    for frame_index, frame in enumerate(frames_generator):\n\n        results = model.infer(image)[0]\n        detections = sv.Detections.from_inference(results)\n        sink.append(detections, {\"frame_index\": frame_index})\n</code></pre> <pre><code>import supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\nframes_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;)\n\nwith sv.CSVSink(&lt;TARGET_CSV_PATH&gt;) as sink:\n    for frame_index, frame in enumerate(frames_generator):\n\n        results = model(frame)[0]\n        detections = sv.Detections.from_ultralytics(results)\n        sink.append(detections, {\"frame_index\": frame_index})\n</code></pre> <pre><code>import torch\nimport supervision as sv\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\nframes_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;)\n\nwith sv.CSVSink(&lt;TARGET_CSV_PATH&gt;) as sink:\n    for frame_index, frame in enumerate(frames_generator):\n\n        frame = sv.cv2_to_pillow(frame)\n        inputs = processor(images=frame, return_tensors=\"pt\")\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        width, height = frame.size\n        target_size = torch.tensor([[height, width]])\n        results = processor.post_process_object_detection(\n            outputs=outputs, target_sizes=target_size)[0]\n        detections = sv.Detections.from_transformers(results)\n        sink.append(detections, {\"frame_index\": frame_index})\n</code></pre> x_min y_min x_max y_max class_id confidence tracker_id class_name frame_index 2941.14 1269.31 3220.77 1500.67 2 0.8517 car 0 944.889 899.641 1235.42 1308.80 7 0.6752 truck 0 1439.78 1077.79 1621.27 1231.40 2 0.6450 car 0"},{"location":"how_to/save_detections/#save-detections-as-json","title":"Save Detections as JSON","text":"<p>If you prefer to save the result in a <code>.JSON</code> file instead of a <code>.CSV</code> file, all you need to do is replace <code>sv.CSVSink</code> with <code>sv.JSONSink</code>.</p> InferenceUltralyticsTransformers <pre><code>import supervision as sv\nfrom inference import get_model\n\nmodel = get_model(model_id=\"yolov8n-640\")\nframes_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;)\n\nwith sv.JSONSink(&lt;TARGET_CSV_PATH&gt;) as sink:\n    for frame_index, frame in enumerate(frames_generator):\n\n        results = model.infer(image)[0]\n        detections = sv.Detections.from_inference(results)\n        sink.append(detections, {\"frame_index\": frame_index})\n</code></pre> <pre><code>import supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\nframes_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;)\n\nwith sv.JSONSink(&lt;TARGET_CSV_PATH&gt;) as sink:\n    for frame_index, frame in enumerate(frames_generator):\n\n        results = model(frame)[0]\n        detections = sv.Detections.from_ultralytics(results)\n        sink.append(detections, {\"frame_index\": frame_index})\n</code></pre> <pre><code>import torch\nimport supervision as sv\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\nframes_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;)\n\nwith sv.JSONSink(&lt;TARGET_CSV_PATH&gt;) as sink:\n    for frame_index, frame in enumerate(frames_generator):\n\n        frame = sv.cv2_to_pillow(frame)\n        inputs = processor(images=frame, return_tensors=\"pt\")\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n\n        width, height = frame.size\n        target_size = torch.tensor([[height, width]])\n        results = processor.post_process_object_detection(\n            outputs=outputs, target_sizes=target_size)[0]\n        detections = sv.Detections.from_transformers(results)\n        sink.append(detections, {\"frame_index\": frame_index})\n</code></pre>"},{"location":"how_to/track_objects/","title":"Track Objects","text":"<p>Leverage Supervision's advanced capabilities for enhancing your video analysis by seamlessly tracking objects recognized by a multitude of object detection and segmentation models. This comprehensive guide will take you through the steps to perform inference using the YOLOv8 model via either the Inference or Ultralytics packages. Following this, you'll discover how to track these objects efficiently and annotate your video content for a deeper analysis.</p> <p>To make it easier for you to follow our tutorial download the video we will use as an example. You can do this using <code>supervision[assets]</code> extension.</p> <pre><code>from supervision.assets import download_assets, VideoAssets\n\ndownload_assets(VideoAssets.PEOPLE_WALKING)\n</code></pre>"},{"location":"how_to/track_objects/#run-inference","title":"Run Inference","text":"<p>First, you'll need to obtain predictions from your object detection or segmentation model. In this tutorial, we are using the YOLOv8 model as an example. However, Supervision is versatile and compatible with various models. Check this link for guidance on how to plug in other models.</p> <p>We will define a <code>callback</code> function, which will process each frame of the video by obtaining model predictions and then annotating the frame based on these predictions. This <code>callback</code> function will be essential in the subsequent steps of the tutorial, as it will be modified to include tracking, labeling, and trace annotations.</p> UltralyticsInference <pre><code>import numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\nbox_annotator = sv.BoundingBoxAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -&gt; np.ndarray:\n    results = model(frame)[0]\n    detections = sv.Detections.from_ultralytics(results)\n    return box_annotator.annotate(frame.copy(), detections=detections)\n\nsv.process_video(\n    source_path=\"people-walking.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n</code></pre> <pre><code>import numpy as np\nimport supervision as sv\nfrom inference.models.utils import get_roboflow_model\n\nmodel = get_roboflow_model(model_id=\"yolov8n-640\", api_key=&lt;ROBOFLOW API KEY&gt;)\nbox_annotator = sv.BoundingBoxAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -&gt; np.ndarray:\n    results = model.infer(frame)[0]\n    detections = sv.Detections.from_inference(results)\n    return box_annotator.annotate(frame.copy(), detections=detections)\n\nsv.process_video(\n    source_path=\"people-walking.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n</code></pre>"},{"location":"how_to/track_objects/#tracking","title":"Tracking","text":"<p>After running inference and obtaining predictions, the next step is to track the detected objects throughout the video. Utilizing Supervision\u2019s <code>sv.ByteTrack</code> functionality, each detected object is assigned a unique tracker ID, enabling the continuous following of the object's motion path across different frames.</p> UltralyticsInference <pre><code>import numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\ntracker = sv.ByteTrack()\nbox_annotator = sv.BoundingBoxAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -&gt; np.ndarray:\n    results = model(frame)[0]\n    detections = sv.Detections.from_ultralytics(results)\n    detections = tracker.update_with_detections(detections)\n    return box_annotator.annotate(frame.copy(), detections=detections)\n\nsv.process_video(\n    source_path=\"people-walking.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n</code></pre> <pre><code>import numpy as np\nimport supervision as sv\nfrom inference.models.utils import get_roboflow_model\n\nmodel = get_roboflow_model(model_id=\"yolov8n-640\", api_key=&lt;ROBOFLOW API KEY&gt;)\ntracker = sv.ByteTrack()\nbox_annotator = sv.BoundingBoxAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -&gt; np.ndarray:\n    results = model.infer(frame)[0]\n    detections = sv.Detections.from_inference(results)\n    detections = tracker.update_with_detections(detections)\n    return box_annotator.annotate(frame.copy(), detections=detections)\n\nsv.process_video(\n    source_path=\"people-walking.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n</code></pre>"},{"location":"how_to/track_objects/#annotate-video-with-tracking-ids","title":"Annotate Video with Tracking IDs","text":"<p>Annotating the video with tracking IDs helps in distinguishing and following each object distinctly. With the <code>sv.LabelAnnotator</code> in Supervision, we can overlay the tracker IDs and class labels on the detected objects, offering a clear visual representation of each object's class and unique identifier.</p> UltralyticsInference <pre><code>import numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\ntracker = sv.ByteTrack()\nbox_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -&gt; np.ndarray:\n    results = model(frame)[0]\n    detections = sv.Detections.from_ultralytics(results)\n    detections = tracker.update_with_detections(detections)\n\n    labels = [\n        f\"#{tracker_id} {results.names[class_id]}\"\n        for class_id, tracker_id\n        in zip(detections.class_id, detections.tracker_id)\n    ]\n\n    annotated_frame = box_annotator.annotate(\n        frame.copy(), detections=detections)\n    return label_annotator.annotate(\n        annotated_frame, detections=detections, labels=labels)\n\nsv.process_video(\n    source_path=\"people-walking.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n</code></pre> <pre><code>import numpy as np\nimport supervision as sv\nfrom inference.models.utils import get_roboflow_model\n\nmodel = get_roboflow_model(model_id=\"yolov8n-640\", api_key=&lt;ROBOFLOW API KEY&gt;)\ntracker = sv.ByteTrack()\nbox_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -&gt; np.ndarray:\n    results = model.infer(frame)[0]\n    detections = sv.Detections.from_inference(results)\n    detections = tracker.update_with_detections(detections)\n\n    labels = [\n        f\"#{tracker_id} {results.names[class_id]}\"\n        for class_id, tracker_id\n        in zip(detections.class_id, detections.tracker_id)\n    ]\n\n    annotated_frame = box_annotator.annotate(\n        frame.copy(), detections=detections)\n    return label_annotator.annotate(\n        annotated_frame, detections=detections, labels=labels)\n\nsv.process_video(\n    source_path=\"people-walking.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n</code></pre>"},{"location":"how_to/track_objects/#annotate-video-with-traces","title":"Annotate Video with Traces","text":"<p>Adding traces to the video involves overlaying the historical paths of the detected objects. This feature, powered by the <code>sv.TraceAnnotator</code>, allows for visualizing the trajectories of objects, helping in understanding the movement patterns and interactions between objects in the video.</p> UltralyticsInference <pre><code>import numpy as np\nimport supervision as sv\nfrom ultralytics import YOLO\n\nmodel = YOLO(\"yolov8n.pt\")\ntracker = sv.ByteTrack()\nbox_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\ntrace_annotator = sv.TraceAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -&gt; np.ndarray:\n    results = model(frame)[0]\n    detections = sv.Detections.from_ultralytics(results)\n    detections = tracker.update_with_detections(detections)\n\n    labels = [\n        f\"#{tracker_id} {results.names[class_id]}\"\n        for class_id, tracker_id\n        in zip(detections.class_id, detections.tracker_id)\n    ]\n\n    annotated_frame = box_annotator.annotate(\n        frame.copy(), detections=detections)\n    annotated_frame = label_annotator.annotate(\n        annotated_frame, detections=detections, labels=labels)\n    return trace_annotator.annotate(\n        annotated_frame, detections=detections)\n\nsv.process_video(\n    source_path=\"people-walking.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n</code></pre> <pre><code>import numpy as np\nimport supervision as sv\nfrom inference.models.utils import get_roboflow_model\n\nmodel = get_roboflow_model(model_id=\"yolov8n-640\", api_key=&lt;ROBOFLOW API KEY&gt;)\ntracker = sv.ByteTrack()\nbox_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\ntrace_annotator = sv.TraceAnnotator()\n\ndef callback(frame: np.ndarray, _: int) -&gt; np.ndarray:\n    results = model.infer(frame)[0]\n    detections = sv.Detections.from_inference(results)\n    detections = tracker.update_with_detections(detections)\n\n    labels = [\n        f\"#{tracker_id} {results.names[class_id]}\"\n        for class_id, tracker_id\n        in zip(detections.class_id, detections.tracker_id)\n    ]\n\n    annotated_frame = box_annotator.annotate(\n        frame.copy(), detections=detections)\n    annotated_frame = label_annotator.annotate(\n        annotated_frame, detections=detections, labels=labels)\n    return trace_annotator.annotate(\n        annotated_frame, detections=detections)\n\nsv.process_video(\n    source_path=\"people-walking.mp4\",\n    target_path=\"result.mp4\",\n    callback=callback\n)\n</code></pre> <p>This structured walkthrough should give a detailed pathway to annotate videos effectively using Supervision\u2019s various functionalities, including object tracking and trace annotations.</p>"},{"location":"keypoint/annotators/","title":"Annotators","text":"VertexAnnotatorEdgeAnnotatorVertexLabelAnnotator <pre><code>import supervision as sv\n\nimage = ...\nkey_points = sv.KeyPoints(...)\n\nvertex_annotator = sv.VertexAnnotator(\n    color=sv.Color.GREEN,\n    radius=10\n)\nannotated_frame = vertex_annotator.annotate(\n    scene=image.copy(),\n    key_points=key_points\n)\n</code></pre> <pre><code>import supervision as sv\n\nimage = ...\nkey_points = sv.KeyPoints(...)\n\nedge_annotator = sv.EdgeAnnotator(\n    color=sv.Color.GREEN,\n    thickness=5\n)\nannotated_frame = edge_annotator.annotate(\n    scene=image.copy(),\n    key_points=key_points\n)\n</code></pre> <pre><code>import supervision as sv\n\nimage = ...\nkey_points = sv.KeyPoints(...)\n\nvertex_label_annotator = sv.VertexLabelAnnotator(\n    color=sv.Color.GREEN,\n    text_color=sv.Color.BLACK,\n    border_radius=5\n)\nannotated_frame = vertex_label_annotator.annotate(\n    scene=image.copy(),\n    key_points=key_points\n)\n</code></pre> VertexAnnotator <p>               Bases: <code>BaseKeyPointAnnotator</code></p> <p>A class that specializes in drawing skeleton vertices on images. It uses specified key points to determine the locations where the vertices should be drawn.</p> Source code in <code>supervision/keypoint/annotators.py</code> <pre><code>class VertexAnnotator(BaseKeyPointAnnotator):\n    \"\"\"\n    A class that specializes in drawing skeleton vertices on images. It uses\n    specified key points to determine the locations where the vertices should be\n    drawn.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Color = Color.ROBOFLOW,\n        radius: int = 4,\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            color (Color, optional): The color to use for annotating key points.\n            radius (int, optional): The radius of the circles used to represent the key\n                points.\n        \"\"\"\n        self.color = color\n        self.radius = radius\n\n    @ensure_cv2_image_for_annotation\n    def annotate(self, scene: ImageType, key_points: KeyPoints) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene with skeleton vertices based on the provided key\n        points. It draws circles at each key point location.\n\n        Args:\n            scene (ImageType): The image where skeleton vertices will be drawn.\n                `ImageType` is a flexible type, accepting either `numpy.ndarray` or\n                `PIL.Image.Image`.\n            key_points (KeyPoints): A collection of key points where each key point\n                consists of x and y coordinates.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            key_points = sv.KeyPoints(...)\n\n            vertex_annotator = sv.VertexAnnotator(\n                color=sv.Color.GREEN,\n                radius=10\n            )\n            annotated_frame = vertex_annotator.annotate(\n                scene=image.copy(),\n                key_points=key_points\n            )\n            ```\n\n        ![vertex-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/vertex-annotator-example.png)\n        \"\"\"\n        if len(key_points) == 0:\n            return scene\n\n        for xy in key_points.xy:\n            for x, y in xy:\n                cv2.circle(\n                    img=scene,\n                    center=(int(x), int(y)),\n                    radius=self.radius,\n                    color=self.color.as_bgr(),\n                    thickness=-1,\n                )\n\n        return scene\n</code></pre> EdgeAnnotator <p>               Bases: <code>BaseKeyPointAnnotator</code></p> <p>A class that specializes in drawing skeleton edges on images using specified key points. It connects key points with lines to form the skeleton structure.</p> Source code in <code>supervision/keypoint/annotators.py</code> <pre><code>class EdgeAnnotator(BaseKeyPointAnnotator):\n    \"\"\"\n    A class that specializes in drawing skeleton edges on images using specified key\n    points. It connects key points with lines to form the skeleton structure.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Color = Color.ROBOFLOW,\n        thickness: int = 2,\n        edges: Optional[List[Tuple[int, int]]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            color (Color, optional): The color to use for the edges.\n            thickness (int, optional): The thickness of the edges.\n            edges (Optional[List[Tuple[int, int]]]): The edges to draw.\n                If set to `None`, will attempt to select automatically.\n        \"\"\"\n        self.color = color\n        self.thickness = thickness\n        self.edges = edges\n\n    @ensure_cv2_image_for_annotation\n    def annotate(self, scene: ImageType, key_points: KeyPoints) -&gt; ImageType:\n        \"\"\"\n        Annotates the given scene by drawing lines between specified key points to form\n        edges.\n\n        Args:\n            scene (ImageType): The image where skeleton edges will be drawn. `ImageType`\n                is a flexible type, accepting either `numpy.ndarray` or\n                `PIL.Image.Image`.\n            key_points (KeyPoints): A collection of key points where each key point\n                consists of x and y coordinates.\n\n        Returns:\n            Returns:\n                The annotated image, matching the type of `scene` (`numpy.ndarray`\n                    or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            key_points = sv.KeyPoints(...)\n\n            edge_annotator = sv.EdgeAnnotator(\n                color=sv.Color.GREEN,\n                thickness=5\n            )\n            annotated_frame = edge_annotator.annotate(\n                scene=image.copy(),\n                key_points=key_points\n            )\n            ```\n\n        ![edge-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/edge-annotator-example.png)\n        \"\"\"\n        if len(key_points) == 0:\n            return scene\n\n        for xy in key_points.xy:\n            edges = self.edges\n            if not edges:\n                edges = SKELETONS_BY_VERTEX_COUNT.get(len(xy))\n            if not edges:\n                warn(f\"No skeleton found with {len(xy)} vertices\")\n                return scene\n\n            for class_a, class_b in edges:\n                xy_a = xy[class_a - 1]\n                xy_b = xy[class_b - 1]\n                missing_a = np.allclose(xy_a, 0)\n                missing_b = np.allclose(xy_b, 0)\n                if missing_a or missing_b:\n                    continue\n\n                cv2.line(\n                    img=scene,\n                    pt1=(int(xy_a[0]), int(xy_a[1])),\n                    pt2=(int(xy_b[0]), int(xy_b[1])),\n                    color=self.color.as_bgr(),\n                    thickness=self.thickness,\n                )\n\n        return scene\n</code></pre> VertexLabelAnnotator <p>A class that draws labels of skeleton vertices on images. It uses specified key points to determine the locations where the vertices should be drawn.</p> Source code in <code>supervision/keypoint/annotators.py</code> <pre><code>class VertexLabelAnnotator:\n    \"\"\"\n    A class that draws labels of skeleton vertices on images. It uses specified key\n    points to determine the locations where the vertices should be drawn.\n    \"\"\"\n\n    def __init__(\n        self,\n        color: Union[Color, List[Color]] = Color.ROBOFLOW,\n        text_color: Union[Color, List[Color]] = Color.WHITE,\n        text_scale: float = 0.5,\n        text_thickness: int = 1,\n        text_padding: int = 10,\n        border_radius: int = 0,\n    ):\n        \"\"\"\n        Args:\n            color (Union[Color, List[Color]], optional): The color to use for each\n                keypoint label. If a list is provided, the colors will be used in order\n                for each keypoint.\n            text_color (Union[Color, List[Color]], optional): The color to use\n                for the labels. If a list is provided, the colors will be used in order\n                for each keypoint.\n            text_scale (float, optional): The scale of the text.\n            text_thickness (int, optional): The thickness of the text.\n            text_padding (int, optional): The padding around the text.\n            border_radius (int, optional): The radius of the rounded corners of the\n                boxes. Set to a high value to produce circles.\n        \"\"\"\n        self.border_radius: int = border_radius\n        self.color: Union[Color, List[Color]] = color\n        self.text_color: Union[Color, List[Color]] = text_color\n        self.text_scale: float = text_scale\n        self.text_thickness: int = text_thickness\n        self.text_padding: int = text_padding\n\n    def annotate(\n        self, scene: ImageType, key_points: KeyPoints, labels: List[str] = None\n    ) -&gt; ImageType:\n        \"\"\"\n        A class that draws labels of skeleton vertices on images. It uses specified key\n            points to determine the locations where the vertices should be drawn.\n\n        Args:\n            scene (ImageType): The image where vertex labels will be drawn. `ImageType`\n                is a flexible type, accepting either `numpy.ndarray` or\n                `PIL.Image.Image`.\n            key_points (KeyPoints): A collection of key points where each key point\n                consists of x and y coordinates.\n            labels (List[str], optional): A list of labels to be displayed on the\n                annotated image. If not provided, keypoint indices will be used.\n\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            key_points = sv.KeyPoints(...)\n\n            vertex_label_annotator = sv.VertexLabelAnnotator(\n                color=sv.Color.GREEN,\n                text_color=sv.Color.BLACK,\n                border_radius=5\n            )\n            annotated_frame = vertex_label_annotator.annotate(\n                scene=image.copy(),\n                key_points=key_points\n            )\n            ```\n\n        ![vertex-label-annotator-example](https://media.roboflow.com/\n        supervision-annotator-examples/vertex-label-annotator-example.png)\n\n        !!! tip\n\n            `VertexLabelAnnotator` allows to customize the color of each keypoint label\n            values.\n\n        Example:\n            ```python\n            import supervision as sv\n\n            image = ...\n            key_points = sv.KeyPoints(...)\n\n            LABELS = [\n                \"nose\", \"left eye\", \"right eye\", \"left ear\",\n                \"right ear\", \"left shoulder\", \"right shoulder\", \"left elbow\",\n                \"right elbow\", \"left wrist\", \"right wrist\", \"left hip\",\n                \"right hip\", \"left knee\", \"right knee\", \"left ankle\",\n                \"right ankle\"\n            ]\n\n            COLORS = [\n                \"#FF6347\", \"#FF6347\", \"#FF6347\", \"#FF6347\",\n                \"#FF6347\", \"#FF1493\", \"#00FF00\", \"#FF1493\",\n                \"#00FF00\", \"#FF1493\", \"#00FF00\", \"#FFD700\",\n                \"#00BFFF\", \"#FFD700\", \"#00BFFF\", \"#FFD700\",\n                \"#00BFFF\"\n            ]\n            COLORS = [sv.Color.from_hex(color_hex=c) for c in COLORS]\n\n            vertex_label_annotator = sv.VertexLabelAnnotator(\n                color=COLORS,\n                text_color=sv.Color.BLACK,\n                border_radius=5\n            )\n            annotated_frame = vertex_label_annotator.annotate(\n                scene=image.copy(),\n                key_points=key_points,\n                labels=labels\n            )\n            ```\n        ![vertex-label-annotator-custom-example](https://media.roboflow.com/\n        supervision-annotator-examples/vertex-label-annotator-custom-example.png)\n        \"\"\"\n        font = cv2.FONT_HERSHEY_SIMPLEX\n\n        skeletons_count, points_count, _ = key_points.xy.shape\n        if skeletons_count == 0:\n            return scene\n\n        anchors = key_points.xy.reshape(points_count * skeletons_count, 2).astype(int)\n        mask = np.all(anchors != 0, axis=1)\n\n        if not np.any(mask):\n            return scene\n\n        colors = self.preprocess_and_validate_colors(\n            colors=self.color,\n            points_count=points_count,\n            skeletons_count=skeletons_count,\n        )\n\n        text_colors = self.preprocess_and_validate_colors(\n            colors=self.text_color,\n            points_count=points_count,\n            skeletons_count=skeletons_count,\n        )\n\n        labels = self.preprocess_and_validate_labels(\n            labels=labels, points_count=points_count, skeletons_count=skeletons_count\n        )\n\n        anchors = anchors[mask]\n        colors = colors[mask]\n        text_colors = text_colors[mask]\n        labels = labels[mask]\n\n        xyxy = np.array(\n            [\n                self.get_text_bounding_box(\n                    text=label,\n                    font=font,\n                    text_scale=self.text_scale,\n                    text_thickness=self.text_thickness,\n                    center_coordinates=tuple(anchor),\n                )\n                for anchor, label in zip(anchors, labels)\n            ]\n        )\n\n        xyxy_padded = pad_boxes(xyxy=xyxy, px=self.text_padding)\n\n        for text, color, text_color, box, box_padded in zip(\n            labels, colors, text_colors, xyxy, xyxy_padded\n        ):\n            draw_rounded_rectangle(\n                scene=scene,\n                rect=Rect.from_xyxy(box_padded),\n                color=color,\n                border_radius=self.border_radius,\n            )\n            cv2.putText(\n                img=scene,\n                text=text,\n                org=(box[0], box[3]),\n                fontFace=font,\n                fontScale=self.text_scale,\n                color=text_color.as_bgr(),\n                thickness=self.text_thickness,\n                lineType=cv2.LINE_AA,\n            )\n\n        return scene\n\n    @staticmethod\n    def get_text_bounding_box(\n        text: str,\n        font: int,\n        text_scale: float,\n        text_thickness: int,\n        center_coordinates: Tuple[int, int],\n    ) -&gt; Tuple[int, int, int, int]:\n        text_w, text_h = cv2.getTextSize(\n            text=text,\n            fontFace=font,\n            fontScale=text_scale,\n            thickness=text_thickness,\n        )[0]\n        center_x, center_y = center_coordinates\n        return (\n            center_x - text_w // 2,\n            center_y - text_h // 2,\n            center_x + text_w // 2,\n            center_y + text_h // 2,\n        )\n\n    @staticmethod\n    def preprocess_and_validate_labels(\n        labels: Optional[List[str]], points_count: int, skeletons_count: int\n    ) -&gt; np.array:\n        if labels and len(labels) != points_count:\n            raise ValueError(\n                f\"Number of labels ({len(labels)}) must match number of key points \"\n                f\"({points_count}).\"\n            )\n        if labels is None:\n            labels = [str(i) for i in range(points_count)]\n\n        return np.array(labels * skeletons_count)\n\n    @staticmethod\n    def preprocess_and_validate_colors(\n        colors: Optional[Union[Color, List[Color]]],\n        points_count: int,\n        skeletons_count: int,\n    ) -&gt; np.array:\n        if isinstance(colors, list) and len(colors) != points_count:\n            raise ValueError(\n                f\"Number of colors ({len(colors)}) must match number of key points \"\n                f\"({points_count}).\"\n            )\n        return (\n            np.array(colors * skeletons_count)\n            if isinstance(colors, list)\n            else np.array([colors] * points_count * skeletons_count)\n        )\n</code></pre>"},{"location":"keypoint/annotators/#supervision.keypoint.annotators.VertexAnnotator-functions","title":"Functions","text":""},{"location":"keypoint/annotators/#supervision.keypoint.annotators.VertexAnnotator.__init__","title":"<code>__init__(color=Color.ROBOFLOW, radius=4)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Color</code> <p>The color to use for annotating key points.</p> <code>ROBOFLOW</code> <code>radius</code> <code>int</code> <p>The radius of the circles used to represent the key points.</p> <code>4</code> Source code in <code>supervision/keypoint/annotators.py</code> <pre><code>def __init__(\n    self,\n    color: Color = Color.ROBOFLOW,\n    radius: int = 4,\n) -&gt; None:\n    \"\"\"\n    Args:\n        color (Color, optional): The color to use for annotating key points.\n        radius (int, optional): The radius of the circles used to represent the key\n            points.\n    \"\"\"\n    self.color = color\n    self.radius = radius\n</code></pre>"},{"location":"keypoint/annotators/#supervision.keypoint.annotators.VertexAnnotator.annotate","title":"<code>annotate(scene, key_points)</code>","text":"<p>Annotates the given scene with skeleton vertices based on the provided key points. It draws circles at each key point location.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where skeleton vertices will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>key_points</code> <code>KeyPoints</code> <p>A collection of key points where each key point consists of x and y coordinates.</p> required <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\nkey_points = sv.KeyPoints(...)\n\nvertex_annotator = sv.VertexAnnotator(\n    color=sv.Color.GREEN,\n    radius=10\n)\nannotated_frame = vertex_annotator.annotate(\n    scene=image.copy(),\n    key_points=key_points\n)\n</code></pre> <p></p> Source code in <code>supervision/keypoint/annotators.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(self, scene: ImageType, key_points: KeyPoints) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene with skeleton vertices based on the provided key\n    points. It draws circles at each key point location.\n\n    Args:\n        scene (ImageType): The image where skeleton vertices will be drawn.\n            `ImageType` is a flexible type, accepting either `numpy.ndarray` or\n            `PIL.Image.Image`.\n        key_points (KeyPoints): A collection of key points where each key point\n            consists of x and y coordinates.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        key_points = sv.KeyPoints(...)\n\n        vertex_annotator = sv.VertexAnnotator(\n            color=sv.Color.GREEN,\n            radius=10\n        )\n        annotated_frame = vertex_annotator.annotate(\n            scene=image.copy(),\n            key_points=key_points\n        )\n        ```\n\n    ![vertex-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/vertex-annotator-example.png)\n    \"\"\"\n    if len(key_points) == 0:\n        return scene\n\n    for xy in key_points.xy:\n        for x, y in xy:\n            cv2.circle(\n                img=scene,\n                center=(int(x), int(y)),\n                radius=self.radius,\n                color=self.color.as_bgr(),\n                thickness=-1,\n            )\n\n    return scene\n</code></pre>"},{"location":"keypoint/annotators/#supervision.keypoint.annotators.EdgeAnnotator-functions","title":"Functions","text":""},{"location":"keypoint/annotators/#supervision.keypoint.annotators.EdgeAnnotator.__init__","title":"<code>__init__(color=Color.ROBOFLOW, thickness=2, edges=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Color</code> <p>The color to use for the edges.</p> <code>ROBOFLOW</code> <code>thickness</code> <code>int</code> <p>The thickness of the edges.</p> <code>2</code> <code>edges</code> <code>Optional[List[Tuple[int, int]]]</code> <p>The edges to draw. If set to <code>None</code>, will attempt to select automatically.</p> <code>None</code> Source code in <code>supervision/keypoint/annotators.py</code> <pre><code>def __init__(\n    self,\n    color: Color = Color.ROBOFLOW,\n    thickness: int = 2,\n    edges: Optional[List[Tuple[int, int]]] = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        color (Color, optional): The color to use for the edges.\n        thickness (int, optional): The thickness of the edges.\n        edges (Optional[List[Tuple[int, int]]]): The edges to draw.\n            If set to `None`, will attempt to select automatically.\n    \"\"\"\n    self.color = color\n    self.thickness = thickness\n    self.edges = edges\n</code></pre>"},{"location":"keypoint/annotators/#supervision.keypoint.annotators.EdgeAnnotator.annotate","title":"<code>annotate(scene, key_points)</code>","text":"<p>Annotates the given scene by drawing lines between specified key points to form edges.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where skeleton edges will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>key_points</code> <code>KeyPoints</code> <p>A collection of key points where each key point consists of x and y coordinates.</p> required <p>Returns:</p> Name Type Description <code>Returns</code> <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code>     or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\nkey_points = sv.KeyPoints(...)\n\nedge_annotator = sv.EdgeAnnotator(\n    color=sv.Color.GREEN,\n    thickness=5\n)\nannotated_frame = edge_annotator.annotate(\n    scene=image.copy(),\n    key_points=key_points\n)\n</code></pre> <p></p> Source code in <code>supervision/keypoint/annotators.py</code> <pre><code>@ensure_cv2_image_for_annotation\ndef annotate(self, scene: ImageType, key_points: KeyPoints) -&gt; ImageType:\n    \"\"\"\n    Annotates the given scene by drawing lines between specified key points to form\n    edges.\n\n    Args:\n        scene (ImageType): The image where skeleton edges will be drawn. `ImageType`\n            is a flexible type, accepting either `numpy.ndarray` or\n            `PIL.Image.Image`.\n        key_points (KeyPoints): A collection of key points where each key point\n            consists of x and y coordinates.\n\n    Returns:\n        Returns:\n            The annotated image, matching the type of `scene` (`numpy.ndarray`\n                or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        key_points = sv.KeyPoints(...)\n\n        edge_annotator = sv.EdgeAnnotator(\n            color=sv.Color.GREEN,\n            thickness=5\n        )\n        annotated_frame = edge_annotator.annotate(\n            scene=image.copy(),\n            key_points=key_points\n        )\n        ```\n\n    ![edge-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/edge-annotator-example.png)\n    \"\"\"\n    if len(key_points) == 0:\n        return scene\n\n    for xy in key_points.xy:\n        edges = self.edges\n        if not edges:\n            edges = SKELETONS_BY_VERTEX_COUNT.get(len(xy))\n        if not edges:\n            warn(f\"No skeleton found with {len(xy)} vertices\")\n            return scene\n\n        for class_a, class_b in edges:\n            xy_a = xy[class_a - 1]\n            xy_b = xy[class_b - 1]\n            missing_a = np.allclose(xy_a, 0)\n            missing_b = np.allclose(xy_b, 0)\n            if missing_a or missing_b:\n                continue\n\n            cv2.line(\n                img=scene,\n                pt1=(int(xy_a[0]), int(xy_a[1])),\n                pt2=(int(xy_b[0]), int(xy_b[1])),\n                color=self.color.as_bgr(),\n                thickness=self.thickness,\n            )\n\n    return scene\n</code></pre>"},{"location":"keypoint/annotators/#supervision.keypoint.annotators.VertexLabelAnnotator-functions","title":"Functions","text":""},{"location":"keypoint/annotators/#supervision.keypoint.annotators.VertexLabelAnnotator.__init__","title":"<code>__init__(color=Color.ROBOFLOW, text_color=Color.WHITE, text_scale=0.5, text_thickness=1, text_padding=10, border_radius=0)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>color</code> <code>Union[Color, List[Color]]</code> <p>The color to use for each keypoint label. If a list is provided, the colors will be used in order for each keypoint.</p> <code>ROBOFLOW</code> <code>text_color</code> <code>Union[Color, List[Color]]</code> <p>The color to use for the labels. If a list is provided, the colors will be used in order for each keypoint.</p> <code>WHITE</code> <code>text_scale</code> <code>float</code> <p>The scale of the text.</p> <code>0.5</code> <code>text_thickness</code> <code>int</code> <p>The thickness of the text.</p> <code>1</code> <code>text_padding</code> <code>int</code> <p>The padding around the text.</p> <code>10</code> <code>border_radius</code> <code>int</code> <p>The radius of the rounded corners of the boxes. Set to a high value to produce circles.</p> <code>0</code> Source code in <code>supervision/keypoint/annotators.py</code> <pre><code>def __init__(\n    self,\n    color: Union[Color, List[Color]] = Color.ROBOFLOW,\n    text_color: Union[Color, List[Color]] = Color.WHITE,\n    text_scale: float = 0.5,\n    text_thickness: int = 1,\n    text_padding: int = 10,\n    border_radius: int = 0,\n):\n    \"\"\"\n    Args:\n        color (Union[Color, List[Color]], optional): The color to use for each\n            keypoint label. If a list is provided, the colors will be used in order\n            for each keypoint.\n        text_color (Union[Color, List[Color]], optional): The color to use\n            for the labels. If a list is provided, the colors will be used in order\n            for each keypoint.\n        text_scale (float, optional): The scale of the text.\n        text_thickness (int, optional): The thickness of the text.\n        text_padding (int, optional): The padding around the text.\n        border_radius (int, optional): The radius of the rounded corners of the\n            boxes. Set to a high value to produce circles.\n    \"\"\"\n    self.border_radius: int = border_radius\n    self.color: Union[Color, List[Color]] = color\n    self.text_color: Union[Color, List[Color]] = text_color\n    self.text_scale: float = text_scale\n    self.text_thickness: int = text_thickness\n    self.text_padding: int = text_padding\n</code></pre>"},{"location":"keypoint/annotators/#supervision.keypoint.annotators.VertexLabelAnnotator.annotate","title":"<code>annotate(scene, key_points, labels=None)</code>","text":"<p>A class that draws labels of skeleton vertices on images. It uses specified key     points to determine the locations where the vertices should be drawn.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ImageType</code> <p>The image where vertex labels will be drawn. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>key_points</code> <code>KeyPoints</code> <p>A collection of key points where each key point consists of x and y coordinates.</p> required <code>labels</code> <code>List[str]</code> <p>A list of labels to be displayed on the annotated image. If not provided, keypoint indices will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The annotated image, matching the type of <code>scene</code> (<code>numpy.ndarray</code> or <code>PIL.Image.Image</code>)</p> Example <pre><code>import supervision as sv\n\nimage = ...\nkey_points = sv.KeyPoints(...)\n\nvertex_label_annotator = sv.VertexLabelAnnotator(\n    color=sv.Color.GREEN,\n    text_color=sv.Color.BLACK,\n    border_radius=5\n)\nannotated_frame = vertex_label_annotator.annotate(\n    scene=image.copy(),\n    key_points=key_points\n)\n</code></pre> <p></p> <p>Tip</p> <p><code>VertexLabelAnnotator</code> allows to customize the color of each keypoint label values.</p> Example <pre><code>import supervision as sv\n\nimage = ...\nkey_points = sv.KeyPoints(...)\n\nLABELS = [\n    \"nose\", \"left eye\", \"right eye\", \"left ear\",\n    \"right ear\", \"left shoulder\", \"right shoulder\", \"left elbow\",\n    \"right elbow\", \"left wrist\", \"right wrist\", \"left hip\",\n    \"right hip\", \"left knee\", \"right knee\", \"left ankle\",\n    \"right ankle\"\n]\n\nCOLORS = [\n    \"#FF6347\", \"#FF6347\", \"#FF6347\", \"#FF6347\",\n    \"#FF6347\", \"#FF1493\", \"#00FF00\", \"#FF1493\",\n    \"#00FF00\", \"#FF1493\", \"#00FF00\", \"#FFD700\",\n    \"#00BFFF\", \"#FFD700\", \"#00BFFF\", \"#FFD700\",\n    \"#00BFFF\"\n]\nCOLORS = [sv.Color.from_hex(color_hex=c) for c in COLORS]\n\nvertex_label_annotator = sv.VertexLabelAnnotator(\n    color=COLORS,\n    text_color=sv.Color.BLACK,\n    border_radius=5\n)\nannotated_frame = vertex_label_annotator.annotate(\n    scene=image.copy(),\n    key_points=key_points,\n    labels=labels\n)\n</code></pre> <p></p> Source code in <code>supervision/keypoint/annotators.py</code> <pre><code>def annotate(\n    self, scene: ImageType, key_points: KeyPoints, labels: List[str] = None\n) -&gt; ImageType:\n    \"\"\"\n    A class that draws labels of skeleton vertices on images. It uses specified key\n        points to determine the locations where the vertices should be drawn.\n\n    Args:\n        scene (ImageType): The image where vertex labels will be drawn. `ImageType`\n            is a flexible type, accepting either `numpy.ndarray` or\n            `PIL.Image.Image`.\n        key_points (KeyPoints): A collection of key points where each key point\n            consists of x and y coordinates.\n        labels (List[str], optional): A list of labels to be displayed on the\n            annotated image. If not provided, keypoint indices will be used.\n\n    Returns:\n        The annotated image, matching the type of `scene` (`numpy.ndarray`\n            or `PIL.Image.Image`)\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        key_points = sv.KeyPoints(...)\n\n        vertex_label_annotator = sv.VertexLabelAnnotator(\n            color=sv.Color.GREEN,\n            text_color=sv.Color.BLACK,\n            border_radius=5\n        )\n        annotated_frame = vertex_label_annotator.annotate(\n            scene=image.copy(),\n            key_points=key_points\n        )\n        ```\n\n    ![vertex-label-annotator-example](https://media.roboflow.com/\n    supervision-annotator-examples/vertex-label-annotator-example.png)\n\n    !!! tip\n\n        `VertexLabelAnnotator` allows to customize the color of each keypoint label\n        values.\n\n    Example:\n        ```python\n        import supervision as sv\n\n        image = ...\n        key_points = sv.KeyPoints(...)\n\n        LABELS = [\n            \"nose\", \"left eye\", \"right eye\", \"left ear\",\n            \"right ear\", \"left shoulder\", \"right shoulder\", \"left elbow\",\n            \"right elbow\", \"left wrist\", \"right wrist\", \"left hip\",\n            \"right hip\", \"left knee\", \"right knee\", \"left ankle\",\n            \"right ankle\"\n        ]\n\n        COLORS = [\n            \"#FF6347\", \"#FF6347\", \"#FF6347\", \"#FF6347\",\n            \"#FF6347\", \"#FF1493\", \"#00FF00\", \"#FF1493\",\n            \"#00FF00\", \"#FF1493\", \"#00FF00\", \"#FFD700\",\n            \"#00BFFF\", \"#FFD700\", \"#00BFFF\", \"#FFD700\",\n            \"#00BFFF\"\n        ]\n        COLORS = [sv.Color.from_hex(color_hex=c) for c in COLORS]\n\n        vertex_label_annotator = sv.VertexLabelAnnotator(\n            color=COLORS,\n            text_color=sv.Color.BLACK,\n            border_radius=5\n        )\n        annotated_frame = vertex_label_annotator.annotate(\n            scene=image.copy(),\n            key_points=key_points,\n            labels=labels\n        )\n        ```\n    ![vertex-label-annotator-custom-example](https://media.roboflow.com/\n    supervision-annotator-examples/vertex-label-annotator-custom-example.png)\n    \"\"\"\n    font = cv2.FONT_HERSHEY_SIMPLEX\n\n    skeletons_count, points_count, _ = key_points.xy.shape\n    if skeletons_count == 0:\n        return scene\n\n    anchors = key_points.xy.reshape(points_count * skeletons_count, 2).astype(int)\n    mask = np.all(anchors != 0, axis=1)\n\n    if not np.any(mask):\n        return scene\n\n    colors = self.preprocess_and_validate_colors(\n        colors=self.color,\n        points_count=points_count,\n        skeletons_count=skeletons_count,\n    )\n\n    text_colors = self.preprocess_and_validate_colors(\n        colors=self.text_color,\n        points_count=points_count,\n        skeletons_count=skeletons_count,\n    )\n\n    labels = self.preprocess_and_validate_labels(\n        labels=labels, points_count=points_count, skeletons_count=skeletons_count\n    )\n\n    anchors = anchors[mask]\n    colors = colors[mask]\n    text_colors = text_colors[mask]\n    labels = labels[mask]\n\n    xyxy = np.array(\n        [\n            self.get_text_bounding_box(\n                text=label,\n                font=font,\n                text_scale=self.text_scale,\n                text_thickness=self.text_thickness,\n                center_coordinates=tuple(anchor),\n            )\n            for anchor, label in zip(anchors, labels)\n        ]\n    )\n\n    xyxy_padded = pad_boxes(xyxy=xyxy, px=self.text_padding)\n\n    for text, color, text_color, box, box_padded in zip(\n        labels, colors, text_colors, xyxy, xyxy_padded\n    ):\n        draw_rounded_rectangle(\n            scene=scene,\n            rect=Rect.from_xyxy(box_padded),\n            color=color,\n            border_radius=self.border_radius,\n        )\n        cv2.putText(\n            img=scene,\n            text=text,\n            org=(box[0], box[3]),\n            fontFace=font,\n            fontScale=self.text_scale,\n            color=text_color.as_bgr(),\n            thickness=self.text_thickness,\n            lineType=cv2.LINE_AA,\n        )\n\n    return scene\n</code></pre>"},{"location":"keypoint/core/","title":"Keypoint Detection","text":"<p>The <code>sv.KeyPoints</code> class in the Supervision library standardizes results from various keypoint detection and pose estimation models into a consistent format. This class simplifies data manipulation and filtering, providing a uniform API for integration with Supervision keypoints annotators.</p> UltralyticsInferenceMediaPipe <p>Use <code>sv.KeyPoints.from_ultralytics</code> method, which accepts YOLOv8 pose result.</p> <pre><code>import cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nmodel = YOLO('yolov8s-pose.pt')\n\nresult = model(image)[0]\nkey_points = sv.KeyPoints.from_ultralytics(result)\n</code></pre> <p>Use <code>sv.KeyPoints.from_inference</code> method, which accepts Inference pose result.</p> <pre><code>import cv2\nimport supervision as sv\nfrom inference import get_model\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nmodel = get_model(model_id=&lt;POSE_MODEL_ID&gt;, api_key=&lt;ROBOFLOW_API_KEY&gt;)\n\nresult = model.infer(image)[0]\nkey_points = sv.KeyPoints.from_inference(result)\n</code></pre> <p>Use <code>sv.KeyPoints.from_mediapipe</code> method, which accepts MediaPipe pose result.</p> <pre><code>import cv2\nimport mediapipe as mp\nimport supervision as sv\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nimage_height, image_width, _ = image.shape\nmediapipe_image = mp.Image(\n    image_format=mp.ImageFormat.SRGB,\n    data=cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\noptions = mp.tasks.vision.PoseLandmarkerOptions(\n    base_options=mp.tasks.BaseOptions(\n        model_asset_path=\"pose_landmarker_heavy.task\"\n    ),\n    running_mode=mp.tasks.vision.RunningMode.IMAGE,\n    num_poses=2)\n\nPoseLandmarker = mp.tasks.vision.PoseLandmarker\nwith PoseLandmarker.create_from_options(options) as landmarker:\n    pose_landmarker_result = landmarker.detect(mediapipe_image)\n\nkey_points = sv.KeyPoints.from_mediapipe(\n    pose_landmarker_result, (image_width, image_height))\n</code></pre> <p>Attributes:</p> Name Type Description <code>xy</code> <code>ndarray</code> <p>An array of shape <code>(n, 2)</code> containing the bounding boxes coordinates in format <code>[x1, y1]</code></p> <code>confidence</code> <code>Optional[ndarray]</code> <p>An array of shape <code>(n,)</code> containing the confidence scores of the keypoint keypoints.</p> <code>class_id</code> <code>Optional[ndarray]</code> <p>An array of shape <code>(n,)</code> containing the class ids of the keypoint keypoints.</p> <code>data</code> <code>Dict[str, Union[ndarray, List]]</code> <p>A dictionary containing additional data where each key is a string representing the data type, and the value is either a NumPy array or a list of corresponding data.</p> Source code in <code>supervision/keypoint/core.py</code> <pre><code>@dataclass\nclass KeyPoints:\n    \"\"\"\n    The `sv.KeyPoints` class in the Supervision library standardizes results from\n    various keypoint detection and pose estimation models into a consistent format. This\n    class simplifies data manipulation and filtering, providing a uniform API for\n    integration with Supervision [keypoints annotators](/keypoint/annotators).\n\n    === \"Ultralytics\"\n\n        Use [`sv.KeyPoints.from_ultralytics`](/keypoint/core/#supervision.keypoint.core.KeyPoints.from_ultralytics)\n        method, which accepts [YOLOv8](https://github.com/ultralytics/ultralytics)\n        pose result.\n\n        ```python\n        import cv2\n        import supervision as sv\n        from ultralytics import YOLO\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        model = YOLO('yolov8s-pose.pt')\n\n        result = model(image)[0]\n        key_points = sv.KeyPoints.from_ultralytics(result)\n        ```\n\n    === \"Inference\"\n\n        Use [`sv.KeyPoints.from_inference`](/keypoint/core/#supervision.keypoint.core.KeyPoints.from_inference)\n        method, which accepts [Inference](https://inference.roboflow.com/) pose result.\n\n        ```python\n        import cv2\n        import supervision as sv\n        from inference import get_model\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        model = get_model(model_id=&lt;POSE_MODEL_ID&gt;, api_key=&lt;ROBOFLOW_API_KEY&gt;)\n\n        result = model.infer(image)[0]\n        key_points = sv.KeyPoints.from_inference(result)\n        ```\n\n    === \"MediaPipe\"\n\n        Use [`sv.KeyPoints.from_mediapipe`](/keypoint/core/#supervision.keypoint.core.KeyPoints.from_mediapipe)\n        method, which accepts [MediaPipe](https://github.com/google-ai-edge/mediapipe)\n        pose result.\n\n        ```python\n        import cv2\n        import mediapipe as mp\n        import supervision as sv\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        image_height, image_width, _ = image.shape\n        mediapipe_image = mp.Image(\n            image_format=mp.ImageFormat.SRGB,\n            data=cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n        options = mp.tasks.vision.PoseLandmarkerOptions(\n            base_options=mp.tasks.BaseOptions(\n                model_asset_path=\"pose_landmarker_heavy.task\"\n            ),\n            running_mode=mp.tasks.vision.RunningMode.IMAGE,\n            num_poses=2)\n\n        PoseLandmarker = mp.tasks.vision.PoseLandmarker\n        with PoseLandmarker.create_from_options(options) as landmarker:\n            pose_landmarker_result = landmarker.detect(mediapipe_image)\n\n        key_points = sv.KeyPoints.from_mediapipe(\n            pose_landmarker_result, (image_width, image_height))\n        ```\n\n    Attributes:\n        xy (np.ndarray): An array of shape `(n, 2)` containing\n            the bounding boxes coordinates in format `[x1, y1]`\n        confidence (Optional[np.ndarray]): An array of shape\n            `(n,)` containing the confidence scores of the keypoint keypoints.\n        class_id (Optional[np.ndarray]): An array of shape\n            `(n,)` containing the class ids of the keypoint keypoints.\n        data (Dict[str, Union[np.ndarray, List]]): A dictionary containing additional\n            data where each key is a string representing the data type, and the value\n            is either a NumPy array or a list of corresponding data.\n    \"\"\"  # noqa: E501 // docs\n\n    xy: npt.NDArray[np.float32]\n    class_id: Optional[npt.NDArray[np.int_]] = None\n    confidence: Optional[npt.NDArray[np.float32]] = None\n    data: Dict[str, Union[npt.NDArray[Any], List]] = field(default_factory=dict)\n\n    def __post_init__(self):\n        validate_keypoints_fields(\n            xy=self.xy,\n            confidence=self.confidence,\n            class_id=self.class_id,\n            data=self.data,\n        )\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the number of keypoints in the `sv.KeyPoints` object.\n        \"\"\"\n        return len(self.xy)\n\n    def __iter__(\n        self,\n    ) -&gt; Iterator[\n        Tuple[\n            np.ndarray,\n            Optional[np.ndarray],\n            Optional[float],\n            Optional[int],\n            Optional[int],\n            Dict[str, Union[np.ndarray, List]],\n        ]\n    ]:\n        \"\"\"\n        Iterates over the Keypoint object and yield a tuple of\n        `(xy, confidence, class_id, data)` for each keypoint detection.\n        \"\"\"\n        for i in range(len(self.xy)):\n            yield (\n                self.xy[i],\n                self.confidence[i] if self.confidence is not None else None,\n                self.class_id[i] if self.class_id is not None else None,\n                get_data_item(self.data, i),\n            )\n\n    def __eq__(self, other: KeyPoints) -&gt; bool:\n        return all(\n            [\n                np.array_equal(self.xy, other.xy),\n                np.array_equal(self.class_id, other.class_id),\n                np.array_equal(self.confidence, other.confidence),\n                is_data_equal(self.data, other.data),\n            ]\n        )\n\n    @classmethod\n    def from_inference(cls, inference_result: Union[dict, Any]) -&gt; KeyPoints:\n        \"\"\"\n        Create a `sv.KeyPoints` object from the [Roboflow](https://roboflow.com/)\n        API inference result or the [Inference](https://inference.roboflow.com/)\n        package results.\n\n        Args:\n            inference_result (dict, any): The result from the\n                Roboflow API or Inference package containing predictions with keypoints.\n\n        Returns:\n            A `sv.KeyPoints` object containing the keypoint coordinates, class IDs,\n                and class names, and confidences of each keypoint.\n\n        Examples:\n            ```python\n            import cv2\n            import supervision as sv\n            from inference import get_model\n\n            image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n            model = get_model(model_id=&lt;POSE_MODEL_ID&gt;, api_key=&lt;ROBOFLOW_API_KEY&gt;)\n\n            result = model.infer(image)[0]\n            key_points = sv.KeyPoints.from_inference(result)\n            ```\n\n            ```python\n            import cv2\n            import supervision as sv\n            from inference_sdk import InferenceHTTPClient\n\n            image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n            client = InferenceHTTPClient(\n                api_url=\"https://detect.roboflow.com\",\n                api_key=&lt;ROBOFLOW_API_KEY&gt;\n            )\n\n            result = client.infer(image, model_id=&lt;POSE_MODEL_ID&gt;)\n            key_points = sv.KeyPoints.from_inference(result)\n            ```\n        \"\"\"\n        if isinstance(inference_result, list):\n            raise ValueError(\n                \"from_inference() operates on a single result at a time.\"\n                \"You can retrieve it like so:  inference_result = model.infer(image)[0]\"\n            )\n\n        with suppress(AttributeError):\n            inference_result = inference_result.dict(exclude_none=True, by_alias=True)\n\n        if not inference_result.get(\"predictions\"):\n            return cls.empty()\n\n        xy = []\n        confidence = []\n        class_id = []\n        class_names = []\n\n        for prediction in inference_result[\"predictions\"]:\n            prediction_xy = []\n            prediction_confidence = []\n            for keypoint in prediction[\"keypoints\"]:\n                prediction_xy.append([keypoint[\"x\"], keypoint[\"y\"]])\n                prediction_confidence.append(keypoint[\"confidence\"])\n            xy.append(prediction_xy)\n            confidence.append(prediction_confidence)\n\n            class_id.append(prediction[\"class_id\"])\n            class_names.append(prediction[\"class\"])\n\n        data = {CLASS_NAME_DATA_FIELD: np.array(class_names)}\n\n        return cls(\n            xy=np.array(xy, dtype=np.float32),\n            confidence=np.array(confidence, dtype=np.float32),\n            class_id=np.array(class_id, dtype=int),\n            data=data,\n        )\n\n    @classmethod\n    def from_mediapipe(\n        cls, mediapipe_results, resolution_wh: Tuple[int, int]\n    ) -&gt; KeyPoints:\n        \"\"\"\n        Creates a `sv.KeyPoints` instance from a\n        [MediaPipe](https://github.com/google-ai-edge/mediapipe)\n        pose landmark detection inference result.\n\n        Args:\n            mediapipe_results (Union[PoseLandmarkerResult, FaceLandmarkerResult, SolutionOutputs]):\n                The output results from Mediapipe. It support pose and face landmarks\n                from `PoseLandmaker`, `FaceLandmarker` and the legacy ones\n                from `Pose` and `FaceMesh`.\n            resolution_wh (Tuple[int, int]): A tuple of the form `(width, height)`\n                representing the resolution of the frame.\n\n        Returns:\n            A `sv.KeyPoints` object containing the keypoint coordinates and\n                confidences of each keypoint.\n\n        !!! tip\n            Before you start, download model bundles from the\n            [MediaPipe website](https://ai.google.dev/edge/mediapipe/solutions/vision/pose_landmarker/index#models).\n\n        Examples:\n            ```python\n            import cv2\n            import mediapipe as mp\n            import supervision as sv\n\n            image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n            image_height, image_width, _ = image.shape\n            mediapipe_image = mp.Image(\n                image_format=mp.ImageFormat.SRGB,\n                data=cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n            options = mp.tasks.vision.PoseLandmarkerOptions(\n                base_options=mp.tasks.BaseOptions(\n                    model_asset_path=\"pose_landmarker_heavy.task\"\n                ),\n                running_mode=mp.tasks.vision.RunningMode.IMAGE,\n                num_poses=2)\n\n            PoseLandmarker = mp.tasks.vision.PoseLandmarker\n            with PoseLandmarker.create_from_options(options) as landmarker:\n                pose_landmarker_result = landmarker.detect(mediapipe_image)\n\n            key_points = sv.KeyPoints.from_mediapipe(\n                pose_landmarker_result, (image_width, image_height))\n            ```\n\n            ```python\n            import cv2\n            import mediapipe as mp\n            import supervision as sv\n\n            image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n            image_height, image_width, _ = image.shape\n            mediapipe_image = mp.Image(\n                image_format=mp.ImageFormat.SRGB,\n                data=cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n            options = mp.tasks.vision.FaceLandmarkerOptions(\n                base_options=mp.tasks.BaseOptions(\n                    model_asset_path=\"face_landmarker.task\"\n                ),\n                output_face_blendshapes=True,\n                output_facial_transformation_matrixes=True,\n                num_faces=2)\n\n            FaceLandmarker = mp.tasks.vision.FaceLandmarker\n            with FaceLandmarker.create_from_options(options) as landmarker:\n                face_landmarker_result = landmarker.detect(mediapipe_image)\n\n            key_points = sv.KeyPoints.from_mediapipe(\n                face_landmarker_result, (image_width, image_height))\n            ```\n        \"\"\"  # noqa: E501 // docs\n        if hasattr(mediapipe_results, \"pose_landmarks\"):\n            results = mediapipe_results.pose_landmarks\n            if not isinstance(mediapipe_results.pose_landmarks, list):\n                if mediapipe_results.pose_landmarks is None:\n                    results = []\n                else:\n                    results = [\n                        [\n                            landmark\n                            for landmark in mediapipe_results.pose_landmarks.landmark\n                        ]\n                    ]\n        elif hasattr(mediapipe_results, \"face_landmarks\"):\n            results = mediapipe_results.face_landmarks\n        elif hasattr(mediapipe_results, \"multi_face_landmarks\"):\n            if mediapipe_results.multi_face_landmarks is None:\n                results = []\n            else:\n                results = [\n                    face_landmark.landmark\n                    for face_landmark in mediapipe_results.multi_face_landmarks\n                ]\n\n        if len(results) == 0:\n            return cls.empty()\n\n        xy = []\n        confidence = []\n        for pose in results:\n            prediction_xy = []\n            prediction_confidence = []\n            for landmark in pose:\n                keypoint_xy = [\n                    landmark.x * resolution_wh[0],\n                    landmark.y * resolution_wh[1],\n                ]\n                prediction_xy.append(keypoint_xy)\n                prediction_confidence.append(landmark.visibility)\n\n            xy.append(prediction_xy)\n            confidence.append(prediction_confidence)\n\n        return cls(\n            xy=np.array(xy, dtype=np.float32),\n            confidence=np.array(confidence, dtype=np.float32),\n        )\n\n    @classmethod\n    def from_ultralytics(cls, ultralytics_results) -&gt; KeyPoints:\n        \"\"\"\n        Creates a `sv.KeyPoints` instance from a\n        [YOLOv8](https://github.com/ultralytics/ultralytics) pose inference result.\n\n        Args:\n            ultralytics_results (ultralytics.engine.results.Keypoints):\n                The output Results instance from YOLOv8\n\n        Returns:\n            A `sv.KeyPoints` object containing the keypoint coordinates, class IDs,\n                and class names, and confidences of each keypoint.\n\n        Examples:\n            ```python\n            import cv2\n            import supervision as sv\n            from ultralytics import YOLO\n\n            image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n            model = YOLO('yolov8s-pose.pt')\n\n            result = model(image)[0]\n            key_points = sv.KeyPoints.from_ultralytics(result)\n            ```\n        \"\"\"\n        if ultralytics_results.keypoints.xy.numel() == 0:\n            return cls.empty()\n\n        xy = ultralytics_results.keypoints.xy.cpu().numpy()\n        class_id = ultralytics_results.boxes.cls.cpu().numpy().astype(int)\n        class_names = np.array([ultralytics_results.names[i] for i in class_id])\n\n        confidence = ultralytics_results.keypoints.conf.cpu().numpy()\n        data = {CLASS_NAME_DATA_FIELD: class_names}\n        return cls(xy, class_id, confidence, data)\n\n    @classmethod\n    def from_yolo_nas(cls, yolo_nas_results) -&gt; KeyPoints:\n        \"\"\"\n        Create a `sv.KeyPoints` instance from a [YOLO-NAS](https://github.com/Deci-AI/super-gradients/blob/master/YOLONAS-POSE.md)\n        pose inference results.\n\n        Args:\n            yolo_nas_results (ImagePoseEstimationPrediction): The output object from\n                YOLO NAS.\n\n        Returns:\n            A `sv.KeyPoints` object containing the keypoint coordinates, class IDs,\n                and class names, and confidences of each keypoint.\n\n        Examples:\n            ```python\n            import cv2\n            import torch\n            import supervision as sv\n            import super_gradients\n\n            image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            model = super_gradients.training.models.get(\n                \"yolo_nas_pose_s\", pretrained_weights=\"coco_pose\").to(device)\n\n            results = model.predict(image, conf=0.1)\n            key_points = sv.KeyPoints.from_yolo_nas(results)\n            ```\n        \"\"\"  # noqa: E501 // docs\n        if len(yolo_nas_results.prediction.poses) == 0:\n            return cls.empty()\n\n        xy = yolo_nas_results.prediction.poses[:, :, :2]\n        confidence = yolo_nas_results.prediction.poses[:, :, 2]\n\n        # yolo_nas_results treats params differently.\n        # prediction.labels may not exist, whereas class_names might be None\n        if hasattr(yolo_nas_results.prediction, \"labels\"):\n            class_id = yolo_nas_results.prediction.labels  # np.array[int]\n        else:\n            class_id = None\n\n        data = {}\n        if class_id is not None and yolo_nas_results.class_names is not None:\n            class_names = []\n            for c_id in class_id:\n                name = yolo_nas_results.class_names[c_id]  # tuple[str]\n                class_names.append(name)\n            data[CLASS_NAME_DATA_FIELD] = class_names\n\n        return cls(\n            xy=xy,\n            confidence=confidence,\n            class_id=class_id,\n            data=data,\n        )\n\n    @classmethod\n    def from_detectron2(cls, detectron2_results) -&gt; KeyPoints:\n        \"\"\"\n        Create a `sv.KeyPoints` object from the\n        [Detectron2](https://github.com/facebookresearch/detectron2) inference result.\n\n        Args:\n            detectron2_results: The output of a\n                Detectron2 model containing instances with prediction data.\n\n        Returns:\n            A `sv.KeyPoints` object containing the keypoint coordinates, class IDs,\n                and class names, and confidences of each keypoint.\n\n        Example:\n            ```python\n            import cv2\n            import supervision as sv\n            from detectron2.engine import DefaultPredictor\n            from detectron2.config import get_cfg\n\n\n            image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n            cfg = get_cfg()\n            cfg.merge_from_file(&lt;CONFIG_PATH&gt;)\n            cfg.MODEL.WEIGHTS = &lt;WEIGHTS_PATH&gt;\n            predictor = DefaultPredictor(cfg)\n\n            result = predictor(image)\n            keypoints = sv.KeyPoints.from_detectron2(result)\n            ```\n        \"\"\"\n\n        if hasattr(detectron2_results[\"instances\"], \"pred_keypoints\"):\n            if detectron2_results[\"instances\"].pred_keypoints.cpu().numpy().size == 0:\n                return cls.empty()\n            return cls(\n                xy=detectron2_results[\"instances\"]\n                .pred_keypoints.cpu()\n                .numpy()[:, :, :2],\n                confidence=detectron2_results[\"instances\"]\n                .pred_keypoints.cpu()\n                .numpy()[:, :, 2],\n                class_id=detectron2_results[\"instances\"]\n                .pred_classes.cpu()\n                .numpy()\n                .astype(int),\n            )\n        else:\n            return cls.empty()\n\n    def __getitem__(\n        self, index: Union[int, slice, List[int], np.ndarray, str]\n    ) -&gt; Union[KeyPoints, List, np.ndarray, None]:\n        \"\"\"\n        Get a subset of the `sv.KeyPoints` object or access an item from its data field.\n\n        When provided with an integer, slice, list of integers, or a numpy array, this\n        method returns a new `sv.KeyPoints` object that represents a subset of the\n        original `sv.KeyPoints`. When provided with a string, it accesses the\n        corresponding item in the data dictionary.\n\n        Args:\n            index (Union[int, slice, List[int], np.ndarray, str]): The index, indices,\n                or key to access a subset of the `sv.KeyPoints` or an item from the\n                data.\n\n        Returns:\n            A subset of the `sv.KeyPoints` object or an item from the data field.\n\n        Examples:\n            ```python\n            import supervision as sv\n\n            key_points = sv.KeyPoints()\n\n            # access the first keypoint using an integer index\n            key_points[0]\n\n            # access the first 10 keypoints using index slice\n            key_points[0:10]\n\n            # access selected keypoints using a list of indices\n            key_points[[0, 2, 4]]\n\n            # access keypoints with selected class_id\n            key_points[key_points.class_id == 0]\n\n            # access keypoints with confidence greater than 0.5\n            key_points[key_points.confidence &gt; 0.5]\n            ```\n        \"\"\"\n        if isinstance(index, str):\n            return self.data.get(index)\n        if isinstance(index, int):\n            index = [index]\n        return KeyPoints(\n            xy=self.xy[index],\n            confidence=self.confidence[index] if self.confidence is not None else None,\n            class_id=self.class_id[index] if self.class_id is not None else None,\n            data=get_data_item(self.data, index),\n        )\n\n    def __setitem__(self, key: str, value: Union[np.ndarray, List]):\n        \"\"\"\n        Set a value in the data dictionary of the `sv.KeyPoints` object.\n\n        Args:\n            key (str): The key in the data dictionary to set.\n            value (Union[np.ndarray, List]): The value to set for the key.\n\n        Examples:\n            ```python\n            import cv2\n            import supervision as sv\n            from ultralytics import YOLO\n\n            image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n            model = YOLO('yolov8s.pt')\n\n            result = model(image)[0]\n            keypoints = sv.KeyPoints.from_ultralytics(result)\n\n            keypoints['class_name'] = [\n                 model.model.names[class_id]\n                 for class_id\n                 in keypoints.class_id\n             ]\n            ```\n        \"\"\"\n        if not isinstance(value, (np.ndarray, list)):\n            raise TypeError(\"Value must be a np.ndarray or a list\")\n\n        if isinstance(value, list):\n            value = np.array(value)\n\n        self.data[key] = value\n\n    @classmethod\n    def empty(cls) -&gt; KeyPoints:\n        \"\"\"\n        Create an empty Keypoints object with no keypoints.\n\n        Returns:\n            An empty `sv.KeyPoints` object.\n\n        Examples:\n            ```python\n            import supervision as sv\n\n            key_points = sv.KeyPoints.empty()\n            ```\n        \"\"\"\n        return cls(xy=np.empty((0, 0, 2), dtype=np.float32))\n</code></pre>"},{"location":"keypoint/core/#supervision.keypoint.core.KeyPoints-functions","title":"Functions","text":""},{"location":"keypoint/core/#supervision.keypoint.core.KeyPoints.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get a subset of the <code>sv.KeyPoints</code> object or access an item from its data field.</p> <p>When provided with an integer, slice, list of integers, or a numpy array, this method returns a new <code>sv.KeyPoints</code> object that represents a subset of the original <code>sv.KeyPoints</code>. When provided with a string, it accesses the corresponding item in the data dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Union[int, slice, List[int], ndarray, str]</code> <p>The index, indices, or key to access a subset of the <code>sv.KeyPoints</code> or an item from the data.</p> required <p>Returns:</p> Type Description <code>Union[KeyPoints, List, ndarray, None]</code> <p>A subset of the <code>sv.KeyPoints</code> object or an item from the data field.</p> <p>Examples:</p> <pre><code>import supervision as sv\n\nkey_points = sv.KeyPoints()\n\n# access the first keypoint using an integer index\nkey_points[0]\n\n# access the first 10 keypoints using index slice\nkey_points[0:10]\n\n# access selected keypoints using a list of indices\nkey_points[[0, 2, 4]]\n\n# access keypoints with selected class_id\nkey_points[key_points.class_id == 0]\n\n# access keypoints with confidence greater than 0.5\nkey_points[key_points.confidence &gt; 0.5]\n</code></pre> Source code in <code>supervision/keypoint/core.py</code> <pre><code>def __getitem__(\n    self, index: Union[int, slice, List[int], np.ndarray, str]\n) -&gt; Union[KeyPoints, List, np.ndarray, None]:\n    \"\"\"\n    Get a subset of the `sv.KeyPoints` object or access an item from its data field.\n\n    When provided with an integer, slice, list of integers, or a numpy array, this\n    method returns a new `sv.KeyPoints` object that represents a subset of the\n    original `sv.KeyPoints`. When provided with a string, it accesses the\n    corresponding item in the data dictionary.\n\n    Args:\n        index (Union[int, slice, List[int], np.ndarray, str]): The index, indices,\n            or key to access a subset of the `sv.KeyPoints` or an item from the\n            data.\n\n    Returns:\n        A subset of the `sv.KeyPoints` object or an item from the data field.\n\n    Examples:\n        ```python\n        import supervision as sv\n\n        key_points = sv.KeyPoints()\n\n        # access the first keypoint using an integer index\n        key_points[0]\n\n        # access the first 10 keypoints using index slice\n        key_points[0:10]\n\n        # access selected keypoints using a list of indices\n        key_points[[0, 2, 4]]\n\n        # access keypoints with selected class_id\n        key_points[key_points.class_id == 0]\n\n        # access keypoints with confidence greater than 0.5\n        key_points[key_points.confidence &gt; 0.5]\n        ```\n    \"\"\"\n    if isinstance(index, str):\n        return self.data.get(index)\n    if isinstance(index, int):\n        index = [index]\n    return KeyPoints(\n        xy=self.xy[index],\n        confidence=self.confidence[index] if self.confidence is not None else None,\n        class_id=self.class_id[index] if self.class_id is not None else None,\n        data=get_data_item(self.data, index),\n    )\n</code></pre>"},{"location":"keypoint/core/#supervision.keypoint.core.KeyPoints.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterates over the Keypoint object and yield a tuple of <code>(xy, confidence, class_id, data)</code> for each keypoint detection.</p> Source code in <code>supervision/keypoint/core.py</code> <pre><code>def __iter__(\n    self,\n) -&gt; Iterator[\n    Tuple[\n        np.ndarray,\n        Optional[np.ndarray],\n        Optional[float],\n        Optional[int],\n        Optional[int],\n        Dict[str, Union[np.ndarray, List]],\n    ]\n]:\n    \"\"\"\n    Iterates over the Keypoint object and yield a tuple of\n    `(xy, confidence, class_id, data)` for each keypoint detection.\n    \"\"\"\n    for i in range(len(self.xy)):\n        yield (\n            self.xy[i],\n            self.confidence[i] if self.confidence is not None else None,\n            self.class_id[i] if self.class_id is not None else None,\n            get_data_item(self.data, i),\n        )\n</code></pre>"},{"location":"keypoint/core/#supervision.keypoint.core.KeyPoints.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of keypoints in the <code>sv.KeyPoints</code> object.</p> Source code in <code>supervision/keypoint/core.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the number of keypoints in the `sv.KeyPoints` object.\n    \"\"\"\n    return len(self.xy)\n</code></pre>"},{"location":"keypoint/core/#supervision.keypoint.core.KeyPoints.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Set a value in the data dictionary of the <code>sv.KeyPoints</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key in the data dictionary to set.</p> required <code>value</code> <code>Union[ndarray, List]</code> <p>The value to set for the key.</p> required <p>Examples:</p> <pre><code>import cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nmodel = YOLO('yolov8s.pt')\n\nresult = model(image)[0]\nkeypoints = sv.KeyPoints.from_ultralytics(result)\n\nkeypoints['class_name'] = [\n     model.model.names[class_id]\n     for class_id\n     in keypoints.class_id\n ]\n</code></pre> Source code in <code>supervision/keypoint/core.py</code> <pre><code>def __setitem__(self, key: str, value: Union[np.ndarray, List]):\n    \"\"\"\n    Set a value in the data dictionary of the `sv.KeyPoints` object.\n\n    Args:\n        key (str): The key in the data dictionary to set.\n        value (Union[np.ndarray, List]): The value to set for the key.\n\n    Examples:\n        ```python\n        import cv2\n        import supervision as sv\n        from ultralytics import YOLO\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        model = YOLO('yolov8s.pt')\n\n        result = model(image)[0]\n        keypoints = sv.KeyPoints.from_ultralytics(result)\n\n        keypoints['class_name'] = [\n             model.model.names[class_id]\n             for class_id\n             in keypoints.class_id\n         ]\n        ```\n    \"\"\"\n    if not isinstance(value, (np.ndarray, list)):\n        raise TypeError(\"Value must be a np.ndarray or a list\")\n\n    if isinstance(value, list):\n        value = np.array(value)\n\n    self.data[key] = value\n</code></pre>"},{"location":"keypoint/core/#supervision.keypoint.core.KeyPoints.empty","title":"<code>empty()</code>  <code>classmethod</code>","text":"<p>Create an empty Keypoints object with no keypoints.</p> <p>Returns:</p> Type Description <code>KeyPoints</code> <p>An empty <code>sv.KeyPoints</code> object.</p> <p>Examples:</p> <pre><code>import supervision as sv\n\nkey_points = sv.KeyPoints.empty()\n</code></pre> Source code in <code>supervision/keypoint/core.py</code> <pre><code>@classmethod\ndef empty(cls) -&gt; KeyPoints:\n    \"\"\"\n    Create an empty Keypoints object with no keypoints.\n\n    Returns:\n        An empty `sv.KeyPoints` object.\n\n    Examples:\n        ```python\n        import supervision as sv\n\n        key_points = sv.KeyPoints.empty()\n        ```\n    \"\"\"\n    return cls(xy=np.empty((0, 0, 2), dtype=np.float32))\n</code></pre>"},{"location":"keypoint/core/#supervision.keypoint.core.KeyPoints.from_detectron2","title":"<code>from_detectron2(detectron2_results)</code>  <code>classmethod</code>","text":"<p>Create a <code>sv.KeyPoints</code> object from the Detectron2 inference result.</p> <p>Parameters:</p> Name Type Description Default <code>detectron2_results</code> <p>The output of a Detectron2 model containing instances with prediction data.</p> required <p>Returns:</p> Type Description <code>KeyPoints</code> <p>A <code>sv.KeyPoints</code> object containing the keypoint coordinates, class IDs, and class names, and confidences of each keypoint.</p> Example <pre><code>import cv2\nimport supervision as sv\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\n\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\ncfg = get_cfg()\ncfg.merge_from_file(&lt;CONFIG_PATH&gt;)\ncfg.MODEL.WEIGHTS = &lt;WEIGHTS_PATH&gt;\npredictor = DefaultPredictor(cfg)\n\nresult = predictor(image)\nkeypoints = sv.KeyPoints.from_detectron2(result)\n</code></pre> Source code in <code>supervision/keypoint/core.py</code> <pre><code>@classmethod\ndef from_detectron2(cls, detectron2_results) -&gt; KeyPoints:\n    \"\"\"\n    Create a `sv.KeyPoints` object from the\n    [Detectron2](https://github.com/facebookresearch/detectron2) inference result.\n\n    Args:\n        detectron2_results: The output of a\n            Detectron2 model containing instances with prediction data.\n\n    Returns:\n        A `sv.KeyPoints` object containing the keypoint coordinates, class IDs,\n            and class names, and confidences of each keypoint.\n\n    Example:\n        ```python\n        import cv2\n        import supervision as sv\n        from detectron2.engine import DefaultPredictor\n        from detectron2.config import get_cfg\n\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        cfg = get_cfg()\n        cfg.merge_from_file(&lt;CONFIG_PATH&gt;)\n        cfg.MODEL.WEIGHTS = &lt;WEIGHTS_PATH&gt;\n        predictor = DefaultPredictor(cfg)\n\n        result = predictor(image)\n        keypoints = sv.KeyPoints.from_detectron2(result)\n        ```\n    \"\"\"\n\n    if hasattr(detectron2_results[\"instances\"], \"pred_keypoints\"):\n        if detectron2_results[\"instances\"].pred_keypoints.cpu().numpy().size == 0:\n            return cls.empty()\n        return cls(\n            xy=detectron2_results[\"instances\"]\n            .pred_keypoints.cpu()\n            .numpy()[:, :, :2],\n            confidence=detectron2_results[\"instances\"]\n            .pred_keypoints.cpu()\n            .numpy()[:, :, 2],\n            class_id=detectron2_results[\"instances\"]\n            .pred_classes.cpu()\n            .numpy()\n            .astype(int),\n        )\n    else:\n        return cls.empty()\n</code></pre>"},{"location":"keypoint/core/#supervision.keypoint.core.KeyPoints.from_inference","title":"<code>from_inference(inference_result)</code>  <code>classmethod</code>","text":"<p>Create a <code>sv.KeyPoints</code> object from the Roboflow API inference result or the Inference package results.</p> <p>Parameters:</p> Name Type Description Default <code>inference_result</code> <code>(dict, any)</code> <p>The result from the Roboflow API or Inference package containing predictions with keypoints.</p> required <p>Returns:</p> Type Description <code>KeyPoints</code> <p>A <code>sv.KeyPoints</code> object containing the keypoint coordinates, class IDs, and class names, and confidences of each keypoint.</p> <p>Examples:</p> <pre><code>import cv2\nimport supervision as sv\nfrom inference import get_model\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nmodel = get_model(model_id=&lt;POSE_MODEL_ID&gt;, api_key=&lt;ROBOFLOW_API_KEY&gt;)\n\nresult = model.infer(image)[0]\nkey_points = sv.KeyPoints.from_inference(result)\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom inference_sdk import InferenceHTTPClient\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nclient = InferenceHTTPClient(\n    api_url=\"https://detect.roboflow.com\",\n    api_key=&lt;ROBOFLOW_API_KEY&gt;\n)\n\nresult = client.infer(image, model_id=&lt;POSE_MODEL_ID&gt;)\nkey_points = sv.KeyPoints.from_inference(result)\n</code></pre> Source code in <code>supervision/keypoint/core.py</code> <pre><code>@classmethod\ndef from_inference(cls, inference_result: Union[dict, Any]) -&gt; KeyPoints:\n    \"\"\"\n    Create a `sv.KeyPoints` object from the [Roboflow](https://roboflow.com/)\n    API inference result or the [Inference](https://inference.roboflow.com/)\n    package results.\n\n    Args:\n        inference_result (dict, any): The result from the\n            Roboflow API or Inference package containing predictions with keypoints.\n\n    Returns:\n        A `sv.KeyPoints` object containing the keypoint coordinates, class IDs,\n            and class names, and confidences of each keypoint.\n\n    Examples:\n        ```python\n        import cv2\n        import supervision as sv\n        from inference import get_model\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        model = get_model(model_id=&lt;POSE_MODEL_ID&gt;, api_key=&lt;ROBOFLOW_API_KEY&gt;)\n\n        result = model.infer(image)[0]\n        key_points = sv.KeyPoints.from_inference(result)\n        ```\n\n        ```python\n        import cv2\n        import supervision as sv\n        from inference_sdk import InferenceHTTPClient\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        client = InferenceHTTPClient(\n            api_url=\"https://detect.roboflow.com\",\n            api_key=&lt;ROBOFLOW_API_KEY&gt;\n        )\n\n        result = client.infer(image, model_id=&lt;POSE_MODEL_ID&gt;)\n        key_points = sv.KeyPoints.from_inference(result)\n        ```\n    \"\"\"\n    if isinstance(inference_result, list):\n        raise ValueError(\n            \"from_inference() operates on a single result at a time.\"\n            \"You can retrieve it like so:  inference_result = model.infer(image)[0]\"\n        )\n\n    with suppress(AttributeError):\n        inference_result = inference_result.dict(exclude_none=True, by_alias=True)\n\n    if not inference_result.get(\"predictions\"):\n        return cls.empty()\n\n    xy = []\n    confidence = []\n    class_id = []\n    class_names = []\n\n    for prediction in inference_result[\"predictions\"]:\n        prediction_xy = []\n        prediction_confidence = []\n        for keypoint in prediction[\"keypoints\"]:\n            prediction_xy.append([keypoint[\"x\"], keypoint[\"y\"]])\n            prediction_confidence.append(keypoint[\"confidence\"])\n        xy.append(prediction_xy)\n        confidence.append(prediction_confidence)\n\n        class_id.append(prediction[\"class_id\"])\n        class_names.append(prediction[\"class\"])\n\n    data = {CLASS_NAME_DATA_FIELD: np.array(class_names)}\n\n    return cls(\n        xy=np.array(xy, dtype=np.float32),\n        confidence=np.array(confidence, dtype=np.float32),\n        class_id=np.array(class_id, dtype=int),\n        data=data,\n    )\n</code></pre>"},{"location":"keypoint/core/#supervision.keypoint.core.KeyPoints.from_mediapipe","title":"<code>from_mediapipe(mediapipe_results, resolution_wh)</code>  <code>classmethod</code>","text":"<p>Creates a <code>sv.KeyPoints</code> instance from a MediaPipe pose landmark detection inference result.</p> <p>Parameters:</p> Name Type Description Default <code>mediapipe_results</code> <code>Union[PoseLandmarkerResult, FaceLandmarkerResult, SolutionOutputs]</code> <p>The output results from Mediapipe. It support pose and face landmarks from <code>PoseLandmaker</code>, <code>FaceLandmarker</code> and the legacy ones from <code>Pose</code> and <code>FaceMesh</code>.</p> required <code>resolution_wh</code> <code>Tuple[int, int]</code> <p>A tuple of the form <code>(width, height)</code> representing the resolution of the frame.</p> required <p>Returns:</p> Type Description <code>KeyPoints</code> <p>A <code>sv.KeyPoints</code> object containing the keypoint coordinates and confidences of each keypoint.</p> <p>Tip</p> <p>Before you start, download model bundles from the MediaPipe website.</p> <p>Examples:</p> <pre><code>import cv2\nimport mediapipe as mp\nimport supervision as sv\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nimage_height, image_width, _ = image.shape\nmediapipe_image = mp.Image(\n    image_format=mp.ImageFormat.SRGB,\n    data=cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\noptions = mp.tasks.vision.PoseLandmarkerOptions(\n    base_options=mp.tasks.BaseOptions(\n        model_asset_path=\"pose_landmarker_heavy.task\"\n    ),\n    running_mode=mp.tasks.vision.RunningMode.IMAGE,\n    num_poses=2)\n\nPoseLandmarker = mp.tasks.vision.PoseLandmarker\nwith PoseLandmarker.create_from_options(options) as landmarker:\n    pose_landmarker_result = landmarker.detect(mediapipe_image)\n\nkey_points = sv.KeyPoints.from_mediapipe(\n    pose_landmarker_result, (image_width, image_height))\n</code></pre> <pre><code>import cv2\nimport mediapipe as mp\nimport supervision as sv\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nimage_height, image_width, _ = image.shape\nmediapipe_image = mp.Image(\n    image_format=mp.ImageFormat.SRGB,\n    data=cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\noptions = mp.tasks.vision.FaceLandmarkerOptions(\n    base_options=mp.tasks.BaseOptions(\n        model_asset_path=\"face_landmarker.task\"\n    ),\n    output_face_blendshapes=True,\n    output_facial_transformation_matrixes=True,\n    num_faces=2)\n\nFaceLandmarker = mp.tasks.vision.FaceLandmarker\nwith FaceLandmarker.create_from_options(options) as landmarker:\n    face_landmarker_result = landmarker.detect(mediapipe_image)\n\nkey_points = sv.KeyPoints.from_mediapipe(\n    face_landmarker_result, (image_width, image_height))\n</code></pre> Source code in <code>supervision/keypoint/core.py</code> <pre><code>@classmethod\ndef from_mediapipe(\n    cls, mediapipe_results, resolution_wh: Tuple[int, int]\n) -&gt; KeyPoints:\n    \"\"\"\n    Creates a `sv.KeyPoints` instance from a\n    [MediaPipe](https://github.com/google-ai-edge/mediapipe)\n    pose landmark detection inference result.\n\n    Args:\n        mediapipe_results (Union[PoseLandmarkerResult, FaceLandmarkerResult, SolutionOutputs]):\n            The output results from Mediapipe. It support pose and face landmarks\n            from `PoseLandmaker`, `FaceLandmarker` and the legacy ones\n            from `Pose` and `FaceMesh`.\n        resolution_wh (Tuple[int, int]): A tuple of the form `(width, height)`\n            representing the resolution of the frame.\n\n    Returns:\n        A `sv.KeyPoints` object containing the keypoint coordinates and\n            confidences of each keypoint.\n\n    !!! tip\n        Before you start, download model bundles from the\n        [MediaPipe website](https://ai.google.dev/edge/mediapipe/solutions/vision/pose_landmarker/index#models).\n\n    Examples:\n        ```python\n        import cv2\n        import mediapipe as mp\n        import supervision as sv\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        image_height, image_width, _ = image.shape\n        mediapipe_image = mp.Image(\n            image_format=mp.ImageFormat.SRGB,\n            data=cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n        options = mp.tasks.vision.PoseLandmarkerOptions(\n            base_options=mp.tasks.BaseOptions(\n                model_asset_path=\"pose_landmarker_heavy.task\"\n            ),\n            running_mode=mp.tasks.vision.RunningMode.IMAGE,\n            num_poses=2)\n\n        PoseLandmarker = mp.tasks.vision.PoseLandmarker\n        with PoseLandmarker.create_from_options(options) as landmarker:\n            pose_landmarker_result = landmarker.detect(mediapipe_image)\n\n        key_points = sv.KeyPoints.from_mediapipe(\n            pose_landmarker_result, (image_width, image_height))\n        ```\n\n        ```python\n        import cv2\n        import mediapipe as mp\n        import supervision as sv\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        image_height, image_width, _ = image.shape\n        mediapipe_image = mp.Image(\n            image_format=mp.ImageFormat.SRGB,\n            data=cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n        options = mp.tasks.vision.FaceLandmarkerOptions(\n            base_options=mp.tasks.BaseOptions(\n                model_asset_path=\"face_landmarker.task\"\n            ),\n            output_face_blendshapes=True,\n            output_facial_transformation_matrixes=True,\n            num_faces=2)\n\n        FaceLandmarker = mp.tasks.vision.FaceLandmarker\n        with FaceLandmarker.create_from_options(options) as landmarker:\n            face_landmarker_result = landmarker.detect(mediapipe_image)\n\n        key_points = sv.KeyPoints.from_mediapipe(\n            face_landmarker_result, (image_width, image_height))\n        ```\n    \"\"\"  # noqa: E501 // docs\n    if hasattr(mediapipe_results, \"pose_landmarks\"):\n        results = mediapipe_results.pose_landmarks\n        if not isinstance(mediapipe_results.pose_landmarks, list):\n            if mediapipe_results.pose_landmarks is None:\n                results = []\n            else:\n                results = [\n                    [\n                        landmark\n                        for landmark in mediapipe_results.pose_landmarks.landmark\n                    ]\n                ]\n    elif hasattr(mediapipe_results, \"face_landmarks\"):\n        results = mediapipe_results.face_landmarks\n    elif hasattr(mediapipe_results, \"multi_face_landmarks\"):\n        if mediapipe_results.multi_face_landmarks is None:\n            results = []\n        else:\n            results = [\n                face_landmark.landmark\n                for face_landmark in mediapipe_results.multi_face_landmarks\n            ]\n\n    if len(results) == 0:\n        return cls.empty()\n\n    xy = []\n    confidence = []\n    for pose in results:\n        prediction_xy = []\n        prediction_confidence = []\n        for landmark in pose:\n            keypoint_xy = [\n                landmark.x * resolution_wh[0],\n                landmark.y * resolution_wh[1],\n            ]\n            prediction_xy.append(keypoint_xy)\n            prediction_confidence.append(landmark.visibility)\n\n        xy.append(prediction_xy)\n        confidence.append(prediction_confidence)\n\n    return cls(\n        xy=np.array(xy, dtype=np.float32),\n        confidence=np.array(confidence, dtype=np.float32),\n    )\n</code></pre>"},{"location":"keypoint/core/#supervision.keypoint.core.KeyPoints.from_ultralytics","title":"<code>from_ultralytics(ultralytics_results)</code>  <code>classmethod</code>","text":"<p>Creates a <code>sv.KeyPoints</code> instance from a YOLOv8 pose inference result.</p> <p>Parameters:</p> Name Type Description Default <code>ultralytics_results</code> <code>Keypoints</code> <p>The output Results instance from YOLOv8</p> required <p>Returns:</p> Type Description <code>KeyPoints</code> <p>A <code>sv.KeyPoints</code> object containing the keypoint coordinates, class IDs, and class names, and confidences of each keypoint.</p> <p>Examples:</p> <pre><code>import cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nmodel = YOLO('yolov8s-pose.pt')\n\nresult = model(image)[0]\nkey_points = sv.KeyPoints.from_ultralytics(result)\n</code></pre> Source code in <code>supervision/keypoint/core.py</code> <pre><code>@classmethod\ndef from_ultralytics(cls, ultralytics_results) -&gt; KeyPoints:\n    \"\"\"\n    Creates a `sv.KeyPoints` instance from a\n    [YOLOv8](https://github.com/ultralytics/ultralytics) pose inference result.\n\n    Args:\n        ultralytics_results (ultralytics.engine.results.Keypoints):\n            The output Results instance from YOLOv8\n\n    Returns:\n        A `sv.KeyPoints` object containing the keypoint coordinates, class IDs,\n            and class names, and confidences of each keypoint.\n\n    Examples:\n        ```python\n        import cv2\n        import supervision as sv\n        from ultralytics import YOLO\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        model = YOLO('yolov8s-pose.pt')\n\n        result = model(image)[0]\n        key_points = sv.KeyPoints.from_ultralytics(result)\n        ```\n    \"\"\"\n    if ultralytics_results.keypoints.xy.numel() == 0:\n        return cls.empty()\n\n    xy = ultralytics_results.keypoints.xy.cpu().numpy()\n    class_id = ultralytics_results.boxes.cls.cpu().numpy().astype(int)\n    class_names = np.array([ultralytics_results.names[i] for i in class_id])\n\n    confidence = ultralytics_results.keypoints.conf.cpu().numpy()\n    data = {CLASS_NAME_DATA_FIELD: class_names}\n    return cls(xy, class_id, confidence, data)\n</code></pre>"},{"location":"keypoint/core/#supervision.keypoint.core.KeyPoints.from_yolo_nas","title":"<code>from_yolo_nas(yolo_nas_results)</code>  <code>classmethod</code>","text":"<p>Create a <code>sv.KeyPoints</code> instance from a YOLO-NAS pose inference results.</p> <p>Parameters:</p> Name Type Description Default <code>yolo_nas_results</code> <code>ImagePoseEstimationPrediction</code> <p>The output object from YOLO NAS.</p> required <p>Returns:</p> Type Description <code>KeyPoints</code> <p>A <code>sv.KeyPoints</code> object containing the keypoint coordinates, class IDs, and class names, and confidences of each keypoint.</p> <p>Examples:</p> <pre><code>import cv2\nimport torch\nimport supervision as sv\nimport super_gradients\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = super_gradients.training.models.get(\n    \"yolo_nas_pose_s\", pretrained_weights=\"coco_pose\").to(device)\n\nresults = model.predict(image, conf=0.1)\nkey_points = sv.KeyPoints.from_yolo_nas(results)\n</code></pre> Source code in <code>supervision/keypoint/core.py</code> <pre><code>@classmethod\ndef from_yolo_nas(cls, yolo_nas_results) -&gt; KeyPoints:\n    \"\"\"\n    Create a `sv.KeyPoints` instance from a [YOLO-NAS](https://github.com/Deci-AI/super-gradients/blob/master/YOLONAS-POSE.md)\n    pose inference results.\n\n    Args:\n        yolo_nas_results (ImagePoseEstimationPrediction): The output object from\n            YOLO NAS.\n\n    Returns:\n        A `sv.KeyPoints` object containing the keypoint coordinates, class IDs,\n            and class names, and confidences of each keypoint.\n\n    Examples:\n        ```python\n        import cv2\n        import torch\n        import supervision as sv\n        import super_gradients\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        model = super_gradients.training.models.get(\n            \"yolo_nas_pose_s\", pretrained_weights=\"coco_pose\").to(device)\n\n        results = model.predict(image, conf=0.1)\n        key_points = sv.KeyPoints.from_yolo_nas(results)\n        ```\n    \"\"\"  # noqa: E501 // docs\n    if len(yolo_nas_results.prediction.poses) == 0:\n        return cls.empty()\n\n    xy = yolo_nas_results.prediction.poses[:, :, :2]\n    confidence = yolo_nas_results.prediction.poses[:, :, 2]\n\n    # yolo_nas_results treats params differently.\n    # prediction.labels may not exist, whereas class_names might be None\n    if hasattr(yolo_nas_results.prediction, \"labels\"):\n        class_id = yolo_nas_results.prediction.labels  # np.array[int]\n    else:\n        class_id = None\n\n    data = {}\n    if class_id is not None and yolo_nas_results.class_names is not None:\n        class_names = []\n        for c_id in class_id:\n            name = yolo_nas_results.class_names[c_id]  # tuple[str]\n            class_names.append(name)\n        data[CLASS_NAME_DATA_FIELD] = class_names\n\n    return cls(\n        xy=xy,\n        confidence=confidence,\n        class_id=class_id,\n        data=data,\n    )\n</code></pre>"},{"location":"notebooks/annotate-video-with-detections/","title":"Annotate Video with Detections","text":"In\u00a0[1]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi <pre>Fri Feb 23 03:15:00 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla V100-SXM2-16GB           Off | 00000000:00:04.0 Off |                    0 |\n| N/A   33C    P0              24W / 300W |      0MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n</pre> In\u00a0[\u00a0]: Copied! <pre>!pip install -q inference-gpu \"supervision[assets]\"\n</pre> !pip install -q inference-gpu \"supervision[assets]\" In\u00a0[\u00a0]: Copied! <pre>from supervision.assets import download_assets, VideoAssets\n\n# Download a supervision video asset\npath_to_video = download_assets(VideoAssets.PEOPLE_WALKING)\n</pre> from supervision.assets import download_assets, VideoAssets  # Download a supervision video asset path_to_video = download_assets(VideoAssets.PEOPLE_WALKING) <p>As a result, we've downloaded a video. Let's take a look at the video below.  Keep in mind that the video preview below works only in the web version of the cookbooks and not in Google Colab.</p> In\u00a0[4]: Copied! <pre>import supervision as sv\nfrom supervision.assets import download_assets, VideoAssets\nfrom inference.models.utils import get_roboflow_model\n\n\n# Load a yolov8 model from roboflow.\nmodel = get_roboflow_model(\"yolov8s-640\")\n\n# Create a frame generator and video info object from supervision utilities.\nframe_generator = sv.get_video_frames_generator(path_to_video)\n\n# Yield a single frame from the generator.\nframe = next(frame_generator)\n\n# Run inference on our frame\nresult = model.infer(frame)[0]\n\n# Parse result into detections data model.\ndetections = sv.Detections.from_inference(result)\n\n# Pretty Print the resulting detections.\nfrom pprint import pprint\npprint(detections)\n</pre> import supervision as sv from supervision.assets import download_assets, VideoAssets from inference.models.utils import get_roboflow_model   # Load a yolov8 model from roboflow. model = get_roboflow_model(\"yolov8s-640\")  # Create a frame generator and video info object from supervision utilities. frame_generator = sv.get_video_frames_generator(path_to_video)  # Yield a single frame from the generator. frame = next(frame_generator)  # Run inference on our frame result = model.infer(frame)[0]  # Parse result into detections data model. detections = sv.Detections.from_inference(result)  # Pretty Print the resulting detections. from pprint import pprint pprint(detections) <pre>Detections(xyxy=array([[1140.,  951., 1245., 1079.],\n       [ 666.,  648.,  745.,  854.],\n       [  34.,  794.,  142.,  990.],\n       [1140.,  505., 1211.,  657.],\n       [ 260.,  438.,  332.,  612.],\n       [1413.,  702., 1523.,  887.],\n       [1462.,  472., 1543.,  643.],\n       [1446.,  318., 1516.,  483.],\n       [ 753.,  451.,  821.,  623.],\n       [ 924.,  172.,  983.,  307.],\n       [1791.,  144., 1852.,  275.],\n       [  93.,  132.,  146.,  251.],\n       [ 708.,  240.,  765.,  388.],\n       [ 200.,   44.,  267.,  161.],\n       [1204.,  131., 1255.,  266.],\n       [ 569.,  267.,  628.,  408.],\n       [1163.,  150., 1210.,  280.],\n       [ 799.,   78.,  847.,  204.],\n       [1690.,  152., 1751.,  283.],\n       [ 344.,  495.,  396.,  641.],\n       [1722.,   77., 1782.,  178.]]),\n           mask=None,\n           confidence=array([0.83215541, 0.80572134, 0.7919845 , 0.7912274 , 0.77121079,\n       0.7599591 , 0.75711554, 0.75494027, 0.73076195, 0.71452248,\n       0.69572842, 0.65269446, 0.63952065, 0.62914598, 0.61361706,\n       0.5968492 , 0.55311316, 0.5470854 , 0.54070991, 0.52209878,\n       0.41217673]),\n           class_id=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n           tracker_id=None,\n           data={'class_name': array(['person', 'person', 'person', 'person', 'person', 'person',\n       'person', 'person', 'person', 'person', 'person', 'person',\n       'person', 'person', 'person', 'person', 'person', 'person',\n       'person', 'person', 'person'], dtype='&lt;U6')})\n</pre> <p>First, we load our model using the method <code>get_roboflow_model()</code>. Notice how we pass in a <code>model_id</code>? We're using an alias here. This is where we can pass in other models from Roboflow Universe like this rock, paper, scissors model utilizing our roboflow api key.</p> <pre><code>model = get_roboflow_mode(\n    model_id=\"rock-paper-scissors-sxsw/11\",\n    api_key=\"roboflow_private_api_key\"\n)\n</code></pre> <p>If you don't have an api key, you can create an free Roboflow account. This model wouldn't be much help with detecting people, but it's a nice exercise to see how our code becomes model agnostic!</p> <p>We then create a <code>frame_generator</code> object and yeild a single frame for inference using <code>next()</code>. We pass our frame to <code>model.infer()</code> to run inference. After, we pass that data into a little helpfer function called <code>sv.Detections.from_inference()</code> to parse it. Lastly we print our detections to show we are in fact detecting a few people in the frame!</p> In\u00a0[5]: Copied! <pre># Create a bounding box annotator object.\nbounding_box = sv.BoundingBoxAnnotator()\n\n# Annotate our frame with detections.\nannotated_frame = bounding_box.annotate(scene=frame.copy(), detections=detections)\n\n# Display the frame.\nsv.plot_image(annotated_frame)\n</pre> # Create a bounding box annotator object. bounding_box = sv.BoundingBoxAnnotator()  # Annotate our frame with detections. annotated_frame = bounding_box.annotate(scene=frame.copy(), detections=detections)  # Display the frame. sv.plot_image(annotated_frame) <p>Notice that we create a <code>box_annoator</code> variable by initalizing a BoundingBoxAnnotator. We can change the color and thickness, but for simplicity we keep the defaults. There are a ton of easy to use annotators available in the Supervision package other than a bounding box that are fun to play with.</p> In\u00a0[\u00a0]: Copied! <pre>from tqdm import tqdm\n\n# Create a video_info object for use in the VideoSink.\nvideo_info = sv.VideoInfo.from_video_path(video_path=path_to_video)\n\n# Create a VideoSink context manager to save our frames.\nwith sv.VideoSink(target_path=\"output.mp4\", video_info=video_info) as sink:\n\n    # Iterate through frames yielded from the frame_generator.\n    for frame in tqdm(frame_generator, total=video_info.total_frames):\n\n        # Run inference on our frame.\n        result = model.infer(frame)[0]\n\n        # Parse the result into the detections data model.\n        detections = sv.Detections.from_inference(result)\n\n        # Apply bounding box to detections on a copy of the frame.\n        annotated_frame = bounding_box.annotate(\n            scene=frame.copy(),\n            detections=detections\n        )\n\n        # Write the annotated frame to the video sink.\n        sink.write_frame(frame=annotated_frame)\n</pre> from tqdm import tqdm  # Create a video_info object for use in the VideoSink. video_info = sv.VideoInfo.from_video_path(video_path=path_to_video)  # Create a VideoSink context manager to save our frames. with sv.VideoSink(target_path=\"output.mp4\", video_info=video_info) as sink:      # Iterate through frames yielded from the frame_generator.     for frame in tqdm(frame_generator, total=video_info.total_frames):          # Run inference on our frame.         result = model.infer(frame)[0]          # Parse the result into the detections data model.         detections = sv.Detections.from_inference(result)          # Apply bounding box to detections on a copy of the frame.         annotated_frame = bounding_box.annotate(             scene=frame.copy(),             detections=detections         )          # Write the annotated frame to the video sink.         sink.write_frame(frame=annotated_frame) <p>In the code above we've created a<code>video_info</code> variable to pass information about the video to our <code>VideoSink</code>. The <code>VideoSink</code> is a cool little context manager that allows us to <code>write_frames()</code> to a video ouput file. We're also optionally using <code>tqdm</code> to display a progress bar with a % complete. We only scratched the surface of all of the customizable Annotators and additional features that Supervision and Inference have to offer. Stay tuned for more cookbooks on how to take advantge of them in your computer vision applications. Happy building! \ud83d\ude80</p>"},{"location":"notebooks/annotate-video-with-detections/#annotate-video-with-detections","title":"Annotate Video with Detections\u00b6","text":"<p>One of the most common requirements of computer vision applications is detecting objects in images and displaying bounding boxes around those objects. In this cookbook we'll walk through the steps on how to utilize the open source Roboflow ecosystem to accomplish this task on a video. Let's dive in!</p>"},{"location":"notebooks/annotate-video-with-detections/#before-you-start","title":"Before you start\u00b6","text":"<p>Let's make sure that we have access to GPU. We can use <code>nvidia-smi</code> command to do that. In case of any problems navigate to <code>Edit</code> -&gt; <code>Notebook settings</code> -&gt; <code>Hardware accelerator</code>, set it to <code>GPU</code>, and then click <code>Save</code>.</p>"},{"location":"notebooks/annotate-video-with-detections/#installing-dependencies","title":"Installing Dependencies\u00b6","text":"<p>In this cookbook we'll be utilizing the open source packages Inference and Supervision to accomplish our goals. Let's get those installed in our notebook with pip.</p>"},{"location":"notebooks/annotate-video-with-detections/#download-a-video-asset","title":"Download a Video Asset\u00b6","text":"<p>First, let's download a video that we can detect objects in. Supervision comes with a great utility called Assets to help us hit the ground running. When we run this script, the video is saved in our local directory and can be accessed with the variable <code>path_to_video</code>.</p>"},{"location":"notebooks/annotate-video-with-detections/#detecting-objects","title":"Detecting Objects\u00b6","text":"<p>For this example, the objects in the video that we'd like to detect are people. In order to display bounding boxes around the people in the video, we first need a way to detect them. We'll be using the open source Inference package for this task. Inference allows us to quickly use thousands of models, including fine tuned models from Roboflow Universe, with a few lines of code.  We'll also utilize a few utilities for working with our video data from the Supervision package.</p>"},{"location":"notebooks/annotate-video-with-detections/#annotaing-the-frame-with-bounding-boxes","title":"Annotaing the Frame with Bounding Boxes\u00b6","text":"<p>Now that we're detecting images, let's get to the fun part. Let's annotate the frame and display the bounding boxes on the frame.</p>"},{"location":"notebooks/annotate-video-with-detections/#saving-bounding-boxes-to-a-video","title":"Saving Bounding Boxes to a Video\u00b6","text":"<p>Let's wrap up our code by utilizing a <code>VideoSink</code> to draw bounding boxes and save the resulting video. Take a peak at the final code example below. This can take a couple minutes deppending on your runtime and since since we're processing a full video. Feel free to skip ahead to see the resulting video.</p>"},{"location":"notebooks/count-objects-crossing-the-line/","title":"Count Objects Crossing the Line","text":"<p>Click the <code>Open in Colab</code> button to run the cookbook on Google Colab.</p> In\u00a0[1]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi <pre>Mon Feb 12 13:03:38 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   48C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n</pre> <p>NOTE: To make it easier for us to manage datasets, images and models we create a <code>HOME</code> constant.</p> In\u00a0[2]: Copied! <pre>import os\nHOME = os.getcwd()\nprint(HOME)\n</pre> import os HOME = os.getcwd() print(HOME) <pre>/content\n</pre> In\u00a0[\u00a0]: Copied! <pre>!pip install -q ultralytics supervision==0.18.0\n</pre> !pip install -q ultralytics supervision==0.18.0 In\u00a0[4]: Copied! <pre>import numpy as np\nimport supervision as sv\n\nfrom ultralytics import YOLO\nfrom supervision.assets import download_assets, VideoAssets\n</pre> import numpy as np import supervision as sv  from ultralytics import YOLO from supervision.assets import download_assets, VideoAssets <p>As an example input video, we will use one of the videos available in <code>supervision.assets</code>. Supervision offers an assets download utility that allows you to download video files that you can use in your demos.</p> In\u00a0[\u00a0]: Copied! <pre>download_assets(VideoAssets.VEHICLES)\n</pre> download_assets(VideoAssets.VEHICLES) <p>NOTE: If you want to run the cookbook using your own file as input, simply upload video to Google Colab and replace <code>SOURCE_VIDEO_PATH</code> with the path to your file.</p> In\u00a0[6]: Copied! <pre>SOURCE_VIDEO_PATH = f\"{HOME}/vehicles.mp4\"\n</pre> SOURCE_VIDEO_PATH = f\"{HOME}/vehicles.mp4\" <p>As a result of executing the above commands, you will download a video file and save it at the <code>SOURCE_VIDEO_PATH</code>. Keep in mind that the video preview below works only in the web version of the cookbooks and not in Google Colab.</p> <p>The <code>get_video_frames_generator</code> enables us to easily iterate over video frames. Let's create a video generator for our sample input file and display its first frame on the screen.</p> In\u00a0[7]: Copied! <pre>generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\nframe = next(generator)\n\nsv.plot_image(frame, (12, 12))\n</pre> generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH) frame = next(generator)  sv.plot_image(frame, (12, 12)) <p>We can also use <code>VideoInfo.from_video_path</code> to learn basic information about our video, such as duration, resolution, or FPS.</p> In\u00a0[8]: Copied! <pre>sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n</pre> sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH) Out[8]: <pre>VideoInfo(width=3840, height=2160, fps=25, total_frames=538)</pre> <p>We initiate the model and perform detection on the first frame of the video. Then, we convert the result into a <code>sv.Detections</code> object, which will be useful in the later parts of the cookbook.</p> In\u00a0[9]: Copied! <pre>model = YOLO(\"yolov8x.pt\")\n\nresults = model(frame, verbose=False)[0]\ndetections = sv.Detections.from_ultralytics(results)\n</pre> model = YOLO(\"yolov8x.pt\")  results = model(frame, verbose=False)[0] detections = sv.Detections.from_ultralytics(results) <pre>Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8x.pt to 'yolov8x.pt'...\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 131M/131M [00:00&lt;00:00, 241MB/s]\n</pre> <p>The results we've obtained can be easily visualized with <code>sv.BoundingBoxAnnotator</code>. By default, this annotator uses the same color to highlight objects of the same category. However, with the integration of a tracker, it becomes possible to assign unique colors to each tracked object. We can easily define our own color palettes and adjust parameters such as line thickness, allowing for a highly tailored visualization experience.</p> In\u00a0[10]: Copied! <pre>bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=4)\nannotated_frame = bounding_box_annotator.annotate(frame.copy(), detections)\nsv.plot_image(annotated_frame, (12, 12))\n</pre> bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=4) annotated_frame = bounding_box_annotator.annotate(frame.copy(), detections) sv.plot_image(annotated_frame, (12, 12)) <p>Supervision annotators can be easily combined with one another. Let's enhance our visualization by adding <code>sv.LabelAnnotator</code>, which we will use to mark detections with a label indicating their category and confidence level.</p> In\u00a0[12]: Copied! <pre>labels = [\n    f\"{results.names[class_id]} {confidence:0.2f}\"\n    for class_id, confidence\n    in zip(detections.class_id, detections.confidence)\n]\n\nbounding_box_annotator = sv.BoundingBoxAnnotator(thickness=4)\nlabel_annotator = sv.LabelAnnotator(text_thickness=4, text_scale=2)\n\nannotated_frame = frame.copy()\nannotated_frame = bounding_box_annotator.annotate(annotated_frame, detections)\nannotated_frame = label_annotator.annotate(annotated_frame, detections, labels)\nsv.plot_image(annotated_frame, (12, 12))\n</pre> labels = [     f\"{results.names[class_id]} {confidence:0.2f}\"     for class_id, confidence     in zip(detections.class_id, detections.confidence) ]  bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=4) label_annotator = sv.LabelAnnotator(text_thickness=4, text_scale=2)  annotated_frame = frame.copy() annotated_frame = bounding_box_annotator.annotate(annotated_frame, detections) annotated_frame = label_annotator.annotate(annotated_frame, detections, labels) sv.plot_image(annotated_frame, (12, 12)) <p>To set the position of <code>sv.LineZone</code>, we need to define the <code>start</code> and <code>end</code> points. The position of each point is defined as a pair of coordinates <code>(x, y)</code>. The origin of the coordinate system is located in the top-left corner of the frame. The <code>x</code> axis runs from left to right, and the <code>y</code> axis runs from top to bottom.</p> <p>I decided to place my line horizontally, at the midpoint of the frame's height. I obtained the full dimensions of the frame using <code>sv.VideoInfo</code>.</p> In\u00a0[14]: Copied! <pre>sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n</pre> sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH) Out[14]: <pre>VideoInfo(width=3840, height=2160, fps=25, total_frames=538)</pre> <p>The line we've created, together with the <code>in_count</code> and <code>out_count</code>, can be elegantly visualized using <code>sv.LineZoneAnnotator</code>. This tool also allows for extensive customization options; we can alter the color of both the line and the text, opt to hide the in/out counts, and adjust the labels. By default, the labels are set to <code>in</code> and <code>out</code>, but they can be tailored to fit the context of our project, providing a clear and intuitive display of object movement across the designated line.</p> In\u00a0[22]: Copied! <pre>START = sv.Point(0, 1500)\nEND = sv.Point(3840, 1500)\n\nline_zone = sv.LineZone(start=START, end=END)\n\nline_zone_annotator = sv.LineZoneAnnotator(\n    thickness=4,\n    text_thickness=4,\n    text_scale=2)\n\nannotated_frame = frame.copy()\nannotated_frame = line_zone_annotator.annotate(annotated_frame, line_counter=line_zone)\nsv.plot_image(annotated_frame, (12, 12))\n</pre> START = sv.Point(0, 1500) END = sv.Point(3840, 1500)  line_zone = sv.LineZone(start=START, end=END)  line_zone_annotator = sv.LineZoneAnnotator(     thickness=4,     text_thickness=4,     text_scale=2)  annotated_frame = frame.copy() annotated_frame = line_zone_annotator.annotate(annotated_frame, line_counter=line_zone) sv.plot_image(annotated_frame, (12, 12)) In\u00a0[18]: Copied! <pre>byte_tracker = sv.ByteTrack()\n</pre> byte_tracker = sv.ByteTrack() <p>For an even better visualization, we will add another annotator - <code>sv.TraceAnnotator</code>, which allows for drawing the path traversed by each object over the last few frames. We will use it in combination with <code>sv.BoundingBoxAnnotator</code> and <code>sv.LabelAnnotator</code>, which we became familiar with earlier.</p> In\u00a0[17]: Copied! <pre>bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=4)\nlabel_annotator = sv.LabelAnnotator(text_thickness=4, text_scale=2)\ntrace_annotator = sv.TraceAnnotator(thickness=4)\n</pre> bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=4) label_annotator = sv.LabelAnnotator(text_thickness=4, text_scale=2) trace_annotator = sv.TraceAnnotator(thickness=4) <p>All the operations we plan to perform for each frame of our video - detection, tracking, annotation, and counting - are encapsulated in a function named <code>callback</code>.</p> In\u00a0[19]: Copied! <pre>def callback(frame: np.ndarray, index:int) -&gt; np.ndarray:\n    results = model(frame, verbose=False)[0]\n    detections = sv.Detections.from_ultralytics(results)\n    detections = byte_tracker.update_with_detections(detections)\n\n    labels = [\n        f\"#{tracker_id} {model.model.names[class_id]} {confidence:0.2f}\"\n        for confidence, class_id, tracker_id\n        in zip(detections.confidence, detections.class_id, detections.tracker_id)\n    ]\n\n    annotated_frame = frame.copy()\n    annotated_frame = trace_annotator.annotate(\n        scene=annotated_frame,\n        detections=detections)\n    annotated_frame = bounding_box_annotator.annotate(\n        scene=annotated_frame,\n        detections=detections)\n    annotated_frame = label_annotator.annotate(\n        scene=annotated_frame,\n        detections=detections,\n        labels=labels)\n\n    line_zone.trigger(detections)\n\n    return  line_zone_annotator.annotate(annotated_frame, line_counter=line_zone)\n</pre> def callback(frame: np.ndarray, index:int) -&gt; np.ndarray:     results = model(frame, verbose=False)[0]     detections = sv.Detections.from_ultralytics(results)     detections = byte_tracker.update_with_detections(detections)      labels = [         f\"#{tracker_id} {model.model.names[class_id]} {confidence:0.2f}\"         for confidence, class_id, tracker_id         in zip(detections.confidence, detections.class_id, detections.tracker_id)     ]      annotated_frame = frame.copy()     annotated_frame = trace_annotator.annotate(         scene=annotated_frame,         detections=detections)     annotated_frame = bounding_box_annotator.annotate(         scene=annotated_frame,         detections=detections)     annotated_frame = label_annotator.annotate(         scene=annotated_frame,         detections=detections,         labels=labels)      line_zone.trigger(detections)      return  line_zone_annotator.annotate(annotated_frame, line_counter=line_zone) <p>Finally, we are ready to process our entire video. We will use <code>sv.process_video</code> and pass to it the previously defined <code>SOURCE_VIDEO_PATH</code>, <code>TARGET_VIDEO_PATH</code>, and <code>callback</code>.</p> In\u00a0[21]: Copied! <pre>TARGET_VIDEO_PATH = f\"{HOME}/count-objects-crossing-the-line-result.mp4\"\n</pre> TARGET_VIDEO_PATH = f\"{HOME}/count-objects-crossing-the-line-result.mp4\" In\u00a0[20]: Copied! <pre>sv.process_video(\n    source_path = SOURCE_VIDEO_PATH,\n    target_path = TARGET_VIDEO_PATH,\n    callback=callback\n)\n</pre> sv.process_video(     source_path = SOURCE_VIDEO_PATH,     target_path = TARGET_VIDEO_PATH,     callback=callback ) <p>Keep in mind that the video preview below works only in the web version of the cookbooks and not in Google Colab.</p>"},{"location":"notebooks/count-objects-crossing-the-line/#count-objects-crossing-the-line","title":"Count Objects Crossing the Line\u00b6","text":""},{"location":"notebooks/count-objects-crossing-the-line/#before-you-start","title":"Before you start\u00b6","text":"<p>Let's make sure that we have access to GPU. We can use <code>nvidia-smi</code> command to do that. In case of any problems navigate to <code>Edit</code> -&gt; <code>Notebook settings</code> -&gt; <code>Hardware accelerator</code>, set it to <code>GPU</code>, and then click <code>Save</code>.</p>"},{"location":"notebooks/count-objects-crossing-the-line/#install-required-packages","title":"Install required packages\u00b6","text":"<p>In this cookbook, we'll leverage two Python packages - <code>ultralytics</code> for running object detection, and <code>supervision</code> for tracking, visualizing detections, and crucially, counting objects that cross a line.</p>"},{"location":"notebooks/count-objects-crossing-the-line/#imports","title":"Imports\u00b6","text":""},{"location":"notebooks/count-objects-crossing-the-line/#download-video","title":"Download video\u00b6","text":""},{"location":"notebooks/count-objects-crossing-the-line/#read-single-frame-from-video","title":"Read single frame from video\u00b6","text":""},{"location":"notebooks/count-objects-crossing-the-line/#run-object-detection","title":"Run Object Detection\u00b6","text":"<p>Let's start by running the detection model on the first frame and annotating the results. In this cookbook, we use Ultralytics YOLOv8, but it can be successfully replaced with other models.</p>"},{"location":"notebooks/count-objects-crossing-the-line/#improve-vizualizations","title":"Improve Vizualizations\u00b6","text":""},{"location":"notebooks/count-objects-crossing-the-line/#define-line-position","title":"Define Line Position\u00b6","text":""},{"location":"notebooks/count-objects-crossing-the-line/#process-video","title":"Process Video\u00b6","text":""},{"location":"notebooks/download-supervision-assets/","title":"Download Supervision Assets","text":"In\u00a0[10]: Copied! <pre>!pip install -q \"supervision[assets]\"\n</pre> !pip install -q \"supervision[assets]\" In\u00a0[\u00a0]: Copied! <pre>from supervision.assets import download_assets, VideoAssets\n\n# Download the a video of the subway.\npath_to_video = download_assets(VideoAssets.SUBWAY)\n</pre> from supervision.assets import download_assets, VideoAssets  # Download the a video of the subway. path_to_video = download_assets(VideoAssets.SUBWAY) <p>With this, we now have a high quality video asset for use in demos. Let's take a look at what we downloaded. Keep in mind that the video preview below works only in the web version of the cookbooks and not in Google Colab.</p> <p>We're now equipt with a video asset from Supervision to run some experiments on! For more information on available assets, visit the Supervision API Reference. Happy building!</p>"},{"location":"notebooks/download-supervision-assets/#download-supervision-assets","title":"Download Supervision Assets\u00b6","text":"<p>When experimenting with interesting and useful features of the Supervision package, it's important to have some sort of image or video data to experiment with. Luckily for us, Supervision ships with Assets! Assets is a collection of videos that you can utilize to start experimenting with the various features Supervision has to offer. Let's take a look at how to use this resource.</p>"},{"location":"notebooks/download-supervision-assets/#install-dependencies","title":"Install Dependencies\u00b6","text":""},{"location":"notebooks/download-supervision-assets/#download-a-video","title":"Download a Video\u00b6","text":"<p>From here we can download and utilize a video asset directly from a python script! Note below that we're utilizing the method <code>download_assets</code> to download the <code>VideoAssets.SUBWAY</code> video to our local directory. This method returns the file path, so we can then utilize this path for additional experimentation. From here, you will see a video asset to experiment with in your local directory.</p>"},{"location":"notebooks/evaluating-alignment-of-text-to-image-diffusion-models/","title":"Evaluating Alignment of Text-to-image Diffusion Models","text":"In\u00a0[1]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi <pre>Thu Feb 29 18:16:26 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   46C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n</pre> In\u00a0[\u00a0]: Copied! <pre>!pip install -q torch diffusers accelerate inference-gpu[yolo-world] dill git+https://github.com/openai/CLIP.git supervision==0.19.0rc5\n</pre> !pip install -q torch diffusers accelerate inference-gpu[yolo-world] dill git+https://github.com/openai/CLIP.git supervision==0.19.0rc5 In\u00a0[\u00a0]: Copied! <pre>import itertools\nimport cv2\nfrom diffusers import StableDiffusionXLPipeline\nimport numpy as np\nfrom PIL import Image\nimport supervision as sv\nimport torch\nfrom inference.models import YOLOWorld\n</pre> import itertools import cv2 from diffusers import StableDiffusionXLPipeline import numpy as np from PIL import Image import supervision as sv import torch from inference.models import YOLOWorld In\u00a0[\u00a0]: Copied! <pre>pipeline = StableDiffusionXLPipeline.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    torch_dtype=torch.float16,\n    variant=\"fp16\",\n    use_safetensors=True,\n).to(\"cuda\")\n</pre> pipeline = StableDiffusionXLPipeline.from_pretrained(     \"stabilityai/stable-diffusion-xl-base-1.0\",     torch_dtype=torch.float16,     variant=\"fp16\",     use_safetensors=True, ).to(\"cuda\") <p>In this example, we'll focus on generating an image of a black cat playing with a blue ball next to a parked white car. We don't care about the aesthetic aspect of the image.</p> In\u00a0[41]: Copied! <pre>PROMPT = \"a black cat playing with a blue ball next to a parked white car, wide angle, photorealistic\"\nNEGATIVE_PROMPT = \"low quality, blurred, text, illustration\"\nWIDTH, HEIGHT = 1024, 768\nSEED = 9213799\n\nimage = pipeline(\n    prompt=PROMPT,\n    negative_prompt=NEGATIVE_PROMPT,\n    generator=torch.manual_seed(SEED),\n    width=WIDTH,\n    height=HEIGHT,\n).images[0]\nimage\n</pre> PROMPT = \"a black cat playing with a blue ball next to a parked white car, wide angle, photorealistic\" NEGATIVE_PROMPT = \"low quality, blurred, text, illustration\" WIDTH, HEIGHT = 1024, 768 SEED = 9213799  image = pipeline(     prompt=PROMPT,     negative_prompt=NEGATIVE_PROMPT,     generator=torch.manual_seed(SEED),     width=WIDTH,     height=HEIGHT, ).images[0] image <pre>  0%|          | 0/50 [00:00&lt;?, ?it/s]</pre> Out[41]: <p>Not bad! The results seem to be well-aligned with the prompt.</p> In\u00a0[42]: Copied! <pre>model = YOLOWorld(model_id=\"yolo_world/l\")\n</pre> model = YOLOWorld(model_id=\"yolo_world/l\") <p>YOLO-World model allows us to define our own set of labels. Let's create it by combining lists of pre-defined colors and objects.</p> In\u00a0[43]: Copied! <pre>COLORS = [\"green\", \"yellow\", \"black\", \"blue\", \"red\", \"white\", \"orange\"]\nOBJECTS = [\"car\", \"cat\", \"ball\", \"dog\", \"tree\", \"house\", \"person\"]\nCLASSES = [f\"{color} {obj}\" for color, obj in itertools.product(COLORS, OBJECTS)]\nprint(\"Number of labels:\", len(CLASSES))\n</pre> COLORS = [\"green\", \"yellow\", \"black\", \"blue\", \"red\", \"white\", \"orange\"] OBJECTS = [\"car\", \"cat\", \"ball\", \"dog\", \"tree\", \"house\", \"person\"] CLASSES = [f\"{color} {obj}\" for color, obj in itertools.product(COLORS, OBJECTS)] print(\"Number of labels:\", len(CLASSES)) <pre>Number of labels: 49\n</pre> <p>Let's feed these labels into our model:</p> In\u00a0[44]: Copied! <pre>model.set_classes(CLASSES)\n</pre> model.set_classes(CLASSES) <p>Time to detect some objects!</p> In\u00a0[45]: Copied! <pre>results = model.infer(image)\n</pre> results = model.infer(image) <p>We'll convert the results to the sv.Detections format to enable features like filtering or annotations.</p> In\u00a0[46]: Copied! <pre>detections = sv.Detections.from_inference(results)\n</pre> detections = sv.Detections.from_inference(results) <p>Speaking of which: we only care about strong detections, so we filter out ones that are below 0.6 confidence.</p> In\u00a0[47]: Copied! <pre>valid_detections = detections[detections.confidence &gt;= 0.6]\n</pre> valid_detections = detections[detections.confidence &gt;= 0.6] <p>A quick peek on the detected labels and their score:</p> In\u00a0[48]: Copied! <pre>labels = [\n    f\"{CLASSES[class_id]} {confidence:0.2f}\"\n    for class_id, confidence\n    in zip(valid_detections.class_id, valid_detections.confidence)\n]\nlabels\n</pre> labels = [     f\"{CLASSES[class_id]} {confidence:0.2f}\"     for class_id, confidence     in zip(valid_detections.class_id, valid_detections.confidence) ] labels Out[48]: <pre>['blue ball 0.95', 'black cat 0.72', 'white car 0.68']</pre> In\u00a0[49]: Copied! <pre>bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=2)\nlabel_annotator = sv.LabelAnnotator(text_thickness=1, text_scale=0.5,text_color=sv.Color.BLACK)\n</pre> bounding_box_annotator = sv.BoundingBoxAnnotator(thickness=2) label_annotator = sv.LabelAnnotator(text_thickness=1, text_scale=0.5,text_color=sv.Color.BLACK) <p>Finally, annotating our image is as simple as calling <code>annotate</code> methods from our annotators:</p> In\u00a0[50]: Copied! <pre>annotated_image = bounding_box_annotator.annotate(image, valid_detections)\nannotated_image = label_annotator.annotate(annotated_image, valid_detections, labels)\n\nsv.plot_image(annotated_image, (12, 12))\n</pre> annotated_image = bounding_box_annotator.annotate(image, valid_detections) annotated_image = label_annotator.annotate(annotated_image, valid_detections, labels)  sv.plot_image(annotated_image, (12, 12)) In\u00a0[51]: Copied! <pre>GROUND_TRUTH = {\"black cat\", \"blue ball\", \"white car\"}\nprediction = {CLASSES[class_id] for class_id in valid_detections.class_id}\n\nprediction.issubset(GROUND_TRUTH)\n</pre> GROUND_TRUTH = {\"black cat\", \"blue ball\", \"white car\"} prediction = {CLASSES[class_id] for class_id in valid_detections.class_id}  prediction.issubset(GROUND_TRUTH) Out[51]: <pre>True</pre> <p>Using sv.Detections makes it super easy to do.</p>"},{"location":"notebooks/evaluating-alignment-of-text-to-image-diffusion-models/#evaluating-alignment-of-text-to-image-diffusion-models","title":"Evaluating Alignment of Text-to-image Diffusion Models\u00b6","text":"<p>Click the <code>Open in Colab</code> button to run the cookbook on Google Colab.</p>"},{"location":"notebooks/evaluating-alignment-of-text-to-image-diffusion-models/#introduction","title":"Introduction\u00b6","text":"<p>It is a common scenario to evaluate text-to-image models for its alignment to the prompt. One way to test it is to use a set of prompts, consisting of number of objects and their basic physical properties (e.g. color), to generate images and manually evaluate the results. This process can be greatly improved using object detection models.</p>"},{"location":"notebooks/evaluating-alignment-of-text-to-image-diffusion-models/#before-you-start","title":"Before you start\u00b6","text":"<p>Let's make sure that we have access to GPU. We can use <code>nvidia-smi</code> command to do that. In case of any problems navigate to <code>Edit</code> -&gt; <code>Notebook settings</code> -&gt; <code>Hardware accelerator</code>, set it to <code>GPU</code>, and then click <code>Save</code>.</p>"},{"location":"notebooks/evaluating-alignment-of-text-to-image-diffusion-models/#install-required-packages","title":"Install required packages\u00b6","text":"<p>In this cookbook, we'll leverage the following Python packages:</p> <ul> <li>diffusers for image generation pipelines,</li> <li>inference for running object detection,</li> <li>supervision for visualizing detections.</li> </ul>"},{"location":"notebooks/evaluating-alignment-of-text-to-image-diffusion-models/#imports","title":"Imports\u00b6","text":""},{"location":"notebooks/evaluating-alignment-of-text-to-image-diffusion-models/#generating-an-image","title":"Generating an image\u00b6","text":"<p>We'll use SDXL model to generate our image. Let's initialize our pipeline first:</p>"},{"location":"notebooks/evaluating-alignment-of-text-to-image-diffusion-models/#detecting-objects","title":"Detecting objects\u00b6","text":"<p>Now, let's see how can we detect the objects automatically. For this, we'll use YOLO-World model from <code>inference</code> library.</p>"},{"location":"notebooks/evaluating-alignment-of-text-to-image-diffusion-models/#visualizing-results","title":"Visualizing results\u00b6","text":"<p>Now, let's use the power of <code>supervision</code> to visualize them. Our output image is in Pillow format, but annotators can accept the image to be a BGR <code>np.ndarray</code> or pillow's <code>PIL.Image.Image</code>.</p> <p>Time to define how we want our detections to be visualized. A combination of sv.BoundingBoxAnnotator and sv.LabelAnnotator should be perfect.</p>"},{"location":"notebooks/evaluating-alignment-of-text-to-image-diffusion-models/#testing-it-automatically","title":"Testing it automatically\u00b6","text":"<p>We can also test if all requested objects are in the generated image by comparing a set of ground-truth labels with predicted ones:</p>"},{"location":"notebooks/evaluating-alignment-of-text-to-image-diffusion-models/#next-steps","title":"Next steps\u00b6","text":"<p>In this tutorial you learned how to detect and visualize objects for a simple image generation evaluation study.</p> <p>Having a pipeline capable of evaluating a single image, the natural next step should be to run it on a set of pre-defined scenarios and calculate metrics.</p>"},{"location":"notebooks/object-tracking/","title":"Object Tracking","text":"In\u00a0[1]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi <pre>Fri Feb 23 03:18:02 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla V100-SXM2-16GB           Off | 00000000:00:04.0 Off |                    0 |\n| N/A   33C    P0              24W / 300W |      0MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n</pre> In\u00a0[\u00a0]: Copied! <pre>!pip install -q inference-gpu \"supervision[assets]\"\n</pre> !pip install -q inference-gpu \"supervision[assets]\" In\u00a0[\u00a0]: Copied! <pre>from supervision.assets import download_assets, VideoAssets\n\n# Download a supervision video asset\npath_to_video = download_assets(VideoAssets.PEOPLE_WALKING)\n</pre> from supervision.assets import download_assets, VideoAssets  # Download a supervision video asset path_to_video = download_assets(VideoAssets.PEOPLE_WALKING) In\u00a0[4]: Copied! <pre>import supervision as sv\nfrom inference.models.utils import get_roboflow_model\n\n# Load a pre trained yolov8 nano model from Roboflow Inference.\nmodel = get_roboflow_model('yolov8n-640')\n\n# Create a video info object from the video path.\nvideo_info = sv.VideoInfo.from_video_path(path_to_video)\n\n# Create a label annotator for labeling detections with our tracker_id.\nlabel = sv.LabelAnnotator()\n\n# Create a ByteTrack object to track detections.\nbyte_tracker = sv.ByteTrack(frame_rate=video_info.fps)\n\n# Create a frame generator from video path for iteration of frames.\nframe_generator = sv.get_video_frames_generator(path_to_video)\n\n# Grab a frame from the frame_generator.\nframe = next(frame_generator)\n\n# Run inference on the frame by passing it to our model.\nresult = model.infer(frame)[0]\n\n# Convert model results to a supervision detection object.\ndetections = sv.Detections.from_inference(result)\n\n# Update detections with tracker ids fro byte_tracker.\ntracked_detections = byte_tracker.update_with_detections(detections)\n\n# Create labels with tracker_id for label annotator.\nlabels = [ f\"{tracker_id}\" for tracker_id in tracked_detections.tracker_id ]\n\n# Apply label annotator to frame.\nannotated_frame = label.annotate(scene=frame.copy(), detections=tracked_detections, labels=labels)\n\n# Display the frame.\nsv.plot_image(annotated_frame)\n</pre> import supervision as sv from inference.models.utils import get_roboflow_model  # Load a pre trained yolov8 nano model from Roboflow Inference. model = get_roboflow_model('yolov8n-640')  # Create a video info object from the video path. video_info = sv.VideoInfo.from_video_path(path_to_video)  # Create a label annotator for labeling detections with our tracker_id. label = sv.LabelAnnotator()  # Create a ByteTrack object to track detections. byte_tracker = sv.ByteTrack(frame_rate=video_info.fps)  # Create a frame generator from video path for iteration of frames. frame_generator = sv.get_video_frames_generator(path_to_video)  # Grab a frame from the frame_generator. frame = next(frame_generator)  # Run inference on the frame by passing it to our model. result = model.infer(frame)[0]  # Convert model results to a supervision detection object. detections = sv.Detections.from_inference(result)  # Update detections with tracker ids fro byte_tracker. tracked_detections = byte_tracker.update_with_detections(detections)  # Create labels with tracker_id for label annotator. labels = [ f\"{tracker_id}\" for tracker_id in tracked_detections.tracker_id ]  # Apply label annotator to frame. annotated_frame = label.annotate(scene=frame.copy(), detections=tracked_detections, labels=labels)  # Display the frame. sv.plot_image(annotated_frame) In\u00a0[\u00a0]: Copied! <pre>from tqdm import tqdm\n\n# Load a pre trained yolov8 nano model from Roboflow Inference.\nmodel = get_roboflow_model('yolov8n-640')\n\n# Create a video info object from the video path.\nvideo_info = sv.VideoInfo.from_video_path(path_to_video)\n\n# Create a label annotator for labeling detections with our tracker_id.\nlabel = sv.LabelAnnotator()\n\n# Create a ByteTrack object to track detections.\nbyte_tracker = sv.ByteTrack(frame_rate=video_info.fps)\n\n# Create a frame generator from video path for iteration of frames.\nframe_generator = sv.get_video_frames_generator(path_to_video)\n\n# Create a video sink context manager to save resulting video.\nwith sv.VideoSink(target_path=\"output.mp4\", video_info=video_info) as sink:\n\n    # Iterate through frames yielded from the frame_generator.\n    for frame in tqdm(frame_generator, total=video_info.total_frames):\n\n        # Run inference on the frame by passing it to our model.\n        result = model.infer(frame)[0]\n\n        # Convert model results to a supervision detection object.\n        detections = sv.Detections.from_inference(result)\n\n        # Update detections with tracker ids fro byte_tracker.\n        tracked_detections = byte_tracker.update_with_detections(detections)\n\n        # Create labels with tracker_id for label annotator.\n        labels = [ f\"{tracker_id}\" for tracker_id in tracked_detections.tracker_id ]\n\n        # Apply label annotator to frame.\n        annotated_frame = label.annotate(scene=frame.copy(), detections=tracked_detections, labels=labels)\n\n        # Save the annotated frame to an output video.\n        sink.write_frame(frame=annotated_frame)\n</pre> from tqdm import tqdm  # Load a pre trained yolov8 nano model from Roboflow Inference. model = get_roboflow_model('yolov8n-640')  # Create a video info object from the video path. video_info = sv.VideoInfo.from_video_path(path_to_video)  # Create a label annotator for labeling detections with our tracker_id. label = sv.LabelAnnotator()  # Create a ByteTrack object to track detections. byte_tracker = sv.ByteTrack(frame_rate=video_info.fps)  # Create a frame generator from video path for iteration of frames. frame_generator = sv.get_video_frames_generator(path_to_video)  # Create a video sink context manager to save resulting video. with sv.VideoSink(target_path=\"output.mp4\", video_info=video_info) as sink:      # Iterate through frames yielded from the frame_generator.     for frame in tqdm(frame_generator, total=video_info.total_frames):          # Run inference on the frame by passing it to our model.         result = model.infer(frame)[0]          # Convert model results to a supervision detection object.         detections = sv.Detections.from_inference(result)          # Update detections with tracker ids fro byte_tracker.         tracked_detections = byte_tracker.update_with_detections(detections)          # Create labels with tracker_id for label annotator.         labels = [ f\"{tracker_id}\" for tracker_id in tracked_detections.tracker_id ]          # Apply label annotator to frame.         annotated_frame = label.annotate(scene=frame.copy(), detections=tracked_detections, labels=labels)          # Save the annotated frame to an output video.         sink.write_frame(frame=annotated_frame) <p>Let's take a look at our resulting video. It will also be created in your current directory with the name <code>output.mp4</code> Notice how even with a little flicker, we can see the <code>tracker_id</code> on the people walking in the video. With trackers under your belt, there are now a wide variety of use cases you can solve for! Happy building!</p>"},{"location":"notebooks/object-tracking/#object-tracking","title":"Object Tracking\u00b6","text":"<p>In some cases, it's important for us to track objects across multiple frames of a video. For example, we may need to figure out the direction a vehicle is moving, or count objects in a frame. Some Supervision Annotators and Tools like LineZone require tracking to be setup.  In this cookbook, we'll cover how to get a tracker up and running for use in your computer vision applications.</p>"},{"location":"notebooks/object-tracking/#what-is-a-tracker","title":"What is a Tracker?\u00b6","text":"<p>Trackers are a piece of code that identifies objects across frames and assigns them a unique <code>tracker_id</code>. There are a few popular trackers at the time of writing this including ByteTrack and Bot-SORT. Supervision makes using trackers a breeze and comes with ByteTrack built-in.</p>"},{"location":"notebooks/object-tracking/#before-you-start","title":"Before you start\u00b6","text":"<p>Let's make sure that we have access to GPU. We can use <code>nvidia-smi</code> command to do that. In case of any problems navigate to <code>Edit</code> -&gt; <code>Notebook settings</code> -&gt; <code>Hardware accelerator</code>, set it to <code>GPU</code>, and then click <code>Save</code>.</p>"},{"location":"notebooks/object-tracking/#install-dependencies","title":"Install Dependencies\u00b6","text":""},{"location":"notebooks/object-tracking/#download-a-video-asset","title":"Download a Video Asset\u00b6","text":"<p>Now that we have our enviornment setup, lets download a video that we can detect objects in. Supervision comes with a great utility to help us hit the ground running. We can use the below snippet to he video is save a video asset in our local directory. It can also be accessed with the variable <code>path_to_video</code> for additional application logic.</p>"},{"location":"notebooks/object-tracking/#tracking-objects-in-a-frame","title":"Tracking Objects in a Frame\u00b6","text":"<p>Now that we have our video installed, let's get to work on tracking objects. We'll first pull in a model from roboflow Inference to detect people in our video. Then let's create a <code>byte_tracker</code> object that we'll pass our detections to. This will give us a <code>tracker_id</code>. We'll then utilize that tracker id to label our detections with a <code>label_annotator</code> to display the tracker id.</p>"},{"location":"notebooks/object-tracking/#tracking-objects-in-a-video","title":"Tracking Objects in a Video\u00b6","text":"<p>Finally, we'll use a utility called <code>VideoSink</code> to save the annotated frames to a video. Let's dive in to the code.</p>"},{"location":"notebooks/occupancy_analytics/","title":"How To Analyze Occupancy with Supervision","text":"In\u00a0[\u00a0]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi In\u00a0[\u00a0]: Copied! <pre>!pip install roboflow supervision==0.19.0 -q\n</pre> !pip install roboflow supervision==0.19.0 -q In\u00a0[\u00a0]: Copied! <pre>VIDEO_PATH = \"/content/parkinglot1080.mov\"\n</pre> VIDEO_PATH = \"/content/parkinglot1080.mov\" <p>First, let's create a directory to save the video frames</p> In\u00a0[\u00a0]: Copied! <pre>import os\n\nFRAMES_DIR = \"/content/frames\"\nos.mkdir(FRAMES_DIR)\n</pre> import os  FRAMES_DIR = \"/content/frames\" os.mkdir(FRAMES_DIR) <p>Then, we can use Supervision's <code>get_video_frames_generator</code> function to get, then save, our video frames</p> In\u00a0[\u00a0]: Copied! <pre>import supervision as sv\nfrom PIL import Image\n\nframes_generator = sv.get_video_frames_generator(VIDEO_PATH)\n\nfor i, frame in enumerate(frames_generator):\n  img = Image.fromarray(frame)\n  img.save(f\"{FRAMES_DIR}/{i}.jpg\")\n\nprint(f\"Saved frames to {FRAMES_DIR}\")\n</pre> import supervision as sv from PIL import Image  frames_generator = sv.get_video_frames_generator(VIDEO_PATH)  for i, frame in enumerate(frames_generator):   img = Image.fromarray(frame)   img.save(f\"{FRAMES_DIR}/{i}.jpg\")  print(f\"Saved frames to {FRAMES_DIR}\") <pre>Saved frames to /content/frames\n</pre> In\u00a0[\u00a0]: Copied! <pre># Note: This code block was written by ChatGPT\n\nimport os\nimport random\nfrom PIL import Image\nimport numpy as np\n\n# import shutil\n# shutil.rmtree(\"augmented\")\n\ndef random_crop(img):\n    width, height = img.size\n\n    crop_width = random.randint(int(width * 0.1), int(width * 0.4))\n    crop_height = random.randint(int(height * 0.1), int(height * 0.4))\n\n    left = random.randint(0, width - crop_width)\n    top = random.randint(0, height - crop_height)\n\n    return img.crop((left, top, left + crop_width, top + crop_height))\n\ndef augment_images(source_folder, target_folder, num_images=100):\n    if not os.path.exists(target_folder):\n        os.makedirs(target_folder)\n\n    all_images = [file for file in os.listdir(source_folder) if file.endswith('.jpg')]\n\n    selected_images = np.random.choice(all_images, size=min(num_images, len(all_images)), replace=False)\n\n    for i, filename in enumerate(selected_images):\n        with Image.open(os.path.join(source_folder, filename)) as img:\n            cropped_img = random_crop(img)\n            cropped_img.save(os.path.join(target_folder, f'augmented_{i}.jpg'))\n\n# Paths to the source and target folders\nsource_folder = '/content/frames'\ntarget_folder = '/content/augmented'\n\n# Augment images\naugment_images(source_folder, target_folder)\n</pre> # Note: This code block was written by ChatGPT  import os import random from PIL import Image import numpy as np  # import shutil # shutil.rmtree(\"augmented\")  def random_crop(img):     width, height = img.size      crop_width = random.randint(int(width * 0.1), int(width * 0.4))     crop_height = random.randint(int(height * 0.1), int(height * 0.4))      left = random.randint(0, width - crop_width)     top = random.randint(0, height - crop_height)      return img.crop((left, top, left + crop_width, top + crop_height))  def augment_images(source_folder, target_folder, num_images=100):     if not os.path.exists(target_folder):         os.makedirs(target_folder)      all_images = [file for file in os.listdir(source_folder) if file.endswith('.jpg')]      selected_images = np.random.choice(all_images, size=min(num_images, len(all_images)), replace=False)      for i, filename in enumerate(selected_images):         with Image.open(os.path.join(source_folder, filename)) as img:             cropped_img = random_crop(img)             cropped_img.save(os.path.join(target_folder, f'augmented_{i}.jpg'))  # Paths to the source and target folders source_folder = '/content/frames' target_folder = '/content/augmented'  # Augment images augment_images(source_folder, target_folder)  In\u00a0[\u00a0]: Copied! <pre># Upload the extracted frames to Roboflow\nimport os\nimport roboflow\n\nrf = roboflow.Roboflow(api_key=\"YOUR_ROBOFLOW_API_KEY\")\nproject = rf.workspace().project(\"parking-lot-occupancy-detection-eoaek\")\n\nfor filename in os.listdir(FRAMES_DIR):\n  img_path = os.path.join(FRAMES_DIR, filename)\n  if os.path.isfile(img_path):\n      project.upload(image_path=img_path)\n</pre> # Upload the extracted frames to Roboflow import os import roboflow  rf = roboflow.Roboflow(api_key=\"YOUR_ROBOFLOW_API_KEY\") project = rf.workspace().project(\"parking-lot-occupancy-detection-eoaek\")  for filename in os.listdir(FRAMES_DIR):   img_path = os.path.join(FRAMES_DIR, filename)   if os.path.isfile(img_path):       project.upload(image_path=img_path) <pre>loading Roboflow workspace...\nloading Roboflow project...\n</pre> In\u00a0[\u00a0]: Copied! <pre># PASTE CODE FROM ROBOFLOW HERE\n</pre> # PASTE CODE FROM ROBOFLOW HERE In\u00a0[\u00a0]: Copied! <pre>from roboflow import Roboflow\nimport supervision as sv\nimport numpy as np\nimport cv2\n\nrf = Roboflow(api_key=\"YOUR_ROBOFLOW_API_KEY\") # Get your own API key - This one won't work\nproject = rf.workspace().project(\"parking-lot-occupancy-detection-eoaek\")\nmodel = project.version(\"5\").model\n\ndef callback(x: np.ndarray) -&gt; sv.Detections:\n    result = model.predict(x, confidence=25, overlap=30).json()\n    return sv.Detections.from_inference(result)\n</pre> from roboflow import Roboflow import supervision as sv import numpy as np import cv2  rf = Roboflow(api_key=\"YOUR_ROBOFLOW_API_KEY\") # Get your own API key - This one won't work project = rf.workspace().project(\"parking-lot-occupancy-detection-eoaek\") model = project.version(\"5\").model  def callback(x: np.ndarray) -&gt; sv.Detections:     result = model.predict(x, confidence=25, overlap=30).json()     return sv.Detections.from_inference(result) <pre>loading Roboflow workspace...\nloading Roboflow project...\n</pre> In\u00a0[\u00a0]: Copied! <pre># Polygons From PolygonZone\n\nzones = [\n    {\n        'name': \"Zone 1\",\n        'polygon': np.array([[229, 50],[-3, 306],[1, 614],[369, 50]]),\n        'max': 32\n    },\n    {\n        'name': 'Zone 2',\n        'polygon': np.array([[465, 46],[177, 574],[401, 578],[609, 46]]),\n        'max': 38\n    },\n    {\n        'name': 'Zone 3',\n        'polygon': np.array([[697, 58],[461, 858],[737, 858],[849, 58]]),\n        'max': 46\n    },\n    {\n        'name': 'Zone 4',\n        'polygon': np.array([[941, 58],[909, 862],[1273, 858],[1137, 58]]),\n        'max': 48\n    },\n    {\n        'name': 'Zone 5',\n        'polygon': np.array([[1229, 46],[1501, 1078],[1889, 1078],[1405, 46]]),\n        'max': 52\n    }\n]\n</pre> # Polygons From PolygonZone  zones = [     {         'name': \"Zone 1\",         'polygon': np.array([[229, 50],[-3, 306],[1, 614],[369, 50]]),         'max': 32     },     {         'name': 'Zone 2',         'polygon': np.array([[465, 46],[177, 574],[401, 578],[609, 46]]),         'max': 38     },     {         'name': 'Zone 3',         'polygon': np.array([[697, 58],[461, 858],[737, 858],[849, 58]]),         'max': 46     },     {         'name': 'Zone 4',         'polygon': np.array([[941, 58],[909, 862],[1273, 858],[1137, 58]]),         'max': 48     },     {         'name': 'Zone 5',         'polygon': np.array([[1229, 46],[1501, 1078],[1889, 1078],[1405, 46]]),         'max': 52     } ] In\u00a0[\u00a0]: Copied! <pre>tracker = sv.ByteTrack()\nslicer = sv.InferenceSlicer(\n    callback=callback,\n    slice_wh=(800, 800),\n    overlap_ratio_wh=(0.2, 0.2),\n    thread_workers=10,\n    iou_threshold=0.2\n)\ntriangle_annotator = sv.TriangleAnnotator(\n    base=20,\n    height=20\n)\nheat_map_annotator = sv.HeatMapAnnotator()\n\ndef setup_zones(frame_wh):\n  if zones:\n    for zone in zones:\n      zone['history'] = []\n      zone['PolygonZone'] = sv.PolygonZone(\n          polygon=zone['polygon'],\n          frame_resolution_wh=frame_wh\n      )\n      zone['PolygonZoneAnnotator'] = sv.PolygonZoneAnnotator(\n        zone=zone['PolygonZone'],\n        color=sv.Color.WHITE,\n        thickness=4,\n    )\n\ndef process_frame(frame,heatmap=None):\n    detections = slicer(image=frame)\n    detections = tracker.update_with_detections(detections)\n\n    annotated_frame = frame.copy()\n\n    annotated_frame = triangle_annotator.annotate(\n        scene=annotated_frame,\n        detections=detections\n    )\n\n    if heatmap is None:\n      heatmap = np.full(frame.shape, 255, dtype=np.uint8)\n\n    heat_map_annotator.annotate(\n      scene=heatmap,\n      detections=detections\n    )\n\n    if zones:\n      for zone in zones:\n        zone_presence = zone['PolygonZone'].trigger(detections)\n        zone_present_idxs = [idx for idx, present in enumerate(zone_presence) if present]\n        zone_present = detections[zone_present_idxs]\n\n        zone_count = len(zone_present)\n        zone['history'].append(zone_count)\n\n\n        annotated_frame = zone['PolygonZoneAnnotator'].annotate(\n            scene=annotated_frame,\n            label=f\"{zone['name']}: {zone_count}\"\n        )\n\n        # Heatmap\n        heatmap = zone['PolygonZoneAnnotator'].annotate(\n            scene=heatmap,\n            label=\" \"\n        )\n\n    return annotated_frame, heatmap\n</pre> tracker = sv.ByteTrack() slicer = sv.InferenceSlicer(     callback=callback,     slice_wh=(800, 800),     overlap_ratio_wh=(0.2, 0.2),     thread_workers=10,     iou_threshold=0.2 ) triangle_annotator = sv.TriangleAnnotator(     base=20,     height=20 ) heat_map_annotator = sv.HeatMapAnnotator()  def setup_zones(frame_wh):   if zones:     for zone in zones:       zone['history'] = []       zone['PolygonZone'] = sv.PolygonZone(           polygon=zone['polygon'],           frame_resolution_wh=frame_wh       )       zone['PolygonZoneAnnotator'] = sv.PolygonZoneAnnotator(         zone=zone['PolygonZone'],         color=sv.Color.WHITE,         thickness=4,     )  def process_frame(frame,heatmap=None):     detections = slicer(image=frame)     detections = tracker.update_with_detections(detections)      annotated_frame = frame.copy()      annotated_frame = triangle_annotator.annotate(         scene=annotated_frame,         detections=detections     )      if heatmap is None:       heatmap = np.full(frame.shape, 255, dtype=np.uint8)      heat_map_annotator.annotate(       scene=heatmap,       detections=detections     )      if zones:       for zone in zones:         zone_presence = zone['PolygonZone'].trigger(detections)         zone_present_idxs = [idx for idx, present in enumerate(zone_presence) if present]         zone_present = detections[zone_present_idxs]          zone_count = len(zone_present)         zone['history'].append(zone_count)           annotated_frame = zone['PolygonZoneAnnotator'].annotate(             scene=annotated_frame,             label=f\"{zone['name']}: {zone_count}\"         )          # Heatmap         heatmap = zone['PolygonZoneAnnotator'].annotate(             scene=heatmap,             label=\" \"         )      return annotated_frame, heatmap In\u00a0[\u00a0]: Copied! <pre>image = cv2.imread(\"./frames/5.jpg\")\nimage_wh = (image.shape[1],image.shape[0])\nsetup_zones(image_wh)\n\nannotated_image, heatmap = process_frame(image)\n\nsv.plot_image(annotated_image)\nsv.plot_image(heatmap)\n</pre> image = cv2.imread(\"./frames/5.jpg\") image_wh = (image.shape[1],image.shape[0]) setup_zones(image_wh)  annotated_image, heatmap = process_frame(image)  sv.plot_image(annotated_image) sv.plot_image(heatmap) <pre>(1920, 1080)\n</pre> In\u00a0[\u00a0]: Copied! <pre># Credit to https://matplotlib.org/matplotblog/posts/matplotlib-cyberpunk-style/ for graph styles\n%matplotlib agg\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom io import BytesIO\n\ndef generate_graphs(max_frames):\n  plt.ioff()\n  # Plot Styles\n  plt.style.use(\"seaborn-dark\")\n  for param in ['figure.facecolor', 'axes.facecolor', 'savefig.facecolor']:\n      plt.rcParams[param] = '#212946'\n\n  for param in ['text.color', 'axes.labelcolor', 'xtick.color', 'ytick.color']:\n      plt.rcParams[param] = '0.9'\n\n\n  dataframe = pd.DataFrame()\n  graphs = {}\n\n\n  for zone in zones:\n    percentage_history = [(count/zone['max'])*100 for count in zone['history']]\n    dataframe[zone['name']] = percentage_history\n    plt.title(f'{zone[\"name\"]} Usage')\n\n    # Extra Styles\n    fig, ax1 = plt.subplots()\n    ax1.grid(color='#2A3459')\n\n    # Data\n    ax1.plot(zone[\"history\"])\n\n    # Axis Labeling\n    plt.ylabel('Vehicles')\n    plt.ylim(top=zone[\"max\"])\n    plt.xlim(right=max_frames)\n    ax2 = ax1.twinx()\n    ax2.set_ylabel('Occupied Percentage (%)')\n\n    # Export Graph Image\n    buf = BytesIO()\n    fig.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)\n    buf.seek(0)\n    graphs[zone['name']] = Image.open(buf)\n    plt.close(fig)\n\n\n  plt.ioff()\n  dataframe.plot()\n\n  # Axis\n  plt.ylabel('Occupied (%)', fontsize=15)\n  plt.ylim(top=100)\n  plt.xlim(right=max_frames)\n\n  # Export combined\n  buf = BytesIO()\n  plt.savefig(buf, format='png', bbox_inches='tight')\n  buf.seek(0)\n\n  plt.close()\n\n  graphs['combined_percentage'] = Image.open(buf)\n\n  return graphs\n</pre> # Credit to https://matplotlib.org/matplotblog/posts/matplotlib-cyberpunk-style/ for graph styles %matplotlib agg import pandas as pd import matplotlib.pyplot as plt from PIL import Image from io import BytesIO  def generate_graphs(max_frames):   plt.ioff()   # Plot Styles   plt.style.use(\"seaborn-dark\")   for param in ['figure.facecolor', 'axes.facecolor', 'savefig.facecolor']:       plt.rcParams[param] = '#212946'    for param in ['text.color', 'axes.labelcolor', 'xtick.color', 'ytick.color']:       plt.rcParams[param] = '0.9'     dataframe = pd.DataFrame()   graphs = {}     for zone in zones:     percentage_history = [(count/zone['max'])*100 for count in zone['history']]     dataframe[zone['name']] = percentage_history     plt.title(f'{zone[\"name\"]} Usage')      # Extra Styles     fig, ax1 = plt.subplots()     ax1.grid(color='#2A3459')      # Data     ax1.plot(zone[\"history\"])      # Axis Labeling     plt.ylabel('Vehicles')     plt.ylim(top=zone[\"max\"])     plt.xlim(right=max_frames)     ax2 = ax1.twinx()     ax2.set_ylabel('Occupied Percentage (%)')      # Export Graph Image     buf = BytesIO()     fig.savefig(buf, format='png', bbox_inches='tight', pad_inches=0)     buf.seek(0)     graphs[zone['name']] = Image.open(buf)     plt.close(fig)     plt.ioff()   dataframe.plot()    # Axis   plt.ylabel('Occupied (%)', fontsize=15)   plt.ylim(top=100)   plt.xlim(right=max_frames)    # Export combined   buf = BytesIO()   plt.savefig(buf, format='png', bbox_inches='tight')   buf.seek(0)    plt.close()    graphs['combined_percentage'] = Image.open(buf)    return graphs In\u00a0[\u00a0]: Copied! <pre>generate_graphs(400)['combined_percentage']\n</pre> generate_graphs(400)['combined_percentage'] Out[\u00a0]: In\u00a0[\u00a0]: Copied! <pre>VIDEO_PATH = \"/content/parkinglot1080.mov\"\nMAIN_OUTPUT_PATH = \"/content/parkinglot_annotated.mp4\"\nframes_generator = sv.get_video_frames_generator(source_path=VIDEO_PATH)\nvideo_info = sv.VideoInfo.from_video_path(video_path=VIDEO_PATH)\n\nsetup_zones(video_info.resolution_wh)\n\n\nwith sv.VideoSink(target_path=MAIN_OUTPUT_PATH, video_info=video_info) as sink:\n  heatmap = None\n  for i, frame in enumerate(frames_generator):\n    print(f\"Processing frame {i}\")\n\n    # Infer\n    annotated_frame, heatmap = process_frame(frame, heatmap)\n\n    # Save the latest heatmap\n    Image.fromarray(heatmap).save(f\"/content/heatmap/{i}.jpg\")\n\n    # Create Graphs\n    graphs = generate_graphs(video_info.total_frames)\n    graph = graphs[\"combined_percentage\"].convert(\"RGB\")\n    graph.save(f\"/content/graphs/{i}.jpg\")\n\n    # sv.plot_image(annotated_frame)\n\n    # Send as frame to video\n    sink.write_frame(frame=annotated_frame)\n</pre> VIDEO_PATH = \"/content/parkinglot1080.mov\" MAIN_OUTPUT_PATH = \"/content/parkinglot_annotated.mp4\" frames_generator = sv.get_video_frames_generator(source_path=VIDEO_PATH) video_info = sv.VideoInfo.from_video_path(video_path=VIDEO_PATH)  setup_zones(video_info.resolution_wh)   with sv.VideoSink(target_path=MAIN_OUTPUT_PATH, video_info=video_info) as sink:   heatmap = None   for i, frame in enumerate(frames_generator):     print(f\"Processing frame {i}\")      # Infer     annotated_frame, heatmap = process_frame(frame, heatmap)      # Save the latest heatmap     Image.fromarray(heatmap).save(f\"/content/heatmap/{i}.jpg\")      # Create Graphs     graphs = generate_graphs(video_info.total_frames)     graph = graphs[\"combined_percentage\"].convert(\"RGB\")     graph.save(f\"/content/graphs/{i}.jpg\")      # sv.plot_image(annotated_frame)      # Send as frame to video     sink.write_frame(frame=annotated_frame) <pre>Processing frame 0\n</pre> <pre>Processing frame 1\n</pre> <pre>Processing frame 2\n</pre> <pre>Processing frame 3\n</pre> <pre>Processing frame 4\n</pre> <pre>Processing frame 5\n</pre> <pre>Processing frame 6\n</pre> <pre>Processing frame 7\n</pre> <pre>Processing frame 8\n</pre> <pre>Processing frame 9\n</pre> <pre>Processing frame 10\n</pre> <pre>...</pre> In\u00a0[\u00a0]: Copied! <pre>import cv2\ndef create_videos_from_dir(dir,output):\n  images = len(os.listdir(dir))-1\n\n  sample_img_path = os.path.join(dir,f\"1.jpg\")\n  sample_img = cv2.imread(sample_img_path)\n  height, width, channels = sample_img.shape\n  video_info = sv.VideoInfo(width=width,height=height,fps=24,total_frames=images)\n\n  with sv.VideoSink(target_path=output, video_info=video_info) as sink:\n    for i in range(images):\n      path = os.path.join(dir,f\"{i}.jpg\")\n      img = cv2.imread(path)\n      sink.write_frame(frame=img)\n\n# Graphs\ncreate_videos_from_dir(\"/content/graphs\",\"/content/parkinglot_graph.mp4\")\n\n# Heatmap\ncreate_videos_from_dir(\"/content/heatmap\",\"/content/parkinglot_heatmap.mp4\")\n</pre> import cv2 def create_videos_from_dir(dir,output):   images = len(os.listdir(dir))-1    sample_img_path = os.path.join(dir,f\"1.jpg\")   sample_img = cv2.imread(sample_img_path)   height, width, channels = sample_img.shape   video_info = sv.VideoInfo(width=width,height=height,fps=24,total_frames=images)    with sv.VideoSink(target_path=output, video_info=video_info) as sink:     for i in range(images):       path = os.path.join(dir,f\"{i}.jpg\")       img = cv2.imread(path)       sink.write_frame(frame=img)  # Graphs create_videos_from_dir(\"/content/graphs\",\"/content/parkinglot_graph.mp4\")  # Heatmap create_videos_from_dir(\"/content/heatmap\",\"/content/parkinglot_heatmap.mp4\") In\u00a0[\u00a0]: Copied! <pre>import pickle\n\nwith open('parkinglot_zonedata.pkl', 'wb') as outp:\n  pickle.dump(zones, outp, pickle.HIGHEST_PROTOCOL)\n</pre> import pickle  with open('parkinglot_zonedata.pkl', 'wb') as outp:   pickle.dump(zones, outp, pickle.HIGHEST_PROTOCOL) In\u00a0[\u00a0]: Copied! <pre>with open('parkinglot_zonedata.pkl', 'rb') as inp:\n    zones_imported = pickle.load(inp)\n    zones = zones_imported\n</pre> with open('parkinglot_zonedata.pkl', 'rb') as inp:     zones_imported = pickle.load(inp)     zones = zones_imported In\u00a0[\u00a0]: Copied! <pre>import statistics\nfor zone in zones:\n    occupancy_percent_history = [(count/zone['max'])*100 for count in zone['history']]\n    average_occupancy = round(statistics.mean(occupancy_percent_history))\n    median_occupancy = round(statistics.median(occupancy_percent_history))\n    highest_occupancy = round(max(occupancy_percent_history))\n    lowest_occupancy = round(min(occupancy_percent_history))\n    print(f\"{zone['name']} had an average occupancy of {average_occupancy}% with a median occupancy of {median_occupancy}%.\")\n</pre> import statistics for zone in zones:     occupancy_percent_history = [(count/zone['max'])*100 for count in zone['history']]     average_occupancy = round(statistics.mean(occupancy_percent_history))     median_occupancy = round(statistics.median(occupancy_percent_history))     highest_occupancy = round(max(occupancy_percent_history))     lowest_occupancy = round(min(occupancy_percent_history))     print(f\"{zone['name']} had an average occupancy of {average_occupancy}% with a median occupancy of {median_occupancy}%.\") <pre>Zone 1 had an average occupancy of 60% with a median occupancy of 59%.\nZone 2 had an average occupancy of 69% with a median occupancy of 68%.\nZone 3 had an average occupancy of 85% with a median occupancy of 85%.\nZone 4 had an average occupancy of 85% with a median occupancy of 85%.\nZone 5 had an average occupancy of 91% with a median occupancy of 92%.\n</pre> In\u00a0[\u00a0]: Copied! <pre>lot_history = []\nfor zone in zones:\n    for idx, entry in enumerate(zone['history']):\n      if(idx &gt;= len(lot_history) or len(lot_history)==0): lot_history.append([])\n      lot_history[idx].append(zone['history'][idx]/zone['max'])\n\nlot_occupancy_history = [sum(entry)/len(entry)*100 for entry in lot_history]\n\naverage_occupancy = round(statistics.mean(lot_occupancy_history))\nmedian_occupancy = round(statistics.median(lot_occupancy_history))\nhighest_occupancy = round(max(lot_occupancy_history))\nlowest_occupancy = round(min(lot_occupancy_history))\n\nprint(f\"The entire lot had an average occupancy of {average_occupancy}% with a median occupancy of {median_occupancy}%.\")\n</pre> lot_history = [] for zone in zones:     for idx, entry in enumerate(zone['history']):       if(idx &gt;= len(lot_history) or len(lot_history)==0): lot_history.append([])       lot_history[idx].append(zone['history'][idx]/zone['max'])  lot_occupancy_history = [sum(entry)/len(entry)*100 for entry in lot_history]  average_occupancy = round(statistics.mean(lot_occupancy_history)) median_occupancy = round(statistics.median(lot_occupancy_history)) highest_occupancy = round(max(lot_occupancy_history)) lowest_occupancy = round(min(lot_occupancy_history))  print(f\"The entire lot had an average occupancy of {average_occupancy}% with a median occupancy of {median_occupancy}%.\") <pre>The entire lot had an average occupancy of 78% with a median occupancy of 78%.\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(lot_occupancy_history)\n\n# [\n#    ...\n#    73.51691310215338,\n#    73.34063105087132,\n#    73.86694684034501,\n#    ...\n# ]\n</pre> print(lot_occupancy_history)  # [ #    ... #    73.51691310215338, #    73.34063105087132, #    73.86694684034501, #    ... # ] <pre>[0.0, 73.6265622249604, 73.51691310215338, 73.34063105087132, 73.86694684034501, 73.81677961626474, 74.2515622249604, 74.55142873907177, 74.34309540573842, 76.10547585518981, 75.33624508595904, 75.19454468110075, 74.56954468110075, 74.2334462829314, 74.07528017367835, 74.29457841929236, 74.08624508595905, 74.95162970134366, 75.19619491873496, 75.78914363668368, 76.15564307927008, 75.53779410901838, 75.6293272897964, 75.43910989849205, 76.54025846388546, 76.54025846388546, 77.77632312386316, 76.71654051516751, 77.70969019538813, 78.09430558000352, 77.13320718183418, 78.25247168925658, 79.48853634923428, 78.42875374053864, 78.28452297130787, 77.51782256644957, 78.28452297130787, 77.65952297130787, 78.60250542744822, 79.33715455025524, 79.33715455025524, 78.81083876078155, 78.66913835592325, 78.57045414539694, 78.04413835592325, 77.70803995775391, 77.60935574722761, 77.08303995775391, 77.70803995775391, 77.20662442058324, 78.11755559467231, 77.20662442058324, 77.20662442058324, 78.02602241389427, 77.20662442058324, 77.20662442058324, 78.21623980519861, 77.20662442058324, 76.53860822625124, 76.68030863110955, 76.92069324649417, 77.81768908056092, 76.68030863110955, 79.78422226133897, 79.01752185648066, 78.7315906823916, 79.25790647186528, 78.7315906823916, 78.1065906823916, 77.79957313853194, 77.08303995775391, 76.90675790647185, 77.64140702927888, 77.94842457313852, 77.2707269846858, 78.92345831133017, 78.5067916446635, 76.77811271489762, 78.07365927360208, 78.42622337616618, 79.27767265152849, 79.61212081206361, 81.54146863815058, 81.01515284867688, 80.1472378689198, 80.7541219268908, 79.0098574194684, 80.16117320894209, 79.74450654227543, 78.78340814410608, 79.88873731150619, 80.01947280408379, 80.35392096461891, 79.69851992020185, 79.31390453558646, 79.69851992020185, 80.08313530481722, 80.32351992020186, 79.69851992020185, 79.66646863815056, 80.49980197148389, 79.8706213694772, 78.86100598486182, 78.68725429795222, 80.46357008742592, 78.49703690664789, 78.35280613741712, 80.70813530481722, 80.27335269612156, 79.23168602945492, 79.5041219268908, 79.55681951534353, 79.50665229126326, 79.6163014140703, 81.9855659214927, 81.41073314557296, 82.03573314557296, 81.45671976764653, 81.3651865868685, 80.03043771636449, 81.374501261515, 81.40908290793874, 81.64946752332337, 82.17578331279704, 83.04534853018835, 82.17578331279704, 81.84386551663441, 80.88276711846505, 82.12979669072347, 80.46775068943262, 81.17034853018833, 80.75368186352169, 79.51761720354398, 81.18428387021063, 80.55928387021063, 79.7259505368773, 79.91616792818166, 80.35095053687732, 79.77446752332335, 79.79258346535234, 79.82463474740362, 80.01485213870797, 79.77446752332335, 79.3578008566567, 78.55651880537464, 78.58857008742592, 79.38985213870797, 79.38985213870797, 78.97318547204131, 78.41481840051634, 78.721835944376, 79.14946752332335, 79.3578008566567, 80.15908290793874, 79.59818547204131, 80.63985213870797, 81.47318547204131, 80.01485213870797, 81.61741624127208, 81.20074957460541, 80.99241624127208, 81.02446752332335, 81.40908290793874, 82.03408290793874, 81.64946752332337, 81.02446752332335, 81.02446752332335, 81.40908290793874, 80.36741624127208, 80.77014756791645, 80.86883177844277, 80.24383177844277, 81.01053218330107, 79.474601009212, 80.48421639382738, 80.85071583641377, 79.7103649592208, 80.52558235052514, 79.99926656105146, 80.85071583641377, 79.26461743824443, 79.16593322771813, 78.36465117643607, 78.78131784310274, 78.78131784310274, 79.19798450976941, 79.74241624127207, 81.20074957460541, 79.93263363257641, 79.38820190107376, 79.88961743824443, 79.67413307516283, 79.56448395235581, 79.14781728568914, 78.63246640849616, 78.2157997418295, 78.74926656105147, 78.36465117643607, 78.78131784310274, 79.16593322771813, 78.9575998943848, 79.5825998943848, 78.22295077157777, 78.73115061902247, 78.62150149621544, 78.1867188875198, 77.77005222085313, 78.20483482954879, 78.1727835474975, 77.78816816288212, 78.09518570674176, 78.20483482954879, 77.71057032212639, 79.0562841049111, 78.62150149621544, 80.52558235052514, 79.47295077157777, 78.11330164877074, 78.28958370005282, 78.00365252596374, 77.20237047468169, 78.20483482954879, 77.58698585929707, 77.58698585929707, 77.6785190400751, 77.6785190400751, 78.58945021416417, 77.61188611160007, 78.12723698879304, 75.51758786598603, 75.08280525729039, 78.24403714134836, 76.3234905826439, 77.37612216159127, 77.23442175673296, 77.76073754620666, 78.30516927770933, 76.52467288622894, 76.03040837880654, 76.04852432083553, 76.86792231414657, 75.85665669189696, 75.90517367834302, 77.24373643137945, 77.00797248137064, 78.44521944493339, 77.77467288622894, 76.7148902775333, 77.53428827084431, 77.72450566214869, 76.1885744880596, 75.53152320600833, 76.01229243677756, 74.26802792935516, 74.34309540573842, 76.700954937511, 75.29575925599954, 74.87194155958458, 75.94565950830253, 75.32065950830254, 75.16249339904945, 75.91360822625126, 75.88155694419997, 77.14967288622894, 77.29137329108724, 76.56603884292672, 75.07609429091121, 76.80642345831133, 76.38975679164466, 77.09950566214869, 77.90078771343074, 77.0674543800974, 76.10635598192805, 77.74515196855015, 78.58945021416417, 78.74761632341725, 77.83668514932818, 77.66205333568033, 78.97153523440709, 76.8426553423693, 77.11762160417767, 76.73300621956228, 76.90928827084433, 78.01758786598603, 77.28972305345303, 76.36485653934166, 77.6924543800974, 78.63543683623774, 77.41748811828903, 76.78152320600834, 75.35524115472627, 77.16360822625126, 77.07207504547323, 75.98024115472629, 76.49559203191927, 75.64414275655695, 76.35389162706097, 75.19542480783899, 76.36485653934166, 76.46354074986797, 77.97838555418647, 77.71057032212639, 78.07706976471279, 77.11597136654345, 78.60338555418647, 78.50470134366016, 79.03816816288212, 80.36576600363784, 76.60777151909876, 77.54360294549082, 76.63267177140176, 79.22838555418647, 77.97838555418647, 78.27146775802383, 80.78243267030452, 76.76593762835181, 78.57430469987679, 78.1757539752391, 77.35635598192806, 77.35635598192806, 78.74508595904477, 77.8341547849557, 78.86188611160007, 76.89117232881536, 77.55918852314733, 78.36762160417766, 77.19818987267499, 77.63297248137064, 77.63297248137064, 76.43992401572494, 76.92322361086663, 77.43142345831131, 76.48844100217099, 75.82207504547321, 79.310604060318, 79.310604060318, 78.78428827084433, 79.2119198497917, 77.63297248137064, 78.25082145162237, 79.07021944493339, 79.07021944493339, 78.21877016957109, 76.34674059731267, 77.84130581470399, 77.25767177140176, 78.02437217626006, 77.49805638678637, 76.48844100217099, 77.06327377809072, 77.06327377809072, 78.9534192923781, 78.3284192923781, 78.12008595904476, 77.49508595904476, 77.33691984979171, 77.67136801032682, 77.26863668368246, 76.71907087953998, 77.22727072698468, 77.12693627882416, 74.78972305345305, 76.03972305345303, 76.71026961215748, 75.5589538226838, 75.65510766883764, 76.08273924778501, 77.04933843806842, 75.62305638678637, 77.04933843806842, 77.5756542275421, 76.56603884292672, 77.2091547849557, 75.52437217626006, 76.98270550959337, 77.74940591445169, 77.99232089420876, 77.60770550959337, 78.42710350290443, 78.04248811828904, 78.3174543800974, 77.17710350290442, 76.68283899548202, 76.5841547849557, 76.5951196972364, 75.27049228422226, 74.43297834888224, 74.86776095757789, 74.51057618963797, 77.22727072698468, 76.21600510473507, 77.48412104676406, 76.84815613448336, 77.04933843806842, 76.9506542275421, 77.99232089420876, 76.98270550959337, 75.52734260400165, 75.8132737780907, 77.93962330575603, 76.74529132195038]\n</pre> In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\n\nimport matplotlib.pyplot as plt\n\nfig, ax1 = plt.subplots()\nplt.title('Total Lot Usage')\nax1.grid(color='#2A3459')\n\nax1.plot(lot_occupancy_history)\nax1.set_ylabel('Occupied Percentage (%)')\n\nplt.ylim(top=100)\nplt.xlim(right=len(lot_occupancy_history))\n\nplt.show()\n</pre> %matplotlib inline  import matplotlib.pyplot as plt  fig, ax1 = plt.subplots() plt.title('Total Lot Usage') ax1.grid(color='#2A3459')  ax1.plot(lot_occupancy_history) ax1.set_ylabel('Occupied Percentage (%)')  plt.ylim(top=100) plt.xlim(right=len(lot_occupancy_history))  plt.show() In\u00a0[\u00a0]: Copied! <pre>import cv2\nimport numpy as np\n\ndef transform_image(image, points):\n    width = max(np.linalg.norm(points[0] - points[1]), np.linalg.norm(points[2] - points[3]))\n    height = max(np.linalg.norm(points[0] - points[3]), np.linalg.norm(points[1] - points[2]))\n    dest_points = np.array([[0, 0], [width - 1, 0], [width - 1, height - 1], [0, height - 1]], dtype=\"float32\")\n    matrix = cv2.getPerspectiveTransform(points.astype(\"float32\"), dest_points)\n    transformed_image = cv2.warpPerspective(image, matrix, (int(width), int(height)))\n\n    return transformed_image\n\ndef generate_top_down_views(frame,show=True):\n  heatmap = cv2.imread(f\"heatmap/{frame}.jpg\")\n  image = cv2.imread(f\"frames/{frame}.jpg\")\n\n  images = []\n\n  for zone in zones:\n    if show: print(f\"Occupancy Visualization of {zone['name']}\")\n    top_down_image = transform_image(image, zone['polygon'])\n    top_down_heatmap = transform_image(heatmap, zone['polygon'])\n\n    combined_image = cv2.addWeighted(top_down_image, 0.7, top_down_heatmap, 0.3, 0)\n\n    if show: sv.plot_image(combined_image, size=(5,5))\n\n    images.append(combined_image)\n\n  return images\n</pre> import cv2 import numpy as np  def transform_image(image, points):     width = max(np.linalg.norm(points[0] - points[1]), np.linalg.norm(points[2] - points[3]))     height = max(np.linalg.norm(points[0] - points[3]), np.linalg.norm(points[1] - points[2]))     dest_points = np.array([[0, 0], [width - 1, 0], [width - 1, height - 1], [0, height - 1]], dtype=\"float32\")     matrix = cv2.getPerspectiveTransform(points.astype(\"float32\"), dest_points)     transformed_image = cv2.warpPerspective(image, matrix, (int(width), int(height)))      return transformed_image  def generate_top_down_views(frame,show=True):   heatmap = cv2.imread(f\"heatmap/{frame}.jpg\")   image = cv2.imread(f\"frames/{frame}.jpg\")    images = []    for zone in zones:     if show: print(f\"Occupancy Visualization of {zone['name']}\")     top_down_image = transform_image(image, zone['polygon'])     top_down_heatmap = transform_image(heatmap, zone['polygon'])      combined_image = cv2.addWeighted(top_down_image, 0.7, top_down_heatmap, 0.3, 0)      if show: sv.plot_image(combined_image, size=(5,5))      images.append(combined_image)    return images  In\u00a0[\u00a0]: Copied! <pre>generate_top_down_views(400)\n</pre> generate_top_down_views(400) In\u00a0[\u00a0]: Copied! <pre>import os\nimport numpy as np\nfrom PIL import Image\nimport supervision as sv\n\nfor filename in os.listdir(\"frames\"):\n  img_path = os.path.join(\"frames\", filename)\n  heatmap_path = os.path.join(\"heatmap\", filename)\n  if os.path.isfile(img_path) and os.path.isfile(heatmap_path):\n    frame = int(filename.replace(\".jpg\",\"\"))\n    images = generate_top_down_views(frame,False)\n    gap = 10\n\n    pil_images = [Image.fromarray(image) for image in images]\n\n    # Resize images to have the same width\n    widths, heights = zip(*(i.size for i in pil_images))\n    max_width = max(widths)\n    total_height = sum(heights) + gap * (len(images) - 1)\n    resized_images = [i.resize((max_width, int(i.height * max_width / i.width))) for i in pil_images]\n\n    # Create a new image with the correct combined size\n    combined_image = Image.new('RGB', (max_width, total_height))\n\n    # Paste each image into the combined image with the specified gap\n    y_offset = 0\n    for img in resized_images:\n        combined_image.paste(img, (0, y_offset))\n        y_offset += img.height + gap\n\n    combined_image = combined_image.rotate(90, expand=True)\n\n    combined_image.save(f\"sectionheatmaps/{frame}.jpg\")\n\n    sv.plot_image(np.array(combined_image))\n</pre> import os import numpy as np from PIL import Image import supervision as sv  for filename in os.listdir(\"frames\"):   img_path = os.path.join(\"frames\", filename)   heatmap_path = os.path.join(\"heatmap\", filename)   if os.path.isfile(img_path) and os.path.isfile(heatmap_path):     frame = int(filename.replace(\".jpg\",\"\"))     images = generate_top_down_views(frame,False)     gap = 10      pil_images = [Image.fromarray(image) for image in images]      # Resize images to have the same width     widths, heights = zip(*(i.size for i in pil_images))     max_width = max(widths)     total_height = sum(heights) + gap * (len(images) - 1)     resized_images = [i.resize((max_width, int(i.height * max_width / i.width))) for i in pil_images]      # Create a new image with the correct combined size     combined_image = Image.new('RGB', (max_width, total_height))      # Paste each image into the combined image with the specified gap     y_offset = 0     for img in resized_images:         combined_image.paste(img, (0, y_offset))         y_offset += img.height + gap      combined_image = combined_image.rotate(90, expand=True)      combined_image.save(f\"sectionheatmaps/{frame}.jpg\")      sv.plot_image(np.array(combined_image))"},{"location":"notebooks/occupancy_analytics/#how-to-analyze-occupancy-with-supervision","title":"How To Analyze Occupancy with Supervision\u00b6","text":"<p>In this notebook, we'll use a parking lot to demonstrate how we can extract numerous informative metrics and detailed graphics, all from one video, using Supervision.</p> <p>This notebook accompanies the Occupancy Analytics with Computer Vision tutorial on the Roboflow Blog. Check it out for deeper explanations and context!</p> <p></p> <p>In this notebook, we will cover the following:</p> <ol> <li>Getting training data</li> <li>Training a object detection model</li> <li>Detect vehicles</li> <li>Analyze data and generate statistics</li> </ol>"},{"location":"notebooks/occupancy_analytics/#before-you-start","title":"Before You Start\u00b6","text":"<p>Let's make sure that we have access to GPU. We can use <code>nvidia-smi</code> command to do that. In case of any problems navigate to <code>Edit</code> -&gt; <code>Notebook settings</code> -&gt; <code>Hardware accelerator</code>, set it to <code>GPU</code>, and then click <code>Save</code>.</p>"},{"location":"notebooks/occupancy_analytics/#install-relevant-packages","title":"Install Relevant Packages\u00b6","text":"<p>Here, we will install the Roboflow package, for uploading and training our model, and Supervision for visualization and extracting metrics from our predicted model results.</p>"},{"location":"notebooks/occupancy_analytics/#getting-video-data","title":"Getting Video Data\u00b6","text":"<p>We will start with turning a single video into a folder of frame images, for training our model. Upload your video and set your video's file path here.</p>"},{"location":"notebooks/occupancy_analytics/#random-crop-sampling-if-using-sahi","title":"Random Crop Sampling (If Using SAHI)\u00b6","text":"<p>If we are using SAHI (which we are in our example), randomly sampling cropped portions of our image can help mimic the effect of SAHI detection during training, improving performance.</p>"},{"location":"notebooks/occupancy_analytics/#training-a-model","title":"Training a Model\u00b6","text":"<p>Now that we have our images, we can upload our extracted frames as training data to Roboflow.</p>"},{"location":"notebooks/occupancy_analytics/#upload-training-data","title":"Upload Training Data\u00b6","text":""},{"location":"notebooks/occupancy_analytics/#training-model-using-autodistill-optional","title":"Training Model Using Autodistill (Optional)\u00b6","text":"<p>We can train our model using Automated Labeling, powered by Autodistill, to automatically label our data. Copy the code required for this section from the Roboflow app.</p> <p>Note: It's not required to use Autodistill</p>"},{"location":"notebooks/occupancy_analytics/#vehicle-detection","title":"Vehicle Detection\u00b6","text":"<p>Now, we can run our model to get inference data for our video data.</p>"},{"location":"notebooks/occupancy_analytics/#setup-model","title":"Setup Model\u00b6","text":"<p>First, set the model up as a callback function so that we can call it later on while using Supervision.</p>"},{"location":"notebooks/occupancy_analytics/#configure-zones","title":"Configure Zones\u00b6","text":"<p>Next, we will set up a list of the zones to be used with PolygonZone. You can get these polygon coordinates using this web utility.</p> <p>For our example, we have have zones, but you can add as many or as little zones as you would like.</p>"},{"location":"notebooks/occupancy_analytics/#setup-supervision","title":"Setup Supervision\u00b6","text":"<p>For our use case, we will use the following features of Supervision. Refer to the linked documentation for more details:</p> <ul> <li>ByteTrack: To track the location of our vehicles, so we can assess how long they are parked</li> <li>InferenceSlicer: A helper utility to run SAHI on our model</li> <li>TriangleAnnotator: To help visualize the locations of the vehicles</li> <li>HeatMapAnnotator: To generate heatmaps so we can identify our busiest areas</li> <li>PolygonZone, PolygonZoneAnnotator: To help count and identify vehicles in our respective zones and the annotator to help visualize those zones.</li> </ul>"},{"location":"notebooks/occupancy_analytics/#try-with-a-single-image","title":"Try With a Single Image\u00b6","text":""},{"location":"notebooks/occupancy_analytics/#setup-graphs","title":"Setup Graphs\u00b6","text":"<p>Before we run the model on the entire video, we will set up the logic to generate our graphs using matplotlib.</p>"},{"location":"notebooks/occupancy_analytics/#process-video","title":"Process Video\u00b6","text":"<p>Now, we can process the video to get detections from the entire video.</p>"},{"location":"notebooks/occupancy_analytics/#generate-graphsheatmap-video-optional","title":"Generate Graphs/Heatmap Video (optional)\u00b6","text":""},{"location":"notebooks/occupancy_analytics/#analyze-data","title":"Analyze Data\u00b6","text":"<p>Lastly, we can analyze the data we got to extract quantitative metrics from our video.</p>"},{"location":"notebooks/occupancy_analytics/#save-your-data-for-later","title":"Save your data for later\u00b6","text":"<p>Using Pickle, we can save our zone detection data so that we can load it in for later analysis. Remember to download your file from the Colab file manager.</p>"},{"location":"notebooks/occupancy_analytics/#import-your-data","title":"Import your data\u00b6","text":"<p>To load your data back in, upload the saved file to the Colab enviorment and run the code cell.</p>"},{"location":"notebooks/occupancy_analytics/#occupancy-per-section","title":"Occupancy Per Section\u00b6","text":"<p>Since we recorded the number of objects (vehicles) in each zone, we can compare that against our hardcoded <code>max</code> that we put in while setting up our zones. Using this data, we can calculate the average and median occupancy, as well as any other metrics such as the max or the minimum occupancy throughout that time period.</p>"},{"location":"notebooks/occupancy_analytics/#total-occupancy","title":"Total Occupancy\u00b6","text":"<p>Using the occupancy for the zones, we can also add up all the occupancy metrics throughout all the zones in order to calculate metrics for the whole parking lot.</p>"},{"location":"notebooks/occupancy_analytics/#busy-areas","title":"Busy Areas\u00b6","text":"<p>Using Supervision's heat map annotator, we can use heatmaps while transforming the images in order to create images on top-down views of each zone.</p>"},{"location":"notebooks/quickstart/","title":"Supervision Quickstart","text":"In\u00a0[1]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi <pre>Wed Jul 17 14:51:30 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n| N/A   63C    P8              14W /  72W |      1MiB / 23034MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n</pre> <p>NOTE: To make it easier for us to manage datasets, images and models we create a <code>HOME</code> constant.</p> In\u00a0[2]: Copied! <pre>import os\n\nHOME = os.getcwd()\nprint(HOME)\n</pre> import os  HOME = os.getcwd() print(HOME) <pre>/content\n</pre> <p>NOTE: During our demo, we will need some example images.</p> In\u00a0[3]: Copied! <pre>!mkdir {HOME}/images\n</pre> !mkdir {HOME}/images <p>NOTE: Feel free to use your images. Just make sure to put them into <code>images</code> directory that we just created. \u261d\ufe0f</p> In\u00a0[4]: Copied! <pre>%cd {HOME}/images\n\n!wget -q https://media.roboflow.com/notebooks/examples/dog.jpeg\n!wget -q https://media.roboflow.com/notebooks/examples/dog-2.jpeg\n!wget -q https://media.roboflow.com/notebooks/examples/dog-3.jpeg\n!wget -q https://media.roboflow.com/notebooks/examples/dog-4.jpeg\n!wget -q https://media.roboflow.com/notebooks/examples/dog-5.jpeg\n!wget -q https://media.roboflow.com/notebooks/examples/dog-6.jpeg\n!wget -q https://media.roboflow.com/notebooks/examples/dog-7.jpeg\n!wget -q https://media.roboflow.com/notebooks/examples/dog-8.jpeg\n</pre> %cd {HOME}/images  !wget -q https://media.roboflow.com/notebooks/examples/dog.jpeg !wget -q https://media.roboflow.com/notebooks/examples/dog-2.jpeg !wget -q https://media.roboflow.com/notebooks/examples/dog-3.jpeg !wget -q https://media.roboflow.com/notebooks/examples/dog-4.jpeg !wget -q https://media.roboflow.com/notebooks/examples/dog-5.jpeg !wget -q https://media.roboflow.com/notebooks/examples/dog-6.jpeg !wget -q https://media.roboflow.com/notebooks/examples/dog-7.jpeg !wget -q https://media.roboflow.com/notebooks/examples/dog-8.jpeg <pre>/content/images\n</pre> In\u00a0[5]: Copied! <pre>!pip install -q supervision\n\nimport supervision as sv\n\nprint(sv.__version__)\n</pre> !pip install -q supervision  import supervision as sv  print(sv.__version__) <pre>     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 0.0/135.7 kB ? eta -:--:--\r     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 135.7/135.7 kB 3.9 MB/s eta 0:00:00\n0.22.0\n</pre> In\u00a0[6]: Copied! <pre>import cv2\n\nIMAGE_PATH = f\"{HOME}/images/dog.jpeg\"\n\nimage = cv2.imread(IMAGE_PATH)\n</pre> import cv2  IMAGE_PATH = f\"{HOME}/images/dog.jpeg\"  image = cv2.imread(IMAGE_PATH) In\u00a0[\u00a0]: Copied! <pre>!pip install -q ultralytics\n</pre> !pip install -q ultralytics In\u00a0[\u00a0]: Copied! <pre>from ultralytics import YOLO\n\nmodel = YOLO(\"yolov8s.pt\")\nresult = model(image, verbose=False)[0]\ndetections = sv.Detections.from_ultralytics(result)\n</pre> from ultralytics import YOLO  model = YOLO(\"yolov8s.pt\") result = model(image, verbose=False)[0] detections = sv.Detections.from_ultralytics(result) In\u00a0[9]: Copied! <pre>\"detections\", len(detections)\n</pre> \"detections\", len(detections) Out[9]: <pre>('detections', 4)</pre> In\u00a0[\u00a0]: Copied! <pre>!pip install -q inference\n</pre> !pip install -q inference In\u00a0[\u00a0]: Copied! <pre>from inference import get_model\n\nmodel = get_model(model_id=\"yolov8s-640\")\nresult = model.infer(image)[0]\ndetections = sv.Detections.from_inference(result)\n</pre> from inference import get_model  model = get_model(model_id=\"yolov8s-640\") result = model.infer(image)[0] detections = sv.Detections.from_inference(result) In\u00a0[12]: Copied! <pre>\"detections\", len(detections)\n</pre> \"detections\", len(detections) Out[12]: <pre>('detections', 4)</pre> In\u00a0[\u00a0]: Copied! <pre>!pip install -q super-gradients\n!pip install --upgrade urllib3\n</pre> !pip install -q super-gradients !pip install --upgrade urllib3 In\u00a0[\u00a0]: Copied! <pre>from super_gradients.training import models\n\nmodel = models.get(\"yolo_nas_s\", pretrained_weights=\"coco\")\nresult = model.predict(image)\ndetections = sv.Detections.from_yolo_nas(result)\n</pre> from super_gradients.training import models  model = models.get(\"yolo_nas_s\", pretrained_weights=\"coco\") result = model.predict(image) detections = sv.Detections.from_yolo_nas(result) In\u00a0[15]: Copied! <pre>\"detections\", len(detections)\n</pre> \"detections\", len(detections) Out[15]: <pre>('detections', 7)</pre> In\u00a0[\u00a0]: Copied! <pre>from ultralytics import YOLO\n\nmodel = YOLO(\"yolov8x.pt\")\nresult = model(image, verbose=False)[0]\ndetections = sv.Detections.from_ultralytics(result)\n</pre> from ultralytics import YOLO  model = YOLO(\"yolov8x.pt\") result = model(image, verbose=False)[0] detections = sv.Detections.from_ultralytics(result) In\u00a0[31]: Copied! <pre>box_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_image = image.copy()\nannotated_image = box_annotator.annotate(annotated_image, detections=detections)\nannotated_image = label_annotator.annotate(annotated_image, detections=detections)\n\nsv.plot_image(image=annotated_image, size=(8, 8))\n</pre> box_annotator = sv.BoxAnnotator() label_annotator = sv.LabelAnnotator()  annotated_image = image.copy() annotated_image = box_annotator.annotate(annotated_image, detections=detections) annotated_image = label_annotator.annotate(annotated_image, detections=detections)  sv.plot_image(image=annotated_image, size=(8, 8)) <p>NOTE: By default <code>sv.LabelAnnotator</code> use corresponding <code>class_id</code> as label, however, the labels can have arbitrary format.</p> In\u00a0[32]: Copied! <pre>box_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nlabels = [\n    f\"{model.model.names[class_id]} {confidence:.2f}\"\n    for class_id, confidence in zip(detections.class_id, detections.confidence)\n]\n\nannotated_image = image.copy()\nannotated_image = box_annotator.annotate(annotated_image, detections=detections)\nannotated_image = label_annotator.annotate(\n    annotated_image, detections=detections, labels=labels)\n\nsv.plot_image(image=annotated_image, size=(8, 8))\n</pre> box_annotator = sv.BoxAnnotator() label_annotator = sv.LabelAnnotator()  labels = [     f\"{model.model.names[class_id]} {confidence:.2f}\"     for class_id, confidence in zip(detections.class_id, detections.confidence) ]  annotated_image = image.copy() annotated_image = box_annotator.annotate(annotated_image, detections=detections) annotated_image = label_annotator.annotate(     annotated_image, detections=detections, labels=labels)  sv.plot_image(image=annotated_image, size=(8, 8)) In\u00a0[33]: Copied! <pre>from ultralytics import YOLO\n\nmodel = YOLO(\"yolov8x-seg.pt\")\nresult = model(image, verbose=False)[0]\ndetections = sv.Detections.from_ultralytics(result)\n</pre> from ultralytics import YOLO  model = YOLO(\"yolov8x-seg.pt\") result = model(image, verbose=False)[0] detections = sv.Detections.from_ultralytics(result) In\u00a0[34]: Copied! <pre>mask_annotator = sv.MaskAnnotator()\n\nannotated_image = image.copy()\nannotated_image = mask_annotator.annotate(annotated_image, detections=detections)\n\nsv.plot_image(image=annotated_image, size=(8, 8))\n</pre> mask_annotator = sv.MaskAnnotator()  annotated_image = image.copy() annotated_image = mask_annotator.annotate(annotated_image, detections=detections)  sv.plot_image(image=annotated_image, size=(8, 8)) In\u00a0[35]: Copied! <pre>detections_index = detections[0]\ndetections_index_list = detections[[0, 1, 3]]\ndetections_index_slice = detections[:2]\n</pre> detections_index = detections[0] detections_index_list = detections[[0, 1, 3]] detections_index_slice = detections[:2] In\u00a0[36]: Copied! <pre>box_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nimages = []\nfor d in [detections_index, detections_index_list, detections_index_slice]:\n    annotated_image = box_annotator.annotate(image.copy(), detections=d)\n    annotated_image = label_annotator.annotate(annotated_image, detections=d)\n    images.append(annotated_image)\ntitles = [\n    \"by index - detections[0]\",\n    \"by index list - detections[[0, 1, 3]]\",\n    \"by index slice - detections[:2]\",\n]\n\nsv.plot_images_grid(images=images, titles=titles, grid_size=(1, 3))\n</pre> box_annotator = sv.BoxAnnotator() label_annotator = sv.LabelAnnotator()  images = [] for d in [detections_index, detections_index_list, detections_index_slice]:     annotated_image = box_annotator.annotate(image.copy(), detections=d)     annotated_image = label_annotator.annotate(annotated_image, detections=d)     images.append(annotated_image) titles = [     \"by index - detections[0]\",     \"by index list - detections[[0, 1, 3]]\",     \"by index slice - detections[:2]\", ]  sv.plot_images_grid(images=images, titles=titles, grid_size=(1, 3)) In\u00a0[37]: Copied! <pre>detections_filtered = detections[detections.class_id == 0]\n</pre> detections_filtered = detections[detections.class_id == 0] In\u00a0[38]: Copied! <pre>box_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\nannotated_image = box_annotator.annotate(image.copy(), detections=detections_filtered)\nannotated_image = label_annotator.annotate(\n    annotated_image, detections=detections_filtered\n)\nsv.plot_image(image=annotated_image, size=(8, 8))\n</pre> box_annotator = sv.BoxAnnotator() label_annotator = sv.LabelAnnotator() annotated_image = box_annotator.annotate(image.copy(), detections=detections_filtered) annotated_image = label_annotator.annotate(     annotated_image, detections=detections_filtered ) sv.plot_image(image=annotated_image, size=(8, 8)) In\u00a0[39]: Copied! <pre>detections_filtered = detections[detections.confidence &gt; 0.7]\n</pre> detections_filtered = detections[detections.confidence &gt; 0.7] In\u00a0[40]: Copied! <pre>box_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\nlabels = []\nfor class_id, confidence in zip(\n    detections_filtered.class_id, detections_filtered.confidence\n):\n    labels.append(f\"{model.model.names[class_id]} {confidence:.2f}\")\nannotated_image = box_annotator.annotate(\n    image.copy(),\n    detections=detections_filtered,\n)\nannotated_image = label_annotator.annotate(\n    annotated_image, detections=detections_filtered, labels=labels\n)\nsv.plot_image(image=annotated_image, size=(8, 8))\n</pre> box_annotator = sv.BoxAnnotator() label_annotator = sv.LabelAnnotator() labels = [] for class_id, confidence in zip(     detections_filtered.class_id, detections_filtered.confidence ):     labels.append(f\"{model.model.names[class_id]} {confidence:.2f}\") annotated_image = box_annotator.annotate(     image.copy(),     detections=detections_filtered, ) annotated_image = label_annotator.annotate(     annotated_image, detections=detections_filtered, labels=labels ) sv.plot_image(image=annotated_image, size=(8, 8)) In\u00a0[41]: Copied! <pre>detections_filtered = detections[\n    (detections.class_id != 0) &amp; (detections.confidence &gt; 0.7)\n]\n</pre> detections_filtered = detections[     (detections.class_id != 0) &amp; (detections.confidence &gt; 0.7) ] In\u00a0[42]: Copied! <pre>box_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\nlabels = [\n    f\"{class_id} {confidence:.2f}\"\n    for class_id, confidence in zip(\n        detections_filtered.class_id, detections_filtered.confidence\n    )\n]\nannotated_image = box_annotator.annotate(\n    image.copy(),\n    detections=detections_filtered,\n)\nannotated_image = label_annotator.annotate(\n    annotated_image, detections=detections_filtered, labels=labels\n)\n\nsv.plot_image(image=annotated_image, size=(8, 8))\n</pre> box_annotator = sv.BoxAnnotator() label_annotator = sv.LabelAnnotator() labels = [     f\"{class_id} {confidence:.2f}\"     for class_id, confidence in zip(         detections_filtered.class_id, detections_filtered.confidence     ) ] annotated_image = box_annotator.annotate(     image.copy(),     detections=detections_filtered, ) annotated_image = label_annotator.annotate(     annotated_image, detections=detections_filtered, labels=labels )  sv.plot_image(image=annotated_image, size=(8, 8)) <p>NOTE: During our demo, we will need some example videos.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install -q supervision[assets]\n</pre> !pip install -q supervision[assets] In\u00a0[\u00a0]: Copied! <pre>!mkdir {HOME}/videos\n</pre> !mkdir {HOME}/videos <p>NOTE: Feel free to use your videos. Just make sure to put them into <code>videos</code> directory that we just created. \u261d\ufe0f</p> In\u00a0[\u00a0]: Copied! <pre>%cd {HOME}/videos\n</pre> %cd {HOME}/videos In\u00a0[\u00a0]: Copied! <pre>from supervision.assets import download_assets, VideoAssets\n\ndownload_assets(VideoAssets.VEHICLES)\nVIDEO_PATH = VideoAssets.VEHICLES.value\n</pre> from supervision.assets import download_assets, VideoAssets  download_assets(VideoAssets.VEHICLES) VIDEO_PATH = VideoAssets.VEHICLES.value In\u00a0[\u00a0]: Copied! <pre>sv.VideoInfo.from_video_path(video_path=VIDEO_PATH)\n</pre> sv.VideoInfo.from_video_path(video_path=VIDEO_PATH) Out[\u00a0]: <pre>VideoInfo(width=3840, height=2160, fps=25, total_frames=538)</pre> In\u00a0[\u00a0]: Copied! <pre>frame_generator = sv.get_video_frames_generator(source_path=VIDEO_PATH)\n</pre> frame_generator = sv.get_video_frames_generator(source_path=VIDEO_PATH) In\u00a0[\u00a0]: Copied! <pre>frame = next(iter(frame_generator))\nsv.plot_image(image=frame, size=(8, 8))\n</pre> frame = next(iter(frame_generator)) sv.plot_image(image=frame, size=(8, 8)) In\u00a0[\u00a0]: Copied! <pre>RESULT_VIDEO_PATH = f\"{HOME}/videos/vehicle-counting-result.mp4\"\n</pre> RESULT_VIDEO_PATH = f\"{HOME}/videos/vehicle-counting-result.mp4\" <p>NOTE: Note that this time we have given a custom value for the <code>stride</code> parameter equal to <code>2</code>. As a result, <code>get_video_frames_generator</code> will return us every second video frame.</p> In\u00a0[\u00a0]: Copied! <pre>video_info = sv.VideoInfo.from_video_path(video_path=VIDEO_PATH)\n\nwith sv.VideoSink(target_path=RESULT_VIDEO_PATH, video_info=video_info) as sink:\n    for frame in sv.get_video_frames_generator(source_path=VIDEO_PATH, stride=2):\n        sink.write_frame(frame=frame)\n</pre> video_info = sv.VideoInfo.from_video_path(video_path=VIDEO_PATH)  with sv.VideoSink(target_path=RESULT_VIDEO_PATH, video_info=video_info) as sink:     for frame in sv.get_video_frames_generator(source_path=VIDEO_PATH, stride=2):         sink.write_frame(frame=frame) <p>NOTE: If we once again use <code>VideoInfo</code> we will notice that the final video has 2 times fewer frames.</p> In\u00a0[\u00a0]: Copied! <pre>sv.VideoInfo.from_video_path(video_path=RESULT_VIDEO_PATH)\n</pre> sv.VideoInfo.from_video_path(video_path=RESULT_VIDEO_PATH) Out[\u00a0]: <pre>VideoInfo(width=3840, height=2160, fps=25, total_frames=269)</pre> In\u00a0[\u00a0]: Copied! <pre>!pip install -q roboflow\n</pre> !pip install -q roboflow In\u00a0[\u00a0]: Copied! <pre>!mkdir {HOME}/datasets\n%cd {HOME}/datasets\n\nimport roboflow\nfrom roboflow import Roboflow\n\nroboflow.login()\n\nrf = Roboflow()\n\nproject = rf.workspace(\"roboflow-jvuqo\").project(\"fashion-assistant-segmentation\")\ndataset = project.version(5).download(\"yolov8\")\n</pre> !mkdir {HOME}/datasets %cd {HOME}/datasets  import roboflow from roboflow import Roboflow  roboflow.login()  rf = Roboflow()  project = rf.workspace(\"roboflow-jvuqo\").project(\"fashion-assistant-segmentation\") dataset = project.version(5).download(\"yolov8\") <pre>/content/datasets/images/datasets\n\rvisit https://app.roboflow.com/auth-cli to get your authentication token.\nPaste the authentication token here: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nloading Roboflow workspace...\nloading Roboflow project...\nDependency ultralytics==8.0.196 is required but found version=8.2.54, to fix: `pip install ultralytics==8.0.196`\n</pre> <pre>Downloading Dataset Version Zip in fashion-assistant-segmentation-5 to yolov8:: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 122509/122509 [00:03&lt;00:00, 37319.95it/s]\nExtracting Dataset Version Zip to fashion-assistant-segmentation-5 in yolov8::  15%|\u2588\u258d        | 187/1254 [00:00&lt;00:00, 1860.52it/s]</pre> <pre>\n</pre> <pre>\rExtracting Dataset Version Zip to fashion-assistant-segmentation-5 in yolov8::  30%|\u2588\u2588\u2589       | 374/1254 [00:00&lt;00:00, 1609.45it/s]\rExtracting Dataset Version Zip to fashion-assistant-segmentation-5 in yolov8::  43%|\u2588\u2588\u2588\u2588\u258e     | 538/1254 [00:00&lt;00:00, 1529.93it/s]</pre> In\u00a0[\u00a0]: Copied! <pre>ds = sv.DetectionDataset.from_yolo(\n    images_directory_path=f\"{dataset.location}/train/images\",\n    annotations_directory_path=f\"{dataset.location}/train/labels\",\n    data_yaml_path=f\"{dataset.location}/data.yaml\",\n)\n</pre> ds = sv.DetectionDataset.from_yolo(     images_directory_path=f\"{dataset.location}/train/images\",     annotations_directory_path=f\"{dataset.location}/train/labels\",     data_yaml_path=f\"{dataset.location}/data.yaml\", ) <pre>\rExtracting Dataset Version Zip to fashion-assistant-segmentation-5 in yolov8::  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 989/1254 [00:00&lt;00:00, 2606.19it/s]\rExtracting Dataset Version Zip to fashion-assistant-segmentation-5 in yolov8:: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1254/1254 [00:00&lt;00:00, 2505.30it/s]\n</pre> In\u00a0[\u00a0]: Copied! <pre>len(ds)\n</pre> len(ds) Out[\u00a0]: <pre>573</pre> In\u00a0[\u00a0]: Copied! <pre>ds.classes\n</pre> ds.classes Out[\u00a0]: <pre>['baseball cap',\n 'hoodie',\n 'jacket',\n 'pants',\n 'shirt',\n 'shorts',\n 'sneaker',\n 'sunglasses',\n 'sweatshirt',\n 't-shirt']</pre> In\u00a0[\u00a0]: Copied! <pre>IMAGE_NAME = list(ds.images.keys())[0]\n\nimage = ds.images[IMAGE_NAME]\nannotations = ds.annotations[IMAGE_NAME]\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\nmask_annotator = sv.MaskAnnotator()\n\nlabels = [f\"{ds.classes[class_id]}\" for class_id in annotations.class_id]\n\nannotated_image = mask_annotator.annotate(image.copy(), detections=annotations)\nannotated_image = box_annotator.annotate(annotated_image, detections=annotations)\nannotated_image = label_annotator.annotate(\n    annotated_image, detections=annotations, labels=labels\n)\n\nsv.plot_image(image=annotated_image, size=(8, 8))\n</pre> IMAGE_NAME = list(ds.images.keys())[0]  image = ds.images[IMAGE_NAME] annotations = ds.annotations[IMAGE_NAME]  box_annotator = sv.BoxAnnotator() label_annotator = sv.LabelAnnotator() mask_annotator = sv.MaskAnnotator()  labels = [f\"{ds.classes[class_id]}\" for class_id in annotations.class_id]  annotated_image = mask_annotator.annotate(image.copy(), detections=annotations) annotated_image = box_annotator.annotate(annotated_image, detections=annotations) annotated_image = label_annotator.annotate(     annotated_image, detections=annotations, labels=labels )  sv.plot_image(image=annotated_image, size=(8, 8)) In\u00a0[\u00a0]: Copied! <pre>ds_train, ds_test = ds.split(split_ratio=0.8)\n</pre> ds_train, ds_test = ds.split(split_ratio=0.8) In\u00a0[\u00a0]: Copied! <pre>\"ds_train\", len(ds_train), \"ds_test\", len(ds_test)\n</pre> \"ds_train\", len(ds_train), \"ds_test\", len(ds_test) Out[\u00a0]: <pre>('ds_train', 458, 'ds_test', 115)</pre> In\u00a0[\u00a0]: Copied! <pre>ds_train.as_pascal_voc(\n    images_directory_path=f\"{HOME}/datasets/result/images\",\n    annotations_directory_path=f\"{HOME}/datasets/result/labels\",\n)\n</pre> ds_train.as_pascal_voc(     images_directory_path=f\"{HOME}/datasets/result/images\",     annotations_directory_path=f\"{HOME}/datasets/result/labels\", )"},{"location":"notebooks/quickstart/#supervision-quickstart","title":"Supervision Quickstart\u00b6","text":"<p>We write your reusable computer vision tools. Whether you need to load your dataset from your hard drive, draw detections on an image or video, or count how many detections are in a zone. You can count on us! \ud83e\udd1d</p> <p>We hope that the resources in this notebook will help you get the most out of Supervision. Please browse the Supervision docs for details, raise an issue on GitHub for support, and join our discussions section for questions!</p>"},{"location":"notebooks/quickstart/#table-of-contents","title":"Table of contents\u00b6","text":"<ul> <li>Before you start</li> <li>Install</li> <li>Detection API<ul> <li>Plug in your model<ul> <li>YOLOv8 (<code>pip install ultralytics</code>)</li> <li>Inference (<code>pip install inference</code>)</li> <li>YOLO-NAS (<code>pip install super-gradients</code>)</li> </ul> </li> <li>Annotate<ul> <li><code>BoxAnnotator</code></li> <li><code>MaskAnnotator</code></li> <li><code>LabelAnnotator</code></li> </ul> </li> <li>Filter<ul> <li>By index, index list and index slice</li> <li>By <code>class_id</code></li> <li>By <code>confidence</code></li> <li>By advanced logical condition</li> </ul> </li> </ul> </li> <li>Video API<ul> <li><code>VideoInfo</code></li> <li><code>get_video_frames_generator</code></li> <li><code>VideoSink</code></li> </ul> </li> <li>Dataset API<ul> <li><code>DetectionDataset.from_yolo</code></li> <li>Visualize annotations</li> <li><code>split</code></li> <li><code>DetectionDataset.as_pascal_voc</code></li> </ul> </li> </ul>"},{"location":"notebooks/quickstart/#before-you-start","title":"\u26a1 Before you start\u00b6","text":"<p>NOTE: In this notebook, we aim to show - among other things - how simple it is to integrate <code>supervision</code> with popular object detection and instance segmentation libraries and frameworks. GPU access is optional but will certainly make the ride smoother.</p> <p>Let's make sure that we have access to GPU. We can use <code>nvidia-smi</code> command to do that. In case of any problems navigate to <code>Edit</code> -&gt; <code>Notebook settings</code> -&gt; <code>Hardware accelerator</code>, set it to <code>GPU</code>, and then click <code>Save</code>.</p>"},{"location":"notebooks/quickstart/#install","title":"\u200d\ud83d\udcbb Install\u00b6","text":""},{"location":"notebooks/quickstart/#detection-api","title":"\ud83d\udc41\ufe0f Detection API\u00b6","text":"<ul> <li>xyxy <code>(np.ndarray)</code>: An array of shape <code>(n, 4)</code> containing the bounding boxes coordinates in format <code>[x1, y1, x2, y2]</code></li> <li>mask: <code>(Optional[np.ndarray])</code>: An array of shape <code>(n, W, H)</code> containing the segmentation masks.</li> <li>confidence <code>(Optional[np.ndarray])</code>: An array of shape <code>(n,)</code> containing the confidence scores of the detections.</li> <li>class_id <code>(Optional[np.ndarray])</code>: An array of shape <code>(n,)</code> containing the class ids of the detections.</li> <li>tracker_id <code>(Optional[np.ndarray])</code>: An array of shape <code>(n,)</code> containing the tracker ids of the detections.</li> </ul>"},{"location":"notebooks/quickstart/#plug-in-your-model","title":"\ud83d\udd0c Plug in your model\u00b6","text":"<p>NOTE: In our example, we will focus only on integration with YOLO-NAS and YOLOv8. However, keep in mind that supervision allows seamless integration with many other models like SAM, Transformers, and YOLOv5. You can learn more from our documentation.</p>"},{"location":"notebooks/quickstart/#ultralytics","title":"Ultralytics \ud83d\udcda\u00b6","text":""},{"location":"notebooks/quickstart/#inference","title":"Inference \ud83d\udcda\u00b6","text":""},{"location":"notebooks/quickstart/#yolo-nas","title":"YOLO-NAS \ud83d\udcda\u00b6","text":""},{"location":"notebooks/quickstart/#annotate","title":"\ud83d\udc69\u200d\ud83c\udfa8 Annotate\u00b6","text":""},{"location":"notebooks/quickstart/#boxannotator","title":"BoxAnnotator \ud83d\udcda\u00b6","text":""},{"location":"notebooks/quickstart/#maskannotator","title":"MaskAnnotator \ud83d\udcda\u00b6","text":""},{"location":"notebooks/quickstart/#filter","title":"\ud83d\uddd1 Filter \ud83d\udcda\u00b6","text":""},{"location":"notebooks/quickstart/#by-index-index-list-and-index-slice","title":"By index, index list and index slice\u00b6","text":"<p>NOTE: <code>sv.Detections</code> filter API allows you to access detections by index, index list or index slice</p>"},{"location":"notebooks/quickstart/#by-class_id","title":"By class_id\u00b6","text":"<p>NOTE: Let's use <code>sv.Detections</code> filter API to display only objects with <code>class_id == 0</code></p>"},{"location":"notebooks/quickstart/#by-confidence","title":"By confidence\u00b6","text":"<p>NOTE: Let's use <code>sv.Detections</code> filter API to display only objects with <code>confidence &gt; 0.7</code></p>"},{"location":"notebooks/quickstart/#by-advanced-logical-condition","title":"By advanced logical condition\u00b6","text":"<p>NOTE: Let's use <code>sv.Detections</code> filter API allows you to build advanced logical conditions. Let's select only detections with <code>class_id != 0</code> and <code>confidence &gt; 0.7</code>.</p>"},{"location":"notebooks/quickstart/#video-api","title":"\ud83c\udfac Video API\u00b6","text":"<p>NOTE: <code>supervision</code> offers a lot of utils to make working with videos easier. Let's take a look at some of them.</p>"},{"location":"notebooks/quickstart/#videoinfo","title":"VideoInfo \ud83d\udcda\u00b6","text":"<p>NOTE: <code>VideoInfo</code> allows us to easily retrieve information about video files, such as resolution, FPS and total number of frames.</p>"},{"location":"notebooks/quickstart/#get_video_frames_generator","title":"get_video_frames_generator \ud83d\udcda\u00b6","text":""},{"location":"notebooks/quickstart/#videosink","title":"VideoSink \ud83d\udcda\u00b6","text":""},{"location":"notebooks/quickstart/#dataset-api","title":"\ud83d\uddbc\ufe0f Dataset API\u00b6","text":"<p>NOTE: In order to demonstrate the capabilities of the Dataset API, we need a dataset. Let's download one from Roboflow Universe. To do this we first need to install the <code>roboflow</code> pip package.</p>"},{"location":"notebooks/quickstart/#detectiondatasetfrom_yolo","title":"DetectionDataset.from_yolo \ud83d\udcda\u00b6","text":"<p>NOTE: Currently Dataset API always loads loads images from hard drive. In the future, we plan to add lazy loading.</p>"},{"location":"notebooks/quickstart/#visualize-annotations","title":"\ud83c\udff7\ufe0f Visualize annotations\u00b6","text":""},{"location":"notebooks/quickstart/#split","title":"split \ud83d\udcda\u00b6","text":""},{"location":"notebooks/quickstart/#detectiondatasetas_pascal_voc","title":"DetectionDataset.as_pascal_voc \ud83d\udcda\u00b6","text":""},{"location":"notebooks/quickstart/#congratulations","title":"\ud83c\udfc6 Congratulations\u00b6","text":""},{"location":"notebooks/quickstart/#learning-resources","title":"Learning Resources\u00b6","text":"<ul> <li>Documentation</li> <li>GitHub</li> <li>YouTube Supervision Playlist</li> </ul>"},{"location":"notebooks/serialise-detections-to-csv/","title":"Serialise Detections to a CSV File","text":"<p>Click the <code>Open in Colab</code> button to run the cookbook on Google Colab.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install -q inference requests tqdm supervision==0.21.0\n</pre> !pip install -q inference requests tqdm supervision==0.21.0 In\u00a0[5]: Copied! <pre>import csv\nfrom typing import List\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\n\nimport supervision as sv\nfrom supervision.assets import download_assets, VideoAssets\nfrom inference import InferencePipeline\nfrom inference.core.interfaces.camera.entities import VideoFrame\n</pre> import csv from typing import List from collections import defaultdict  import numpy as np import pandas as pd  import supervision as sv from supervision.assets import download_assets, VideoAssets from inference import InferencePipeline from inference.core.interfaces.camera.entities import VideoFrame  <p>The parameters defined below are:</p> <ul> <li><code>SOURCE_VIDEO_PATH</code> - the path to the input video</li> <li><code>CONFIDENCE_THRESHOLD</code> - do not include detections below this confidence level</li> <li><code>IOU_THRESHOLD</code> - discard detections that overlap with others by more than this IOU ratio</li> <li><code>FILE_NAME</code> - write the json output to this file</li> <li><code>INFERENCE_MODEL</code> - model id. This cookbook uses a model alias, but it can also be a fine-tuned model or a model from the Universe.</li> </ul> In\u00a0[6]: Copied! <pre>SOURCE_VIDEO_PATH = download_assets(VideoAssets.PEOPLE_WALKING)\nCONFIDENCE_THRESHOLD = 0.3\nIOU_THRESHOLD = 0.7\nFILE_NAME = \"detections.csv\"\nINFERENCE_MODEL = \"yolov8n-640\"\n</pre> SOURCE_VIDEO_PATH = download_assets(VideoAssets.PEOPLE_WALKING) CONFIDENCE_THRESHOLD = 0.3 IOU_THRESHOLD = 0.7 FILE_NAME = \"detections.csv\" INFERENCE_MODEL = \"yolov8n-640\" <pre>people-walking.mp4 asset download complete. \n\n</pre> <p>As a result of executing the above <code>download_assets(VideoAssets.PEOPLE_WALKING)</code> , you will download a video file and save it at the <code>SOURCE_VIDEO_PATH</code>. Keep in mind that the video preview below works only in the web version of the cookbooks and not in Google Colab.</p> In\u00a0[7]: Copied! <pre>generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\nframe = next(generator)\n\nsv.plot_image(frame, (12, 12))\n</pre> generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH) frame = next(generator)  sv.plot_image(frame, (12, 12)) In\u00a0[8]: Copied! <pre>byte_track = sv.ByteTrack(minimum_consecutive_frames=3)\nbyte_track.reset()\n</pre> byte_track = sv.ByteTrack(minimum_consecutive_frames=3) byte_track.reset() In\u00a0[32]: Copied! <pre>csv_sink = sv.CSVSink(FILE_NAME)\ncsv_sink.open()\n</pre> csv_sink = sv.CSVSink(FILE_NAME) csv_sink.open() In\u00a0[33]: Copied! <pre>def callback(predictions: dict, frame: VideoFrame) -&gt; np.ndarray:\n    detections = sv.Detections.from_inference(predictions)\n\n    # Only keep person detections\n    detections = detections[detections.class_id == 0]\n    detections.data[\"class_name\"] = np.array([\"person\" for _ in range(len(detections))])\n\n    detections = byte_track.update_with_detections(detections)\n    csv_sink.append(detections, custom_data={'frame_number': frame.frame_id})\n</pre> def callback(predictions: dict, frame: VideoFrame) -&gt; np.ndarray:     detections = sv.Detections.from_inference(predictions)      # Only keep person detections     detections = detections[detections.class_id == 0]     detections.data[\"class_name\"] = np.array([\"person\" for _ in range(len(detections))])      detections = byte_track.update_with_detections(detections)     csv_sink.append(detections, custom_data={'frame_number': frame.frame_id}) In\u00a0[34]: Copied! <pre>pipeline = InferencePipeline.init(\n    model_id=INFERENCE_MODEL,\n    video_reference=SOURCE_VIDEO_PATH,\n    on_prediction=callback,\n    iou_threshold=IOU_THRESHOLD,\n    confidence=CONFIDENCE_THRESHOLD,\n)\n</pre> pipeline = InferencePipeline.init(     model_id=INFERENCE_MODEL,     video_reference=SOURCE_VIDEO_PATH,     on_prediction=callback,     iou_threshold=IOU_THRESHOLD,     confidence=CONFIDENCE_THRESHOLD, ) <pre>UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\nUserWarning: Specified provider 'OpenVINOExecutionProvider' is not in available provider names.Available providers: 'CPUExecutionProvider'\n</pre> In\u00a0[35]: Copied! <pre>pipeline.start()\npipeline.join()\n</pre> pipeline.start() pipeline.join() In\u00a0[36]: Copied! <pre>csv_sink.close()\n</pre> csv_sink.close() In\u00a0[37]: Copied! <pre>df = pd.read_csv(FILE_NAME)\ndf\n</pre> df = pd.read_csv(FILE_NAME) df Out[37]: x_min y_min x_max y_max class_id confidence tracker_id class_name frame_number 0 1142.0 950.0 1245.0 1080.0 0 0.767850 185 person 1 1 750.0 450.0 823.0 620.0 0 0.748268 168 person 1 2 1419.0 702.0 1526.0 887.0 0 0.550286 179 person 1 3 1674.0 12.0 1727.0 141.0 0 0.546864 174 person 1 4 1616.0 18.0 1655.0 127.0 0 0.520566 181 person 1 ... ... ... ... ... ... ... ... ... ... 8757 394.0 471.0 460.0 633.0 0 0.466400 244 person 341 8758 115.0 256.0 199.0 399.0 0 0.431374 223 person 341 8759 77.0 700.0 169.0 892.0 0 0.416318 240 person 341 8760 1010.0 59.0 1055.0 174.0 0 0.403422 246 person 341 8761 1758.0 0.0 1809.0 96.0 0 0.313793 236 person 341 <p>8762 rows \u00d7 9 columns</p> In\u00a0[38]: Copied! <pre>def csv_to_detections(csv_file: str) -&gt; List[sv.Detections]:\n    rows_by_frame_number = defaultdict(list)\n    with open(csv_file, 'r') as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            frame_number = int(row[\"frame_number\"])\n            rows_by_frame_number[frame_number].append(row)\n\n    detections_list = []\n    for frame_number, rows in rows_by_frame_number.items():\n        xyxy = []\n        class_id = []\n        confidence = []\n        tracker_id = []\n        custom_data = defaultdict(list)\n\n        for row in rows:\n            xyxy.append([row[key] for key in [\"x_min\", \"y_min\", \"x_max\", \"y_max\"]])\n            class_id.append(row[\"class_id\"])\n            confidence.append(row[\"confidence\"])\n            tracker_id.append(row[\"tracker_id\"])\n\n            for custom_key in row.keys():\n                if custom_key in [\"x_min\", \"y_min\", \"x_max\", \"y_max\", \"class_id\", \"confidence\", \"tracker_id\"]:\n                    continue\n                custom_data[custom_key].append(row[custom_key])\n\n        if all([val == \"\" for val in class_id]):\n            class_id = None\n        if all([val == \"\" for val in confidence]):\n            confidence = None\n        if all([val == \"\" for val in tracker_id]):\n            tracker_id = None\n\n        detections_list.append(\n            sv.Detections(\n                xyxy=np.array(xyxy, dtype=np.float32),\n                class_id=np.array(class_id, dtype=int),\n                confidence=np.array(confidence, dtype=np.float32),\n                tracker_id=np.array(tracker_id, dtype=int),\n                data=dict(custom_data)\n            )\n        )\n    \n    return detections_list\n</pre> def csv_to_detections(csv_file: str) -&gt; List[sv.Detections]:     rows_by_frame_number = defaultdict(list)     with open(csv_file, 'r') as f:         reader = csv.DictReader(f)         for row in reader:             frame_number = int(row[\"frame_number\"])             rows_by_frame_number[frame_number].append(row)      detections_list = []     for frame_number, rows in rows_by_frame_number.items():         xyxy = []         class_id = []         confidence = []         tracker_id = []         custom_data = defaultdict(list)          for row in rows:             xyxy.append([row[key] for key in [\"x_min\", \"y_min\", \"x_max\", \"y_max\"]])             class_id.append(row[\"class_id\"])             confidence.append(row[\"confidence\"])             tracker_id.append(row[\"tracker_id\"])              for custom_key in row.keys():                 if custom_key in [\"x_min\", \"y_min\", \"x_max\", \"y_max\", \"class_id\", \"confidence\", \"tracker_id\"]:                     continue                 custom_data[custom_key].append(row[custom_key])          if all([val == \"\" for val in class_id]):             class_id = None         if all([val == \"\" for val in confidence]):             confidence = None         if all([val == \"\" for val in tracker_id]):             tracker_id = None          detections_list.append(             sv.Detections(                 xyxy=np.array(xyxy, dtype=np.float32),                 class_id=np.array(class_id, dtype=int),                 confidence=np.array(confidence, dtype=np.float32),                 tracker_id=np.array(tracker_id, dtype=int),                 data=dict(custom_data)             )         )          return detections_list In\u00a0[39]: Copied! <pre>detections_list = csv_to_detections(FILE_NAME)\ndetections_list\n\nprint(f\"Detections: {len(detections_list)}\")\nprint(detections_list[0])\n</pre> detections_list = csv_to_detections(FILE_NAME) detections_list  print(f\"Detections: {len(detections_list)}\") print(detections_list[0]) <pre>Detections: 341\nDetections(xyxy=array([[1142.,  950., 1245., 1080.],\n       [ 750.,  450.,  823.,  620.],\n       [1419.,  702., 1526.,  887.],\n       [1674.,   12., 1727.,  141.],\n       [1616.,   18., 1655.,  127.],\n       [1388.,    2., 1437.,  184.]], dtype=float32), mask=None, confidence=array([0.7678498 , 0.7482683 , 0.5502863 , 0.5468636 , 0.5205659 ,\n       0.31684005], dtype=float32), class_id=array([0, 0, 0, 0, 0, 0]), tracker_id=array([185, 168, 179, 174, 181,  28]), data={'class_name': ['person', 'person', 'person', 'person', 'person', 'person'], 'frame_number': ['1', '1', '1', '1', '1', '1']})\n</pre> In\u00a0[40]: Copied! <pre>FRAME_NUMBER = 100\n\ndetections = detections_list[FRAME_NUMBER]\nframe_number = detections.data[\"frame_number\"][0]\n\n\ngenerator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH, start=FRAME_NUMBER)\nframe = next(generator)\n</pre> FRAME_NUMBER = 100  detections = detections_list[FRAME_NUMBER] frame_number = detections.data[\"frame_number\"][0]   generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH, start=FRAME_NUMBER) frame = next(generator) In\u00a0[41]: Copied! <pre>bounding_box_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_frame = frame.copy()\nannotated_frame = bounding_box_annotator.annotate(scene=annotated_frame, detections=detections)\nannotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections)\nsv.plot_image(annotated_frame, (12, 12))\n</pre> bounding_box_annotator = sv.BoundingBoxAnnotator() label_annotator = sv.LabelAnnotator()  annotated_frame = frame.copy() annotated_frame = bounding_box_annotator.annotate(scene=annotated_frame, detections=detections) annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections) sv.plot_image(annotated_frame, (12, 12))"},{"location":"notebooks/serialise-detections-to-csv/#serialise-detections-to-a-csv-file","title":"Serialise Detections to a CSV File\u00b6","text":"<p>This cookbook introduce sv.CSVSink tool designed to write captured object detection data to file from video streams/file</p>"},{"location":"notebooks/serialise-detections-to-csv/#read-single-frame-from-video","title":"Read single frame from video\u00b6","text":"<p>The <code>get_video_frames_generator</code> enables us to easily iterate over video frames. Let's create a video generator for our sample input file and display its first frame on the screen.</p>"},{"location":"notebooks/serialise-detections-to-csv/#initialize-bytetrack","title":"Initialize ByteTrack\u00b6","text":"<p>ByteTrack is a multi-object tracking algorithm used by Supervision to track and link detected objects across multiple frames, providing consistent IDs for each object.Initialize the  object.</p>"},{"location":"notebooks/serialise-detections-to-csv/#initialize-csvsink","title":"Initialize CSVSink\u00b6","text":"<p>To save detections to a <code>CSV</code> file, open our <code>sv.CSVSink</code> and then pass the <code>sv.Detections</code> object resulting from the inference to it.</p> <p>Note that empty detections will be skipped.</p>"},{"location":"notebooks/serialise-detections-to-csv/#process-video-and-save-detections-to-csv-file","title":"Process video and save detections to csv file\u00b6","text":"<p>The <code>InferencePipeline</code> interface is made for streaming and is likely the best route to go for real time use cases. It is an asynchronous interface that can consume many different video sources including local devices (like webcams), RTSP video streams, video files, etc. With this interface, you define the source of a video stream and sinks.</p> <p>All the operations we plan to perform for each frame of our video - detection, tracking, annotation, and write to csv are encapsulated in a function named <code>callback</code>.</p>"},{"location":"notebooks/serialise-detections-to-csv/#visualizate-results-of-detections-csv-data-with-pandas","title":"Visualizate results of detections csv data with Pandas\u00b6","text":"<p>Let's take a look at our resulting data with by using Pandas.</p> <p>It will also be created in your current directory with the name detections.csv as well.</p>"},{"location":"notebooks/serialise-detections-to-csv/#convert-csv-data-to-svdetections","title":"Convert CSV data to sv.Detections\u00b6","text":""},{"location":"notebooks/serialise-detections-to-csv/#annotate-first-frame","title":"Annotate First Frame\u00b6","text":"<p>Visualize the first frame of a video alongside the initial detections obtained by parsing CSV data into <code>sv.Detections</code> objects. The annotated image will show the original video frame, marked with the first bounding box detected from the parsed data, providing a visual representation of the identified object(s) in the scene.</p>"},{"location":"notebooks/serialise-detections-to-csv/#get-back-svdetections","title":"Get back <code>sv.Detections</code>\u00b6","text":""},{"location":"notebooks/serialise-detections-to-csv/#annotate-image-with-detections","title":"Annotate Image with Detections\u00b6","text":"<p>Finally, we can annotate the image with the predictions. Since we are working with an object detection model, we will use the <code>sv.BoundingBoxAnnotator</code> and <code>sv.LabelAnnotator</code> classes.</p>"},{"location":"notebooks/serialise-detections-to-csv/#references","title":"References \ud83d\udcda\u00b6","text":"<ul> <li>Supervision: https://supervision.roboflow.com</li> <li>sv.Detections: https://supervision.roboflow.com/develop/detection/core/#detections</li> <li>Save Detections to CSV: https://supervision.roboflow.com/develop/how_to/save_detections/#save-detections-as-csv</li> <li>Custom fields: https://supervision.roboflow.com/develop/how_to/save_detections/#custom-fields</li> <li>ByteTrack: https://supervision.roboflow.com/trackers/#supervision.tracker.byte_tracker.core.ByteTrack</li> <li>Inference: https://inference.roboflow.com/</li> <li>Inference Pipeline: https://inference.roboflow.com/using_inference/inference_pipeline/</li> <li>Inference Aliases: https://inference.roboflow.com/quickstart/aliases/</li> </ul>"},{"location":"notebooks/serialise-detections-to-json/","title":"Serialise Detections to a JSON File","text":"<p>Click the <code>Open in Colab</code> button to run the cookbook on Google Colab.</p> In\u00a0[\u00a0]: Copied! <pre>!pip install -q inference requests tqdm supervision==0.21.0\n</pre> !pip install -q inference requests tqdm supervision==0.21.0 In\u00a0[1]: Copied! <pre>import json\nfrom typing import List\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\n\nimport supervision as sv\nfrom supervision.assets import download_assets, VideoAssets\nfrom inference import InferencePipeline\nfrom inference.core.interfaces.camera.entities import VideoFrame\n</pre> import json from typing import List from collections import defaultdict  import numpy as np import pandas as pd  import supervision as sv from supervision.assets import download_assets, VideoAssets from inference import InferencePipeline from inference.core.interfaces.camera.entities import VideoFrame  <p>The parameters defined below are:</p> <ul> <li><code>SOURCE_VIDEO_PATH</code> - the path to the input video</li> <li><code>CONFIDENCE_THRESHOLD</code> - do not include detections below this confidence level</li> <li><code>IOU_THRESHOLD</code> - discard detections that overlap with others by more than this IOU ratio</li> <li><code>FILE_NAME</code> - write the json output to this file</li> <li><code>INFERENCE_MODEL</code> - model id. This cookbook uses a model alias, but it can also be a fine-tuned model or a model from the Universe.</li> </ul> In\u00a0[2]: Copied! <pre>SOURCE_VIDEO_PATH = download_assets(VideoAssets.PEOPLE_WALKING)\nCONFIDENCE_THRESHOLD = 0.3\nIOU_THRESHOLD = 0.7\nFILE_NAME = \"detections.json\"\nINFERENCE_MODEL = \"yolov8n-640\"\n</pre> SOURCE_VIDEO_PATH = download_assets(VideoAssets.PEOPLE_WALKING) CONFIDENCE_THRESHOLD = 0.3 IOU_THRESHOLD = 0.7 FILE_NAME = \"detections.json\" INFERENCE_MODEL = \"yolov8n-640\" <pre>people-walking.mp4 asset download complete. \n\n</pre> <p>As a result of executing the above <code>download_assets(VideoAssets.PEOPLE_WALKING)</code> , you will download a video file and save it at the <code>SOURCE_VIDEO_PATH</code>. Keep in mind that the video preview below works only in the web version of the cookbooks and not in Google Colab.</p> In\u00a0[3]: Copied! <pre>generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\nframe = next(generator)\nsv.plot_image(frame, (12, 12))\n</pre> generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH) frame = next(generator) sv.plot_image(frame, (12, 12)) <p>We can also use <code>VideoInfo.from_video_path</code> to learn basic information about our video, such as duration, resolution, or FPS.</p> In\u00a0[4]: Copied! <pre>sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n</pre> sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH) Out[4]: <pre>VideoInfo(width=1920, height=1080, fps=25, total_frames=341)</pre> In\u00a0[5]: Copied! <pre>byte_track = sv.ByteTrack(minimum_consecutive_frames=3)\nbyte_track.reset()\n</pre> byte_track = sv.ByteTrack(minimum_consecutive_frames=3) byte_track.reset() In\u00a0[40]: Copied! <pre>json_sink = sv.JSONSink(FILE_NAME)\njson_sink.open()\n</pre> json_sink = sv.JSONSink(FILE_NAME) json_sink.open() In\u00a0[41]: Copied! <pre>def callback(predictions: dict, frame: VideoFrame) -&gt; np.ndarray:\n    detections = sv.Detections.from_inference(predictions)\n    \n    # Only keep person detections\n    detections = detections[detections.class_id == 0]\n    detections.data[\"class_name\"] = np.array([\"person\" for _ in range(len(detections))])\n\n    detections = byte_track.update_with_detections(detections)\n    json_sink.append(detections, custom_data={'frame_number': frame.frame_id})\n</pre> def callback(predictions: dict, frame: VideoFrame) -&gt; np.ndarray:     detections = sv.Detections.from_inference(predictions)          # Only keep person detections     detections = detections[detections.class_id == 0]     detections.data[\"class_name\"] = np.array([\"person\" for _ in range(len(detections))])      detections = byte_track.update_with_detections(detections)     json_sink.append(detections, custom_data={'frame_number': frame.frame_id}) In\u00a0[42]: Copied! <pre>pipeline = InferencePipeline.init(\n    model_id=INFERENCE_MODEL,\n    video_reference=SOURCE_VIDEO_PATH,\n    on_prediction=callback,\n    iou_threshold=IOU_THRESHOLD,\n    confidence=CONFIDENCE_THRESHOLD,\n)\n</pre> pipeline = InferencePipeline.init(     model_id=INFERENCE_MODEL,     video_reference=SOURCE_VIDEO_PATH,     on_prediction=callback,     iou_threshold=IOU_THRESHOLD,     confidence=CONFIDENCE_THRESHOLD, ) In\u00a0[43]: Copied! <pre>pipeline.start()\npipeline.join()\n</pre> pipeline.start() pipeline.join() In\u00a0[44]: Copied! <pre>json_sink.write_and_close()\n</pre> json_sink.write_and_close() In\u00a0[45]: Copied! <pre>df = pd.read_json(FILE_NAME)\ndf\n</pre> df = pd.read_json(FILE_NAME) df Out[45]: x_min y_min x_max y_max class_id confidence tracker_id class_name frame_number 0 1142 950 1245 1080 0 0.767850 365 person 1 1 750 450 823 620 0 0.748268 348 person 1 2 1419 702 1526 887 0 0.550286 359 person 1 3 1674 12 1727 141 0 0.546864 354 person 1 4 1616 18 1655 127 0 0.520566 361 person 1 ... ... ... ... ... ... ... ... ... ... 8757 394 471 460 633 0 0.466400 424 person 341 8758 115 256 199 399 0 0.431374 403 person 341 8759 77 700 169 892 0 0.416318 420 person 341 8760 1010 59 1055 174 0 0.403422 426 person 341 8761 1758 0 1809 96 0 0.313793 416 person 341 <p>8762 rows \u00d7 9 columns</p> In\u00a0[46]: Copied! <pre>def json_to_detections(json_file: str) -&gt; List[sv.Detections]:\n    rows_by_frame_number = defaultdict(list)\n    with open(json_file, \"r\") as f:\n        data = json.load(f)\n    for row in data:\n        frame_number = int(row[\"frame_number\"])\n        rows_by_frame_number[frame_number].append(row)\n\n    detections_list = []\n    for frame_number, rows in rows_by_frame_number.items():\n        xyxy = []\n        class_id = []\n        confidence = []\n        tracker_id = []\n        custom_data = defaultdict(list)\n\n        for row in rows:\n            xyxy.append([row[key] for key in [\"x_min\", \"y_min\", \"x_max\", \"y_max\"]])\n            class_id.append(row[\"class_id\"])\n            confidence.append(row[\"confidence\"])\n            tracker_id.append(row[\"tracker_id\"])\n\n            for custom_key in row.keys():\n                if custom_key in [\"x_min\", \"y_min\", \"x_max\", \"y_max\", \"class_id\", \"confidence\", \"tracker_id\"]:\n                    continue\n                custom_data[custom_key].append(row[custom_key])\n\n        if all([val == \"\" for val in class_id]):\n            class_id = None\n        if all([val == \"\" for val in confidence]):\n            confidence = None\n        if all([val == \"\" for val in tracker_id]):\n            tracker_id = None\n\n        detections_list.append(\n            sv.Detections(\n                xyxy=np.array(xyxy, dtype=np.float32),\n                class_id=np.array(class_id, dtype=int),\n                confidence=np.array(confidence, dtype=np.float32),\n                tracker_id=np.array(tracker_id, dtype=int),\n                data=dict(custom_data)\n            )\n        )\n    \n    return detections_list\n</pre> def json_to_detections(json_file: str) -&gt; List[sv.Detections]:     rows_by_frame_number = defaultdict(list)     with open(json_file, \"r\") as f:         data = json.load(f)     for row in data:         frame_number = int(row[\"frame_number\"])         rows_by_frame_number[frame_number].append(row)      detections_list = []     for frame_number, rows in rows_by_frame_number.items():         xyxy = []         class_id = []         confidence = []         tracker_id = []         custom_data = defaultdict(list)          for row in rows:             xyxy.append([row[key] for key in [\"x_min\", \"y_min\", \"x_max\", \"y_max\"]])             class_id.append(row[\"class_id\"])             confidence.append(row[\"confidence\"])             tracker_id.append(row[\"tracker_id\"])              for custom_key in row.keys():                 if custom_key in [\"x_min\", \"y_min\", \"x_max\", \"y_max\", \"class_id\", \"confidence\", \"tracker_id\"]:                     continue                 custom_data[custom_key].append(row[custom_key])          if all([val == \"\" for val in class_id]):             class_id = None         if all([val == \"\" for val in confidence]):             confidence = None         if all([val == \"\" for val in tracker_id]):             tracker_id = None          detections_list.append(             sv.Detections(                 xyxy=np.array(xyxy, dtype=np.float32),                 class_id=np.array(class_id, dtype=int),                 confidence=np.array(confidence, dtype=np.float32),                 tracker_id=np.array(tracker_id, dtype=int),                 data=dict(custom_data)             )         )          return detections_list In\u00a0[47]: Copied! <pre>detections_list = json_to_detections(FILE_NAME)\ndetections_list\n\nprint(f\"Detections: {len(detections_list)}\")\nprint(detections_list[0])\n</pre> detections_list = json_to_detections(FILE_NAME) detections_list  print(f\"Detections: {len(detections_list)}\") print(detections_list[0]) <pre>Detections: 341\nDetections(xyxy=array([[1142.,  950., 1245., 1080.],\n       [ 750.,  450.,  823.,  620.],\n       [1419.,  702., 1526.,  887.],\n       [1674.,   12., 1727.,  141.],\n       [1616.,   18., 1655.,  127.],\n       [1388.,    2., 1437.,  184.]], dtype=float32), mask=None, confidence=array([0.7678498 , 0.7482683 , 0.5502863 , 0.5468636 , 0.5205659 ,\n       0.31684005], dtype=float32), class_id=array([0, 0, 0, 0, 0, 0]), tracker_id=array([365, 348, 359, 354, 361,  28]), data={'class_name': ['person', 'person', 'person', 'person', 'person', 'person'], 'frame_number': [1, 1, 1, 1, 1, 1]})\n</pre> In\u00a0[48]: Copied! <pre>FRAME_NUMBER = 100\n\ndetections = detections_list[FRAME_NUMBER]\nframe_number = detections.data[\"frame_number\"][0]\n\n\ngenerator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH, start=FRAME_NUMBER)\nframe = next(generator)\n</pre> FRAME_NUMBER = 100  detections = detections_list[FRAME_NUMBER] frame_number = detections.data[\"frame_number\"][0]   generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH, start=FRAME_NUMBER) frame = next(generator) In\u00a0[49]: Copied! <pre>bounding_box_annotator = sv.BoundingBoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nannotated_frame = frame.copy()\nannotated_frame = bounding_box_annotator.annotate(scene=annotated_frame, detections=detections)\nannotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections)\nsv.plot_image(annotated_frame, (12, 12))\n</pre> bounding_box_annotator = sv.BoundingBoxAnnotator() label_annotator = sv.LabelAnnotator()  annotated_frame = frame.copy() annotated_frame = bounding_box_annotator.annotate(scene=annotated_frame, detections=detections) annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections) sv.plot_image(annotated_frame, (12, 12))"},{"location":"notebooks/serialise-detections-to-json/#serialise-detections-to-a-json-file","title":"Serialise Detections to a JSON File\u00b6","text":"<p>This cookbook introduce sv.JSONSink tool designed to write captured object detection data to file from video streams/file</p>"},{"location":"notebooks/serialise-detections-to-json/#read-single-frame-from-video","title":"Read single frame from video\u00b6","text":"<p>The <code>get_video_frames_generator</code> enables us to easily iterate over video frames. Let's create a video generator for our sample input file and display its first frame on the screen.</p>"},{"location":"notebooks/serialise-detections-to-json/#initialize-bytetrack","title":"Initialize ByteTrack\u00b6","text":"<p>ByteTrack is a multi-object tracking algorithm used by Supervision to track and link detected objects across multiple frames, providing consistent IDs for each object.</p>"},{"location":"notebooks/serialise-detections-to-json/#initialize-svjsonsink","title":"Initialize sv.JSONSink\u00b6","text":"<p>To save detections to a <code>JSON</code> file, open our <code>sv.JSONSink</code> and then pass the <code>sv.Detections</code> object resulting from the inference to it.</p> <p>Note that empty detections will be skipped.</p>"},{"location":"notebooks/serialise-detections-to-json/#process-video-and-save-detections-to-json-file","title":"Process video and save detections to json file\u00b6","text":"<p>The <code>InferencePipeline</code> interface is made for streaming and is likely the best route to go for real time use cases. It is an asynchronous interface that can consume many different video sources including local devices (like webcams), RTSP video streams, video files, etc. With this interface, you define the source of a video stream and sinks.</p> <p>All the operations we plan to perform for each frame of our video - detection, tracking, annotation, and write to json - are encapsulated in a function named <code>callback</code>.</p>"},{"location":"notebooks/serialise-detections-to-json/#visualizate-results-of-detections-json-data-with-pandas","title":"Visualizate results of detections json data with Pandas\u00b6","text":"<p>Let's take a look at our resulting data with by using Pandas.</p> <p>It will also be created in your current directory with the name detections.json as well.</p>"},{"location":"notebooks/serialise-detections-to-json/#convert-json-data-to-svdetections","title":"Convert JSON data to sv.Detections\u00b6","text":""},{"location":"notebooks/serialise-detections-to-json/#annotate-first-frame","title":"Annotate First Frame\u00b6","text":"<p>Visualize the first frame of a video alongside the initial detections obtained by parsing JSON data into <code>sv.Detections</code> objects. The annotated image will show the original video frame, marked with the first bounding box detected from the parsed data, providing a visual representation of the identified object(s) in the scene.</p>"},{"location":"notebooks/serialise-detections-to-json/#get-back-svdetections","title":"Get back <code>sv.Detections</code>\u00b6","text":""},{"location":"notebooks/serialise-detections-to-json/#first-frame-from-video-before-annotate","title":"First frame from video (Before Annotate)\u00b6","text":""},{"location":"notebooks/serialise-detections-to-json/#annotate-image-with-detections","title":"Annotate Image with Detections\u00b6","text":"<p>Finally, we can annotate the image with the predictions. Since we are working with an object detection model, we will use the <code>sv.BoundingBoxAnnotator</code> and <code>sv.LabelAnnotator</code> classes.</p>"},{"location":"notebooks/serialise-detections-to-json/#references","title":"References \ud83d\udcda\u00b6","text":"<ul> <li>Supervision: https://supervision.roboflow.com</li> <li>sv.Detections: https://supervision.roboflow.com/develop/detection/core/#detections</li> <li>Save Detections to JSON: https://supervision.roboflow.com/develop/how_to/save_detections/#save-detections-as-json</li> <li>Custom fields: https://supervision.roboflow.com/develop/how_to/save_detections/#custom-fields</li> <li>ByteTrack: https://supervision.roboflow.com/trackers/#supervision.tracker.byte_tracker.core.ByteTrack</li> <li>Inference: https://inference.roboflow.com/</li> <li>Inference Pipeline: https://inference.roboflow.com/using_inference/inference_pipeline/</li> <li>Inference Aliases: https://inference.roboflow.com/quickstart/aliases/</li> </ul>"},{"location":"notebooks/zero-shot-object-detection-with-yolo-world/","title":"Zero-Shot Object Detection with YOLO-World","text":"In\u00a0[1]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi <pre>Fri Feb 16 12:46:14 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   65C    P8              13W /  70W |      0MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n</pre> <p>NOTE: To make it easier for us to manage datasets, images and models we create a <code>HOME</code> constant.</p> In\u00a0[2]: Copied! <pre>import os\nHOME = os.getcwd()\nprint(HOME)\n</pre> import os HOME = os.getcwd() print(HOME) <pre>/content\n</pre> In\u00a0[\u00a0]: Copied! <pre>!pip install -q inference-gpu[yolo-world]==0.9.12rc1\n</pre> !pip install -q inference-gpu[yolo-world]==0.9.12rc1 In\u00a0[\u00a0]: Copied! <pre>!pip install -q supervision==0.19.0rc3\n</pre> !pip install -q supervision==0.19.0rc3 In\u00a0[\u00a0]: Copied! <pre>import cv2\nimport supervision as sv\n\nfrom tqdm import tqdm\nfrom inference.models.yolo_world.yolo_world import YOLOWorld\n</pre> import cv2 import supervision as sv  from tqdm import tqdm from inference.models.yolo_world.yolo_world import YOLOWorld In\u00a0[6]: Copied! <pre>!wget -P {HOME} -q https://media.roboflow.com/notebooks/examples/dog.jpeg\n!wget -P {HOME} -q https://media.roboflow.com/supervision/cookbooks/yellow-filling.mp4\n</pre> !wget -P {HOME} -q https://media.roboflow.com/notebooks/examples/dog.jpeg !wget -P {HOME} -q https://media.roboflow.com/supervision/cookbooks/yellow-filling.mp4 In\u00a0[7]: Copied! <pre>SOURCE_IMAGE_PATH = f\"{HOME}/dog.jpeg\"\nSOURCE_VIDEO_PATH = f\"{HOME}/yellow-filling.mp4\"\n</pre> SOURCE_IMAGE_PATH = f\"{HOME}/dog.jpeg\" SOURCE_VIDEO_PATH = f\"{HOME}/yellow-filling.mp4\" <p>NOTE: If you want to run the cookbook using your own file as input, simply upload video to Google Colab and replace <code>SOURCE_IMAGE_PATH</code> and <code>SOURCE_VIDEO_PATH</code> with the path to your file.</p> In\u00a0[8]: Copied! <pre>model = YOLOWorld(model_id=\"yolo_world/l\")\n</pre> model = YOLOWorld(model_id=\"yolo_world/l\") <p>YOLO-World is a zero-shot model, enabling object detection without any training. You only need to define a prompt as a list of classes (things) you are searching for.</p> In\u00a0[9]: Copied! <pre>classes = [\"person\", \"backpack\", \"dog\", \"eye\", \"nose\", \"ear\", \"tongue\"]\nmodel.set_classes(classes)\n</pre> classes = [\"person\", \"backpack\", \"dog\", \"eye\", \"nose\", \"ear\", \"tongue\"] model.set_classes(classes) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 338M/338M [00:03&lt;00:00, 106MiB/s]\n</pre> <p>We perform detection on our sample image. Then, we convert the result into a <code>sv.Detections</code> object, which will be useful in the later parts of the cookbook.</p> In\u00a0[10]: Copied! <pre>image = cv2.imread(SOURCE_IMAGE_PATH)\nresults = model.infer(image)\ndetections = sv.Detections.from_inference(results)\n</pre> image = cv2.imread(SOURCE_IMAGE_PATH) results = model.infer(image) detections = sv.Detections.from_inference(results) <p>The results we've obtained can be easily visualized with <code>sv.BoundingBoxAnnotator</code> and <code>sv.LabelAnnotator</code>. We can adjust parameters such as line thickness, text scale, line and text color allowing for a highly tailored visualization experience.</p> In\u00a0[11]: Copied! <pre>BOUNDING_BOX_ANNOTATOR = sv.BoundingBoxAnnotator(thickness=2)\nLABEL_ANNOTATOR = sv.LabelAnnotator(text_thickness=2, text_scale=1, text_color=sv.Color.BLACK)\n</pre> BOUNDING_BOX_ANNOTATOR = sv.BoundingBoxAnnotator(thickness=2) LABEL_ANNOTATOR = sv.LabelAnnotator(text_thickness=2, text_scale=1, text_color=sv.Color.BLACK) In\u00a0[12]: Copied! <pre>annotated_image = image.copy()\nannotated_image = BOUNDING_BOX_ANNOTATOR.annotate(annotated_image, detections)\nannotated_image = LABEL_ANNOTATOR.annotate(annotated_image, detections)\nsv.plot_image(annotated_image, (10, 10))\n</pre> annotated_image = image.copy() annotated_image = BOUNDING_BOX_ANNOTATOR.annotate(annotated_image, detections) annotated_image = LABEL_ANNOTATOR.annotate(annotated_image, detections) sv.plot_image(annotated_image, (10, 10)) <p>Note that many classes from our prompt were not detected. This is because the default confidence threshold in Inference is set to <code>0.5</code>. Let's try significantly lowering this value. We've observed that the confidence returned by YOLO-World is significantly lower when querying for classes outside the COCO dataset.</p> In\u00a0[13]: Copied! <pre>image = cv2.imread(SOURCE_IMAGE_PATH)\nresults = model.infer(image, confidence=0.003)\ndetections = sv.Detections.from_inference(results)\n</pre> image = cv2.imread(SOURCE_IMAGE_PATH) results = model.infer(image, confidence=0.003) detections = sv.Detections.from_inference(results) <p>By default, <code>sv.LabelAnnotator</code> displays only the names of objects. To also view the confidence levels associated with each detection, we must define custom <code>labels</code> and pass them to <code>sv.LabelAnnotator</code>.</p> In\u00a0[14]: Copied! <pre>labels = [\n    f\"{classes[class_id]} {confidence:0.3f}\"\n    for class_id, confidence\n    in zip(detections.class_id, detections.confidence)\n]\n\nannotated_image = image.copy()\nannotated_image = BOUNDING_BOX_ANNOTATOR.annotate(annotated_image, detections)\nannotated_image = LABEL_ANNOTATOR.annotate(annotated_image, detections, labels=labels)\nsv.plot_image(annotated_image, (10, 10))\n</pre> labels = [     f\"{classes[class_id]} {confidence:0.3f}\"     for class_id, confidence     in zip(detections.class_id, detections.confidence) ]  annotated_image = image.copy() annotated_image = BOUNDING_BOX_ANNOTATOR.annotate(annotated_image, detections) annotated_image = LABEL_ANNOTATOR.annotate(annotated_image, detections, labels=labels) sv.plot_image(annotated_image, (10, 10)) In\u00a0[15]: Copied! <pre>image = cv2.imread(SOURCE_IMAGE_PATH)\nresults = model.infer(image, confidence=0.003)\ndetections = sv.Detections.from_inference(results).with_nms(threshold=0.1)\n</pre> image = cv2.imread(SOURCE_IMAGE_PATH) results = model.infer(image, confidence=0.003) detections = sv.Detections.from_inference(results).with_nms(threshold=0.1) In\u00a0[16]: Copied! <pre>labels = [\n    f\"{classes[class_id]} {confidence:0.3f}\"\n    for class_id, confidence\n    in zip(detections.class_id, detections.confidence)\n]\n\nannotated_image = image.copy()\nannotated_image = BOUNDING_BOX_ANNOTATOR.annotate(annotated_image, detections)\nannotated_image = LABEL_ANNOTATOR.annotate(annotated_image, detections, labels=labels)\nsv.plot_image(annotated_image, (10, 10))\n</pre> labels = [     f\"{classes[class_id]} {confidence:0.3f}\"     for class_id, confidence     in zip(detections.class_id, detections.confidence) ]  annotated_image = image.copy() annotated_image = BOUNDING_BOX_ANNOTATOR.annotate(annotated_image, detections) annotated_image = LABEL_ANNOTATOR.annotate(annotated_image, detections, labels=labels) sv.plot_image(annotated_image, (10, 10)) <p>The <code>get_video_frames_generator</code> enables us to easily iterate over video frames. Let's create a video generator for our sample input file and display its first frame on the screen.</p> In\u00a0[17]: Copied! <pre>generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\nframe = next(generator)\n\nsv.plot_image(frame, (10, 10))\n</pre> generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH) frame = next(generator)  sv.plot_image(frame, (10, 10)) <p>Let's update our list of classes. This time we are looking for <code>yellow filling</code>. The rest of the code performing detection, filtering and visualization remains unchanged.</p> In\u00a0[23]: Copied! <pre>classes = [\"yellow filling\"]\nmodel.set_classes(classes)\n</pre> classes = [\"yellow filling\"] model.set_classes(classes) In\u00a0[38]: Copied! <pre>results = model.infer(frame, confidence=0.002)\ndetections = sv.Detections.from_inference(results).with_nms(threshold=0.1)\n</pre> results = model.infer(frame, confidence=0.002) detections = sv.Detections.from_inference(results).with_nms(threshold=0.1) In\u00a0[39]: Copied! <pre>annotated_image = frame.copy()\nannotated_image = BOUNDING_BOX_ANNOTATOR.annotate(annotated_image, detections)\nannotated_image = LABEL_ANNOTATOR.annotate(annotated_image, detections)\nsv.plot_image(annotated_image, (10, 10))\n</pre> annotated_image = frame.copy() annotated_image = BOUNDING_BOX_ANNOTATOR.annotate(annotated_image, detections) annotated_image = LABEL_ANNOTATOR.annotate(annotated_image, detections) sv.plot_image(annotated_image, (10, 10)) <p>Our prompt allowed us to locate all filled holes, but we also accidentally marked the entire high-level element. To address this issue, we'll filter detections based on their relative area in relation to the entire video frame. If a detection occupies more than 10% of the frame's total area, it will be discarded.</p> <p>We can use <code>VideoInfo.from_video_path</code> to learn basic information about our video, such as duration, resolution, or FPS.</p> In\u00a0[40]: Copied! <pre>video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\nvideo_info\n</pre> video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH) video_info Out[40]: <pre>VideoInfo(width=1280, height=720, fps=25, total_frames=442)</pre> <p>Knowing the frame's resolution allows us to easily calculate its total area, expressed in pixels.</p> In\u00a0[41]: Copied! <pre>width, height = video_info.resolution_wh\nframe_area = width * height\nframe_area\n</pre> width, height = video_info.resolution_wh frame_area = width * height frame_area Out[41]: <pre>921600</pre> <p>On the other hand, by using <code>sv.Detections.area</code> property, we can learn the area of each individual bounding box.</p> In\u00a0[45]: Copied! <pre>results = model.infer(frame, confidence=0.002)\ndetections = sv.Detections.from_inference(results).with_nms(threshold=0.1)\ndetections.area\n</pre> results = model.infer(frame, confidence=0.002) detections = sv.Detections.from_inference(results).with_nms(threshold=0.1) detections.area Out[45]: <pre>array([ 7.5408e+05,       92844,       11255,       12969,      9875.9,      8007.7,      5433.5])</pre> <p>Now, we can combine these two pieces of information to construct a filtering condition for detections with an area greater than 10% of the entire frame.</p> In\u00a0[46]: Copied! <pre>(detections.area / frame_area) &lt; 0.10\n</pre> (detections.area / frame_area) &lt; 0.10 Out[46]: <pre>array([False, False,  True,  True,  True,  True,  True])</pre> In\u00a0[47]: Copied! <pre>detections = detections[(detections.area / frame_area) &lt; 0.10]\n\nannotated_image = frame.copy()\nannotated_image = BOUNDING_BOX_ANNOTATOR.annotate(annotated_image, detections)\nannotated_image = LABEL_ANNOTATOR.annotate(annotated_image, detections)\nsv.plot_image(annotated_image, (10, 10))\n</pre> detections = detections[(detections.area / frame_area) &lt; 0.10]  annotated_image = frame.copy() annotated_image = BOUNDING_BOX_ANNOTATOR.annotate(annotated_image, detections) annotated_image = LABEL_ANNOTATOR.annotate(annotated_image, detections) sv.plot_image(annotated_image, (10, 10)) <p>Finally, we are ready to process our entire video. Now in truth we can appreciate the speed of YOLO-World.</p> In\u00a0[49]: Copied! <pre>TARGET_VIDEO_PATH = f\"{HOME}/yellow-filling-output.mp4\"\n</pre> TARGET_VIDEO_PATH = f\"{HOME}/yellow-filling-output.mp4\" In\u00a0[50]: Copied! <pre>frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH)\nvideo_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n\nwidth, height = video_info.resolution_wh\nframe_area = width * height\nframe_area\n\nwith sv.VideoSink(target_path=TARGET_VIDEO_PATH, video_info=video_info) as sink:\n    for frame in tqdm(frame_generator, total=video_info.total_frames):\n        results = model.infer(frame, confidence=0.002)\n        detections = sv.Detections.from_inference(results).with_nms(threshold=0.1)\n        detections = detections[(detections.area / frame_area) &lt; 0.10]\n\n        annotated_frame = frame.copy()\n        annotated_frame = BOUNDING_BOX_ANNOTATOR.annotate(annotated_frame, detections)\n        annotated_frame = LABEL_ANNOTATOR.annotate(annotated_frame, detections)\n        sink.write_frame(annotated_frame)\n</pre> frame_generator = sv.get_video_frames_generator(SOURCE_VIDEO_PATH) video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)  width, height = video_info.resolution_wh frame_area = width * height frame_area  with sv.VideoSink(target_path=TARGET_VIDEO_PATH, video_info=video_info) as sink:     for frame in tqdm(frame_generator, total=video_info.total_frames):         results = model.infer(frame, confidence=0.002)         detections = sv.Detections.from_inference(results).with_nms(threshold=0.1)         detections = detections[(detections.area / frame_area) &lt; 0.10]          annotated_frame = frame.copy()         annotated_frame = BOUNDING_BOX_ANNOTATOR.annotate(annotated_frame, detections)         annotated_frame = LABEL_ANNOTATOR.annotate(annotated_frame, detections)         sink.write_frame(annotated_frame) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 442/442 [00:31&lt;00:00, 13.90it/s]\n</pre> <p>Keep in mind that the video preview below works only in the web version of the cookbooks and not in Google Colab.</p>"},{"location":"notebooks/zero-shot-object-detection-with-yolo-world/#zero-shot-object-detection-with-yolo-world","title":"Zero-Shot Object Detection with YOLO-World\u00b6","text":"<p>Click the <code>Open in Colab</code> button to run the cookbook on Google Colab.</p> <p>YOLO-World was designed to solve a limitation of existing zero-shot object detection models: speed. Whereas other state-of-the-art models use Transformers, a powerful but typically slower architecture, YOLO-World uses the faster CNN-based YOLO architecture.</p> <p>According to the paper YOLO-World reached between 35.4 AP with 52.0 FPS for the large version and 26.2 AP with 74.1 FPS for the small version. While the V100 is a powerful GPU, achieving such high FPS on any device is impressive.</p> <p></p>"},{"location":"notebooks/zero-shot-object-detection-with-yolo-world/#before-you-start","title":"Before you start\u00b6","text":"<p>Let's make sure that we have access to GPU. We can use <code>nvidia-smi</code> command to do that. In case of any problems navigate to <code>Edit</code> -&gt; <code>Notebook settings</code> -&gt; <code>Hardware accelerator</code>, set it to <code>GPU</code>, and then click <code>Save</code>.</p>"},{"location":"notebooks/zero-shot-object-detection-with-yolo-world/#install-required-packages","title":"Install required packages\u00b6","text":"<p>In this guide, we utilize two Python packages: <code>inference</code>, for executing zero-shot object detection using YOLO-World, and <code>supervision</code>, for post-processing and visualizing the detected objects.</p>"},{"location":"notebooks/zero-shot-object-detection-with-yolo-world/#imports","title":"Imports\u00b6","text":""},{"location":"notebooks/zero-shot-object-detection-with-yolo-world/#download-example-data","title":"Download example data\u00b6","text":""},{"location":"notebooks/zero-shot-object-detection-with-yolo-world/#run-object-detection","title":"Run Object Detection\u00b6","text":"<p>The Inference package provides the YOLO-World model in three versions: <code>S</code>, <code>M</code>, and <code>L</code>. You can load them by defining model_id as <code>yolo_world/s</code>, <code>yolo_world/m</code>, and <code>yolo_world/l</code>, respectively. The <code>ROBOFLOW_API_KEY</code> is not required to utilize this model.</p>"},{"location":"notebooks/zero-shot-object-detection-with-yolo-world/#adjusting-confidence-level","title":"Adjusting Confidence Level\u00b6","text":""},{"location":"notebooks/zero-shot-object-detection-with-yolo-world/#using-non-max-suppression-nms-to-eliminate-double-detections","title":"Using Non-Max Suppression (NMS) to Eliminate Double Detections\u00b6","text":"<p>To eliminate duplicates, we will use Non-Max Suppression (NMS). NMS evaluates the extent to which detections overlap using the Intersection over Union metric and, upon exceeding a defined threshold, treats them as duplicates. Duplicates are then discarded, starting with those of the lowest confidence. The value should be within the range <code>[0, 1]</code>. The smaller the value, the more restrictive the NMS.</p>"},{"location":"notebooks/zero-shot-object-detection-with-yolo-world/#video-processing","title":"Video Processing\u00b6","text":""},{"location":"notebooks/zero-shot-object-detection-with-yolo-world/#filtering-detectuions-by-area","title":"Filtering Detectuions by Area\u00b6","text":""},{"location":"notebooks/zero-shot-object-detection-with-yolo-world/#final-result","title":"Final Result\u00b6","text":""},{"location":"utils/draw/","title":"Draw Utils","text":"draw_line <p>Draws a line on a given scene.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The scene on which the line will be drawn</p> required <code>start</code> <code>Point</code> <p>The starting point of the line</p> required <code>end</code> <code>Point</code> <p>The end point of the line</p> required <code>color</code> <code>Color</code> <p>The color of the line</p> required <code>thickness</code> <code>int</code> <p>The thickness of the line</p> <code>2</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The scene with the line drawn on it</p> Source code in <code>supervision/draw/utils.py</code> <pre><code>def draw_line(\n    scene: np.ndarray, start: Point, end: Point, color: Color, thickness: int = 2\n) -&gt; np.ndarray:\n    \"\"\"\n    Draws a line on a given scene.\n\n    Parameters:\n        scene (np.ndarray): The scene on which the line will be drawn\n        start (Point): The starting point of the line\n        end (Point): The end point of the line\n        color (Color): The color of the line\n        thickness (int): The thickness of the line\n\n    Returns:\n        np.ndarray: The scene with the line drawn on it\n    \"\"\"\n    cv2.line(\n        scene,\n        start.as_xy_int_tuple(),\n        end.as_xy_int_tuple(),\n        color.as_bgr(),\n        thickness=thickness,\n    )\n    return scene\n</code></pre> draw_rectangle <p>Draws a rectangle on an image.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The scene on which the rectangle will be drawn</p> required <code>rect</code> <code>Rect</code> <p>The rectangle to be drawn</p> required <code>color</code> <code>Color</code> <p>The color of the rectangle</p> required <code>thickness</code> <code>int</code> <p>The thickness of the rectangle border</p> <code>2</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The scene with the rectangle drawn on it</p> Source code in <code>supervision/draw/utils.py</code> <pre><code>def draw_rectangle(\n    scene: np.ndarray, rect: Rect, color: Color, thickness: int = 2\n) -&gt; np.ndarray:\n    \"\"\"\n    Draws a rectangle on an image.\n\n    Parameters:\n        scene (np.ndarray): The scene on which the rectangle will be drawn\n        rect (Rect): The rectangle to be drawn\n        color (Color): The color of the rectangle\n        thickness (int): The thickness of the rectangle border\n\n    Returns:\n        np.ndarray: The scene with the rectangle drawn on it\n    \"\"\"\n    cv2.rectangle(\n        scene,\n        rect.top_left.as_xy_int_tuple(),\n        rect.bottom_right.as_xy_int_tuple(),\n        color.as_bgr(),\n        thickness=thickness,\n    )\n    return scene\n</code></pre> draw_filled_rectangle <p>Draws a filled rectangle on an image.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The scene on which the rectangle will be drawn</p> required <code>rect</code> <code>Rect</code> <p>The rectangle to be drawn</p> required <code>color</code> <code>Color</code> <p>The color of the rectangle</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The scene with the rectangle drawn on it</p> Source code in <code>supervision/draw/utils.py</code> <pre><code>def draw_filled_rectangle(scene: np.ndarray, rect: Rect, color: Color) -&gt; np.ndarray:\n    \"\"\"\n    Draws a filled rectangle on an image.\n\n    Parameters:\n        scene (np.ndarray): The scene on which the rectangle will be drawn\n        rect (Rect): The rectangle to be drawn\n        color (Color): The color of the rectangle\n\n    Returns:\n        np.ndarray: The scene with the rectangle drawn on it\n    \"\"\"\n    cv2.rectangle(\n        scene,\n        rect.top_left.as_xy_int_tuple(),\n        rect.bottom_right.as_xy_int_tuple(),\n        color.as_bgr(),\n        -1,\n    )\n    return scene\n</code></pre> draw_polygon <p>Draw a polygon on a scene.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The scene to draw the polygon on.</p> required <code>polygon</code> <code>ndarray</code> <p>The polygon to be drawn, given as a list of vertices.</p> required <code>color</code> <code>Color</code> <p>The color of the polygon.</p> required <code>thickness</code> <code>int</code> <p>The thickness of the polygon lines, by default 2.</p> <code>2</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The scene with the polygon drawn on it.</p> Source code in <code>supervision/draw/utils.py</code> <pre><code>def draw_polygon(\n    scene: np.ndarray, polygon: np.ndarray, color: Color, thickness: int = 2\n) -&gt; np.ndarray:\n    \"\"\"Draw a polygon on a scene.\n\n    Parameters:\n        scene (np.ndarray): The scene to draw the polygon on.\n        polygon (np.ndarray): The polygon to be drawn, given as a list of vertices.\n        color (Color): The color of the polygon.\n        thickness (int, optional): The thickness of the polygon lines, by default 2.\n\n    Returns:\n        np.ndarray: The scene with the polygon drawn on it.\n    \"\"\"\n    cv2.polylines(\n        scene, [polygon], isClosed=True, color=color.as_bgr(), thickness=thickness\n    )\n    return scene\n</code></pre> draw_text <p>Draw text with background on a scene.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>A 2-dimensional numpy ndarray representing an image or scene</p> required <code>text</code> <code>str</code> <p>The text to be drawn.</p> required <code>text_anchor</code> <code>Point</code> <p>The anchor point for the text, represented as a Point object with x and y attributes.</p> required <code>text_color</code> <code>Color</code> <p>The color of the text. Defaults to black.</p> <code>BLACK</code> <code>text_scale</code> <code>float</code> <p>The scale of the text. Defaults to 0.5.</p> <code>0.5</code> <code>text_thickness</code> <code>int</code> <p>The thickness of the text. Defaults to 1.</p> <code>1</code> <code>text_padding</code> <code>int</code> <p>The amount of padding to add around the text when drawing a rectangle in the background. Defaults to 10.</p> <code>10</code> <code>text_font</code> <code>int</code> <p>The font to use for the text. Defaults to cv2.FONT_HERSHEY_SIMPLEX.</p> <code>FONT_HERSHEY_SIMPLEX</code> <code>background_color</code> <code>Color</code> <p>The color of the background rectangle, if one is to be drawn. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The input scene with the text drawn on it.</p> <p>Examples:</p> <pre><code>import numpy as np\n\nscene = np.zeros((100, 100, 3), dtype=np.uint8)\ntext_anchor = Point(x=50, y=50)\nscene = draw_text(scene=scene, text=\"Hello, world!\",text_anchor=text_anchor)\n</code></pre> Source code in <code>supervision/draw/utils.py</code> <pre><code>def draw_text(\n    scene: np.ndarray,\n    text: str,\n    text_anchor: Point,\n    text_color: Color = Color.BLACK,\n    text_scale: float = 0.5,\n    text_thickness: int = 1,\n    text_padding: int = 10,\n    text_font: int = cv2.FONT_HERSHEY_SIMPLEX,\n    background_color: Optional[Color] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Draw text with background on a scene.\n\n    Parameters:\n        scene (np.ndarray): A 2-dimensional numpy ndarray representing an image or scene\n        text (str): The text to be drawn.\n        text_anchor (Point): The anchor point for the text, represented as a\n            Point object with x and y attributes.\n        text_color (Color, optional): The color of the text. Defaults to black.\n        text_scale (float, optional): The scale of the text. Defaults to 0.5.\n        text_thickness (int, optional): The thickness of the text. Defaults to 1.\n        text_padding (int, optional): The amount of padding to add around the text\n            when drawing a rectangle in the background. Defaults to 10.\n        text_font (int, optional): The font to use for the text.\n            Defaults to cv2.FONT_HERSHEY_SIMPLEX.\n        background_color (Color, optional): The color of the background rectangle,\n            if one is to be drawn. Defaults to None.\n\n    Returns:\n        np.ndarray: The input scene with the text drawn on it.\n\n    Examples:\n        ```python\n        import numpy as np\n\n        scene = np.zeros((100, 100, 3), dtype=np.uint8)\n        text_anchor = Point(x=50, y=50)\n        scene = draw_text(scene=scene, text=\"Hello, world!\",text_anchor=text_anchor)\n        ```\n    \"\"\"\n    text_width, text_height = cv2.getTextSize(\n        text=text,\n        fontFace=text_font,\n        fontScale=text_scale,\n        thickness=text_thickness,\n    )[0]\n\n    text_anchor_x, text_anchor_y = text_anchor.as_xy_int_tuple()\n\n    text_rect = Rect(\n        x=text_anchor_x - text_width // 2,\n        y=text_anchor_y - text_height // 2,\n        width=text_width,\n        height=text_height,\n    ).pad(text_padding)\n\n    if background_color is not None:\n        scene = draw_filled_rectangle(\n            scene=scene, rect=text_rect, color=background_color\n        )\n\n    cv2.putText(\n        img=scene,\n        text=text,\n        org=(text_anchor_x - text_width // 2, text_anchor_y + text_height // 2),\n        fontFace=text_font,\n        fontScale=text_scale,\n        color=text_color.as_bgr(),\n        thickness=text_thickness,\n        lineType=cv2.LINE_AA,\n    )\n    return scene\n</code></pre> draw_image <p>Draws an image onto a given scene with specified opacity and dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>Background image where the new image will be drawn.</p> required <code>image</code> <code>Union[str, ndarray]</code> <p>Image to draw.</p> required <code>opacity</code> <code>float</code> <p>Opacity of the image to be drawn.</p> required <code>rect</code> <code>Rect</code> <p>Rectangle specifying where to draw the image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The updated scene.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the image path does not exist.</p> <code>ValueError</code> <p>For invalid opacity or rectangle dimensions.</p> Source code in <code>supervision/draw/utils.py</code> <pre><code>def draw_image(\n    scene: np.ndarray, image: Union[str, np.ndarray], opacity: float, rect: Rect\n) -&gt; np.ndarray:\n    \"\"\"\n    Draws an image onto a given scene with specified opacity and dimensions.\n\n    Args:\n        scene (np.ndarray): Background image where the new image will be drawn.\n        image (Union[str, np.ndarray]): Image to draw.\n        opacity (float): Opacity of the image to be drawn.\n        rect (Rect): Rectangle specifying where to draw the image.\n\n    Returns:\n        np.ndarray: The updated scene.\n\n    Raises:\n        FileNotFoundError: If the image path does not exist.\n        ValueError: For invalid opacity or rectangle dimensions.\n    \"\"\"\n\n    # Validate and load image\n    if isinstance(image, str):\n        if not os.path.exists(image):\n            raise FileNotFoundError(f\"Image path ('{image}') does not exist.\")\n        image = cv2.imread(image, cv2.IMREAD_UNCHANGED)\n\n    # Validate opacity\n    if not 0.0 &lt;= opacity &lt;= 1.0:\n        raise ValueError(\"Opacity must be between 0.0 and 1.0.\")\n\n    # Validate rectangle dimensions\n    if (\n        rect.x &lt; 0\n        or rect.y &lt; 0\n        or rect.x + rect.width &gt; scene.shape[1]\n        or rect.y + rect.height &gt; scene.shape[0]\n    ):\n        raise ValueError(\"Invalid rectangle dimensions.\")\n\n    # Resize and isolate alpha channel\n    image = cv2.resize(image, (rect.width, rect.height))\n    alpha_channel = (\n        image[:, :, 3]\n        if image.shape[2] == 4\n        else np.ones((rect.height, rect.width), dtype=image.dtype) * 255\n    )\n    alpha_scaled = cv2.convertScaleAbs(alpha_channel * opacity)\n\n    # Perform blending\n    scene_roi = scene[rect.y : rect.y + rect.height, rect.x : rect.x + rect.width]\n    alpha_float = alpha_scaled.astype(np.float32) / 255.0\n    blended_roi = cv2.convertScaleAbs(\n        (1 - alpha_float[..., np.newaxis]) * scene_roi\n        + alpha_float[..., np.newaxis] * image[:, :, :3]\n    )\n\n    # Update the scene\n    scene[rect.y : rect.y + rect.height, rect.x : rect.x + rect.width] = blended_roi\n\n    return scene\n</code></pre> calculate_optimal_text_scale <p>Calculate font scale based on the resolution of an image.</p> <p>Parameters:</p> Name Type Description Default <code>resolution_wh</code> <code>Tuple[int, int]</code> <p>A tuple representing the width and height of the image.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The calculated font scale factor.</p> Source code in <code>supervision/draw/utils.py</code> <pre><code>def calculate_optimal_text_scale(resolution_wh: Tuple[int, int]) -&gt; float:\n    \"\"\"\n    Calculate font scale based on the resolution of an image.\n\n    Parameters:\n        resolution_wh (Tuple[int, int]): A tuple representing the width and height\n            of the image.\n\n    Returns:\n         float: The calculated font scale factor.\n    \"\"\"\n    return min(resolution_wh) * 1e-3\n</code></pre> calculate_optimal_line_thickness <p>Calculate line thickness based on the resolution of an image.</p> <p>Parameters:</p> Name Type Description Default <code>resolution_wh</code> <code>Tuple[int, int]</code> <p>A tuple representing the width and height of the image.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The calculated line thickness in pixels.</p> Source code in <code>supervision/draw/utils.py</code> <pre><code>def calculate_optimal_line_thickness(resolution_wh: Tuple[int, int]) -&gt; int:\n    \"\"\"\n    Calculate line thickness based on the resolution of an image.\n\n    Parameters:\n        resolution_wh (Tuple[int, int]): A tuple representing the width and height\n            of the image.\n\n    Returns:\n        int: The calculated line thickness in pixels.\n    \"\"\"\n    if min(resolution_wh) &lt; 1080:\n        return 2\n    return 4\n</code></pre> Color <p>Represents a color in RGB format.</p> <p>This class provides methods to work with colors, including creating colors from hex codes, converting colors to hex strings, RGB tuples, and BGR tuples.</p> <p>Attributes:</p> Name Type Description <code>r</code> <code>int</code> <p>Red channel value (0-255).</p> <code>g</code> <code>int</code> <p>Green channel value (0-255).</p> <code>b</code> <code>int</code> <p>Blue channel value (0-255).</p> Example <pre><code>import supervision as sv\n\nsv.Color.WHITE\n# Color(r=255, g=255, b=255)\n</code></pre> Constant Hex Code RGB <code>WHITE</code> <code>#FFFFFF</code> <code>(255, 255, 255)</code> <code>BLACK</code> <code>#000000</code> <code>(0, 0, 0)</code> <code>RED</code> <code>#FF0000</code> <code>(255, 0, 0)</code> <code>GREEN</code> <code>#00FF00</code> <code>(0, 255, 0)</code> <code>BLUE</code> <code>#0000FF</code> <code>(0, 0, 255)</code> <code>YELLOW</code> <code>#FFFF00</code> <code>(255, 255, 0)</code> <code>ROBOFLOW</code> <code>#A351FB</code> <code>(163, 81, 251)</code> Source code in <code>supervision/draw/color.py</code> <pre><code>@dataclass\nclass Color:\n    \"\"\"\n    Represents a color in RGB format.\n\n    This class provides methods to work with colors, including creating colors from hex\n    codes, converting colors to hex strings, RGB tuples, and BGR tuples.\n\n    Attributes:\n        r (int): Red channel value (0-255).\n        g (int): Green channel value (0-255).\n        b (int): Blue channel value (0-255).\n\n    Example:\n        ```python\n        import supervision as sv\n\n        sv.Color.WHITE\n        # Color(r=255, g=255, b=255)\n        ```\n\n    | Constant   | Hex Code   | RGB              |\n    |------------|------------|------------------|\n    | `WHITE`    | `#FFFFFF`  | `(255, 255, 255)`|\n    | `BLACK`    | `#000000`  | `(0, 0, 0)`      |\n    | `RED`      | `#FF0000`  | `(255, 0, 0)`    |\n    | `GREEN`    | `#00FF00`  | `(0, 255, 0)`    |\n    | `BLUE`     | `#0000FF`  | `(0, 0, 255)`    |\n    | `YELLOW`   | `#FFFF00`  | `(255, 255, 0)`  |\n    | `ROBOFLOW` | `#A351FB`  | `(163, 81, 251)` |\n    \"\"\"\n\n    r: int\n    g: int\n    b: int\n\n    @classmethod\n    def from_hex(cls, color_hex: str) -&gt; Color:\n        \"\"\"\n        Create a Color instance from a hex string.\n\n        Args:\n            color_hex (str): The hex string representing the color. This string can\n                start with '#' followed by either 3 or 6 hexadecimal characters. In\n                case of 3 characters, each character is repeated to form the full\n                6-character hex code.\n\n        Returns:\n            Color: An instance representing the color.\n\n        Example:\n            ```python\n            import supervision as sv\n\n            sv.Color.from_hex('#ff00ff')\n            # Color(r=255, g=0, b=255)\n\n            sv.Color.from_hex('#f0f')\n            # Color(r=255, g=0, b=255)\n            ```\n        \"\"\"\n        _validate_color_hex(color_hex)\n        color_hex = color_hex.lstrip(\"#\")\n        if len(color_hex) == 3:\n            color_hex = \"\".join(c * 2 for c in color_hex)\n        r, g, b = (int(color_hex[i : i + 2], 16) for i in range(0, 6, 2))\n        return cls(r, g, b)\n\n    @classmethod\n    def from_rgb_tuple(cls, color_tuple: Tuple[int, int, int]) -&gt; Color:\n        \"\"\"\n        Create a Color instance from an RGB tuple.\n\n        Args:\n            color_tuple (Tuple[int, int, int]): A tuple representing the color in RGB\n                format, where each element is an integer in the range 0-255.\n\n        Returns:\n            Color: An instance representing the color.\n\n        Example:\n            ```python\n            import supervision as sv\n\n            sv.Color.from_rgb_tuple((255, 255, 0))\n            # Color(r=255, g=255, b=0)\n            ```\n        \"\"\"\n        r, g, b = color_tuple\n        return cls(r=r, g=g, b=b)\n\n    @classmethod\n    def from_bgr_tuple(cls, color_tuple: Tuple[int, int, int]) -&gt; Color:\n        \"\"\"\n        Create a Color instance from a BGR tuple.\n\n        Args:\n            color_tuple (Tuple[int, int, int]): A tuple representing the color in BGR\n                format, where each element is an integer in the range 0-255.\n\n        Returns:\n            Color: An instance representing the color.\n\n        Example:\n            ```python\n            import supervision as sv\n\n            sv.Color.from_bgr_tuple((0, 255, 255))\n            # Color(r=255, g=255, b=0)\n            ```\n        \"\"\"\n        b, g, r = color_tuple\n        return cls(r=r, g=g, b=b)\n\n    def as_hex(self) -&gt; str:\n        \"\"\"\n        Converts the Color instance to a hex string.\n\n        Returns:\n            str: The hexadecimal color string.\n\n        Example:\n            ```python\n            import supervision as sv\n\n            sv.Color(r=255, g=255, b=0).as_hex()\n            # '#ffff00'\n            ```\n        \"\"\"\n        return f\"#{self.r:02x}{self.g:02x}{self.b:02x}\"\n\n    def as_rgb(self) -&gt; Tuple[int, int, int]:\n        \"\"\"\n        Returns the color as an RGB tuple.\n\n        Returns:\n            Tuple[int, int, int]: RGB tuple.\n\n        Example:\n            ```python\n            import supervision as sv\n\n            sv.Color(r=255, g=255, b=0).as_rgb()\n            # (255, 255, 0)\n            ```\n        \"\"\"\n        return self.r, self.g, self.b\n\n    def as_bgr(self) -&gt; Tuple[int, int, int]:\n        \"\"\"\n        Returns the color as a BGR tuple.\n\n        Returns:\n            Tuple[int, int, int]: BGR tuple.\n\n        Example:\n            ```python\n            import supervision as sv\n\n            sv.Color(r=255, g=255, b=0).as_bgr()\n            # (0, 255, 255)\n            ```\n        \"\"\"\n        return self.b, self.g, self.r\n\n    @classproperty\n    def WHITE(cls) -&gt; Color:\n        return Color.from_hex(\"#FFFFFF\")\n\n    @classproperty\n    def BLACK(cls) -&gt; Color:\n        return Color.from_hex(\"#000000\")\n\n    @classproperty\n    def RED(cls) -&gt; Color:\n        return Color.from_hex(\"#FF0000\")\n\n    @classproperty\n    def GREEN(cls) -&gt; Color:\n        return Color.from_hex(\"#00FF00\")\n\n    @classproperty\n    def BLUE(cls) -&gt; Color:\n        return Color.from_hex(\"#0000FF\")\n\n    @classproperty\n    def YELLOW(cls) -&gt; Color:\n        return Color.from_hex(\"#FFFF00\")\n\n    @classproperty\n    def ROBOFLOW(cls) -&gt; Color:\n        return Color.from_hex(\"#A351FB\")\n</code></pre> ColorPalette Source code in <code>supervision/draw/color.py</code> <pre><code>@dataclass\nclass ColorPalette:\n    colors: List[Color]\n\n    @classproperty\n    def DEFAULT(cls) -&gt; ColorPalette:\n        \"\"\"\n        Returns a default color palette.\n\n        Returns:\n            ColorPalette: A ColorPalette instance with default colors.\n\n        Example:\n            ```python\n            import supervision as sv\n\n            sv.ColorPalette.DEFAULT\n            # ColorPalette(colors=[Color(r=255, g=64, b=64), Color(r=255, g=161, b=160), ...])\n            ```\n\n        ![default-color-palette](https://media.roboflow.com/\n        supervision-annotator-examples/default-color-palette.png)\n        \"\"\"  # noqa: E501 // docs\n        return ColorPalette.from_hex(color_hex_list=DEFAULT_COLOR_PALETTE)\n\n    @classproperty\n    def ROBOFLOW(cls) -&gt; ColorPalette:\n        \"\"\"\n        Returns a Roboflow color palette.\n\n        Returns:\n            ColorPalette: A ColorPalette instance with Roboflow colors.\n\n        Example:\n            ```python\n            import supervision as sv\n\n            sv.ColorPalette.ROBOFLOW\n            # ColorPalette(colors=[Color(r=194, g=141, b=252), Color(r=163, g=81, b=251), ...])\n            ```\n\n        ![roboflow-color-palette](https://media.roboflow.com/\n        supervision-annotator-examples/roboflow-color-palette.png)\n        \"\"\"  # noqa: E501 // docs\n        return ColorPalette.from_hex(color_hex_list=ROBOFLOW_COLOR_PALETTE)\n\n    @classproperty\n    def LEGACY(cls) -&gt; ColorPalette:\n        return ColorPalette.from_hex(color_hex_list=LEGACY_COLOR_PALETTE)\n\n    @classmethod\n    def from_hex(cls, color_hex_list: List[str]) -&gt; ColorPalette:\n        \"\"\"\n        Create a ColorPalette instance from a list of hex strings.\n\n        Args:\n            color_hex_list (List[str]): List of color hex strings.\n\n        Returns:\n            ColorPalette: A ColorPalette instance.\n\n        Example:\n            ```python\n            import supervision as sv\n\n            sv.ColorPalette.from_hex(['#ff0000', '#00ff00', '#0000ff'])\n            # ColorPalette(colors=[Color(r=255, g=0, b=0), Color(r=0, g=255, b=0), ...])\n            ```\n        \"\"\"\n        colors = [Color.from_hex(color_hex) for color_hex in color_hex_list]\n        return cls(colors)\n\n    @classmethod\n    def from_matplotlib(cls, palette_name: str, color_count: int) -&gt; ColorPalette:\n        \"\"\"\n        Create a ColorPalette instance from a Matplotlib color palette.\n\n        Args:\n            palette_name (str): Name of the Matplotlib palette.\n            color_count (int): Number of colors to sample from the palette.\n\n        Returns:\n            ColorPalette: A ColorPalette instance.\n\n        Example:\n            ```python\n            import supervision as sv\n\n            sv.ColorPalette.from_matplotlib('viridis', 5)\n            # ColorPalette(colors=[Color(r=68, g=1, b=84), Color(r=59, g=82, b=139), ...])\n            ```\n\n        ![visualized_color_palette](https://media.roboflow.com/\n        supervision-annotator-examples/visualized_color_palette.png)\n        \"\"\"  # noqa: E501 // docs\n        mpl_palette = plt.get_cmap(palette_name, color_count)\n\n        if hasattr(mpl_palette, \"colors\"):\n            colors = mpl_palette.colors\n        else:\n            colors = [mpl_palette(i / (color_count - 1)) for i in range(color_count)]\n\n        return cls(\n            [Color(int(r * 255), int(g * 255), int(b * 255)) for r, g, b, _ in colors]\n        )\n\n    def by_idx(self, idx: int) -&gt; Color:\n        \"\"\"\n        Return the color at a given index in the palette.\n\n        Args:\n            idx (int): Index of the color in the palette.\n\n        Returns:\n            Color: Color at the given index.\n\n        Example:\n            ```python\n            import supervision as sv\n\n            color_palette = sv.ColorPalette.from_hex(['#ff0000', '#00ff00', '#0000ff'])\n            color_palette.by_idx(1)\n            # Color(r=0, g=255, b=0)\n            ```\n        \"\"\"\n        if idx &lt; 0:\n            raise ValueError(\"idx argument should not be negative\")\n        idx = idx % len(self.colors)\n        return self.colors[idx]\n</code></pre>"},{"location":"utils/draw/#supervision.draw.color.Color-functions","title":"Functions","text":""},{"location":"utils/draw/#supervision.draw.color.Color.as_bgr","title":"<code>as_bgr()</code>","text":"<p>Returns the color as a BGR tuple.</p> <p>Returns:</p> Type Description <code>Tuple[int, int, int]</code> <p>Tuple[int, int, int]: BGR tuple.</p> Example <pre><code>import supervision as sv\n\nsv.Color(r=255, g=255, b=0).as_bgr()\n# (0, 255, 255)\n</code></pre> Source code in <code>supervision/draw/color.py</code> <pre><code>def as_bgr(self) -&gt; Tuple[int, int, int]:\n    \"\"\"\n    Returns the color as a BGR tuple.\n\n    Returns:\n        Tuple[int, int, int]: BGR tuple.\n\n    Example:\n        ```python\n        import supervision as sv\n\n        sv.Color(r=255, g=255, b=0).as_bgr()\n        # (0, 255, 255)\n        ```\n    \"\"\"\n    return self.b, self.g, self.r\n</code></pre>"},{"location":"utils/draw/#supervision.draw.color.Color.as_hex","title":"<code>as_hex()</code>","text":"<p>Converts the Color instance to a hex string.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The hexadecimal color string.</p> Example <pre><code>import supervision as sv\n\nsv.Color(r=255, g=255, b=0).as_hex()\n# '#ffff00'\n</code></pre> Source code in <code>supervision/draw/color.py</code> <pre><code>def as_hex(self) -&gt; str:\n    \"\"\"\n    Converts the Color instance to a hex string.\n\n    Returns:\n        str: The hexadecimal color string.\n\n    Example:\n        ```python\n        import supervision as sv\n\n        sv.Color(r=255, g=255, b=0).as_hex()\n        # '#ffff00'\n        ```\n    \"\"\"\n    return f\"#{self.r:02x}{self.g:02x}{self.b:02x}\"\n</code></pre>"},{"location":"utils/draw/#supervision.draw.color.Color.as_rgb","title":"<code>as_rgb()</code>","text":"<p>Returns the color as an RGB tuple.</p> <p>Returns:</p> Type Description <code>Tuple[int, int, int]</code> <p>Tuple[int, int, int]: RGB tuple.</p> Example <pre><code>import supervision as sv\n\nsv.Color(r=255, g=255, b=0).as_rgb()\n# (255, 255, 0)\n</code></pre> Source code in <code>supervision/draw/color.py</code> <pre><code>def as_rgb(self) -&gt; Tuple[int, int, int]:\n    \"\"\"\n    Returns the color as an RGB tuple.\n\n    Returns:\n        Tuple[int, int, int]: RGB tuple.\n\n    Example:\n        ```python\n        import supervision as sv\n\n        sv.Color(r=255, g=255, b=0).as_rgb()\n        # (255, 255, 0)\n        ```\n    \"\"\"\n    return self.r, self.g, self.b\n</code></pre>"},{"location":"utils/draw/#supervision.draw.color.Color.from_bgr_tuple","title":"<code>from_bgr_tuple(color_tuple)</code>  <code>classmethod</code>","text":"<p>Create a Color instance from a BGR tuple.</p> <p>Parameters:</p> Name Type Description Default <code>color_tuple</code> <code>Tuple[int, int, int]</code> <p>A tuple representing the color in BGR format, where each element is an integer in the range 0-255.</p> required <p>Returns:</p> Name Type Description <code>Color</code> <code>Color</code> <p>An instance representing the color.</p> Example <pre><code>import supervision as sv\n\nsv.Color.from_bgr_tuple((0, 255, 255))\n# Color(r=255, g=255, b=0)\n</code></pre> Source code in <code>supervision/draw/color.py</code> <pre><code>@classmethod\ndef from_bgr_tuple(cls, color_tuple: Tuple[int, int, int]) -&gt; Color:\n    \"\"\"\n    Create a Color instance from a BGR tuple.\n\n    Args:\n        color_tuple (Tuple[int, int, int]): A tuple representing the color in BGR\n            format, where each element is an integer in the range 0-255.\n\n    Returns:\n        Color: An instance representing the color.\n\n    Example:\n        ```python\n        import supervision as sv\n\n        sv.Color.from_bgr_tuple((0, 255, 255))\n        # Color(r=255, g=255, b=0)\n        ```\n    \"\"\"\n    b, g, r = color_tuple\n    return cls(r=r, g=g, b=b)\n</code></pre>"},{"location":"utils/draw/#supervision.draw.color.Color.from_hex","title":"<code>from_hex(color_hex)</code>  <code>classmethod</code>","text":"<p>Create a Color instance from a hex string.</p> <p>Parameters:</p> Name Type Description Default <code>color_hex</code> <code>str</code> <p>The hex string representing the color. This string can start with '#' followed by either 3 or 6 hexadecimal characters. In case of 3 characters, each character is repeated to form the full 6-character hex code.</p> required <p>Returns:</p> Name Type Description <code>Color</code> <code>Color</code> <p>An instance representing the color.</p> Example <pre><code>import supervision as sv\n\nsv.Color.from_hex('#ff00ff')\n# Color(r=255, g=0, b=255)\n\nsv.Color.from_hex('#f0f')\n# Color(r=255, g=0, b=255)\n</code></pre> Source code in <code>supervision/draw/color.py</code> <pre><code>@classmethod\ndef from_hex(cls, color_hex: str) -&gt; Color:\n    \"\"\"\n    Create a Color instance from a hex string.\n\n    Args:\n        color_hex (str): The hex string representing the color. This string can\n            start with '#' followed by either 3 or 6 hexadecimal characters. In\n            case of 3 characters, each character is repeated to form the full\n            6-character hex code.\n\n    Returns:\n        Color: An instance representing the color.\n\n    Example:\n        ```python\n        import supervision as sv\n\n        sv.Color.from_hex('#ff00ff')\n        # Color(r=255, g=0, b=255)\n\n        sv.Color.from_hex('#f0f')\n        # Color(r=255, g=0, b=255)\n        ```\n    \"\"\"\n    _validate_color_hex(color_hex)\n    color_hex = color_hex.lstrip(\"#\")\n    if len(color_hex) == 3:\n        color_hex = \"\".join(c * 2 for c in color_hex)\n    r, g, b = (int(color_hex[i : i + 2], 16) for i in range(0, 6, 2))\n    return cls(r, g, b)\n</code></pre>"},{"location":"utils/draw/#supervision.draw.color.Color.from_rgb_tuple","title":"<code>from_rgb_tuple(color_tuple)</code>  <code>classmethod</code>","text":"<p>Create a Color instance from an RGB tuple.</p> <p>Parameters:</p> Name Type Description Default <code>color_tuple</code> <code>Tuple[int, int, int]</code> <p>A tuple representing the color in RGB format, where each element is an integer in the range 0-255.</p> required <p>Returns:</p> Name Type Description <code>Color</code> <code>Color</code> <p>An instance representing the color.</p> Example <pre><code>import supervision as sv\n\nsv.Color.from_rgb_tuple((255, 255, 0))\n# Color(r=255, g=255, b=0)\n</code></pre> Source code in <code>supervision/draw/color.py</code> <pre><code>@classmethod\ndef from_rgb_tuple(cls, color_tuple: Tuple[int, int, int]) -&gt; Color:\n    \"\"\"\n    Create a Color instance from an RGB tuple.\n\n    Args:\n        color_tuple (Tuple[int, int, int]): A tuple representing the color in RGB\n            format, where each element is an integer in the range 0-255.\n\n    Returns:\n        Color: An instance representing the color.\n\n    Example:\n        ```python\n        import supervision as sv\n\n        sv.Color.from_rgb_tuple((255, 255, 0))\n        # Color(r=255, g=255, b=0)\n        ```\n    \"\"\"\n    r, g, b = color_tuple\n    return cls(r=r, g=g, b=b)\n</code></pre>"},{"location":"utils/draw/#supervision.draw.color.ColorPalette-functions","title":"Functions","text":""},{"location":"utils/draw/#supervision.draw.color.ColorPalette.DEFAULT","title":"<code>DEFAULT()</code>","text":"<p>Returns a default color palette.</p> <p>Returns:</p> Name Type Description <code>ColorPalette</code> <code>ColorPalette</code> <p>A ColorPalette instance with default colors.</p> Example <pre><code>import supervision as sv\n\nsv.ColorPalette.DEFAULT\n# ColorPalette(colors=[Color(r=255, g=64, b=64), Color(r=255, g=161, b=160), ...])\n</code></pre> <p></p> Source code in <code>supervision/draw/color.py</code> <pre><code>@classproperty\ndef DEFAULT(cls) -&gt; ColorPalette:\n    \"\"\"\n    Returns a default color palette.\n\n    Returns:\n        ColorPalette: A ColorPalette instance with default colors.\n\n    Example:\n        ```python\n        import supervision as sv\n\n        sv.ColorPalette.DEFAULT\n        # ColorPalette(colors=[Color(r=255, g=64, b=64), Color(r=255, g=161, b=160), ...])\n        ```\n\n    ![default-color-palette](https://media.roboflow.com/\n    supervision-annotator-examples/default-color-palette.png)\n    \"\"\"  # noqa: E501 // docs\n    return ColorPalette.from_hex(color_hex_list=DEFAULT_COLOR_PALETTE)\n</code></pre>"},{"location":"utils/draw/#supervision.draw.color.ColorPalette.ROBOFLOW","title":"<code>ROBOFLOW()</code>","text":"<p>Returns a Roboflow color palette.</p> <p>Returns:</p> Name Type Description <code>ColorPalette</code> <code>ColorPalette</code> <p>A ColorPalette instance with Roboflow colors.</p> Example <pre><code>import supervision as sv\n\nsv.ColorPalette.ROBOFLOW\n# ColorPalette(colors=[Color(r=194, g=141, b=252), Color(r=163, g=81, b=251), ...])\n</code></pre> <p></p> Source code in <code>supervision/draw/color.py</code> <pre><code>@classproperty\ndef ROBOFLOW(cls) -&gt; ColorPalette:\n    \"\"\"\n    Returns a Roboflow color palette.\n\n    Returns:\n        ColorPalette: A ColorPalette instance with Roboflow colors.\n\n    Example:\n        ```python\n        import supervision as sv\n\n        sv.ColorPalette.ROBOFLOW\n        # ColorPalette(colors=[Color(r=194, g=141, b=252), Color(r=163, g=81, b=251), ...])\n        ```\n\n    ![roboflow-color-palette](https://media.roboflow.com/\n    supervision-annotator-examples/roboflow-color-palette.png)\n    \"\"\"  # noqa: E501 // docs\n    return ColorPalette.from_hex(color_hex_list=ROBOFLOW_COLOR_PALETTE)\n</code></pre>"},{"location":"utils/draw/#supervision.draw.color.ColorPalette.by_idx","title":"<code>by_idx(idx)</code>","text":"<p>Return the color at a given index in the palette.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the color in the palette.</p> required <p>Returns:</p> Name Type Description <code>Color</code> <code>Color</code> <p>Color at the given index.</p> Example <pre><code>import supervision as sv\n\ncolor_palette = sv.ColorPalette.from_hex(['#ff0000', '#00ff00', '#0000ff'])\ncolor_palette.by_idx(1)\n# Color(r=0, g=255, b=0)\n</code></pre> Source code in <code>supervision/draw/color.py</code> <pre><code>def by_idx(self, idx: int) -&gt; Color:\n    \"\"\"\n    Return the color at a given index in the palette.\n\n    Args:\n        idx (int): Index of the color in the palette.\n\n    Returns:\n        Color: Color at the given index.\n\n    Example:\n        ```python\n        import supervision as sv\n\n        color_palette = sv.ColorPalette.from_hex(['#ff0000', '#00ff00', '#0000ff'])\n        color_palette.by_idx(1)\n        # Color(r=0, g=255, b=0)\n        ```\n    \"\"\"\n    if idx &lt; 0:\n        raise ValueError(\"idx argument should not be negative\")\n    idx = idx % len(self.colors)\n    return self.colors[idx]\n</code></pre>"},{"location":"utils/draw/#supervision.draw.color.ColorPalette.from_hex","title":"<code>from_hex(color_hex_list)</code>  <code>classmethod</code>","text":"<p>Create a ColorPalette instance from a list of hex strings.</p> <p>Parameters:</p> Name Type Description Default <code>color_hex_list</code> <code>List[str]</code> <p>List of color hex strings.</p> required <p>Returns:</p> Name Type Description <code>ColorPalette</code> <code>ColorPalette</code> <p>A ColorPalette instance.</p> Example <pre><code>import supervision as sv\n\nsv.ColorPalette.from_hex(['#ff0000', '#00ff00', '#0000ff'])\n# ColorPalette(colors=[Color(r=255, g=0, b=0), Color(r=0, g=255, b=0), ...])\n</code></pre> Source code in <code>supervision/draw/color.py</code> <pre><code>@classmethod\ndef from_hex(cls, color_hex_list: List[str]) -&gt; ColorPalette:\n    \"\"\"\n    Create a ColorPalette instance from a list of hex strings.\n\n    Args:\n        color_hex_list (List[str]): List of color hex strings.\n\n    Returns:\n        ColorPalette: A ColorPalette instance.\n\n    Example:\n        ```python\n        import supervision as sv\n\n        sv.ColorPalette.from_hex(['#ff0000', '#00ff00', '#0000ff'])\n        # ColorPalette(colors=[Color(r=255, g=0, b=0), Color(r=0, g=255, b=0), ...])\n        ```\n    \"\"\"\n    colors = [Color.from_hex(color_hex) for color_hex in color_hex_list]\n    return cls(colors)\n</code></pre>"},{"location":"utils/draw/#supervision.draw.color.ColorPalette.from_matplotlib","title":"<code>from_matplotlib(palette_name, color_count)</code>  <code>classmethod</code>","text":"<p>Create a ColorPalette instance from a Matplotlib color palette.</p> <p>Parameters:</p> Name Type Description Default <code>palette_name</code> <code>str</code> <p>Name of the Matplotlib palette.</p> required <code>color_count</code> <code>int</code> <p>Number of colors to sample from the palette.</p> required <p>Returns:</p> Name Type Description <code>ColorPalette</code> <code>ColorPalette</code> <p>A ColorPalette instance.</p> Example <pre><code>import supervision as sv\n\nsv.ColorPalette.from_matplotlib('viridis', 5)\n# ColorPalette(colors=[Color(r=68, g=1, b=84), Color(r=59, g=82, b=139), ...])\n</code></pre> <p></p> Source code in <code>supervision/draw/color.py</code> <pre><code>@classmethod\ndef from_matplotlib(cls, palette_name: str, color_count: int) -&gt; ColorPalette:\n    \"\"\"\n    Create a ColorPalette instance from a Matplotlib color palette.\n\n    Args:\n        palette_name (str): Name of the Matplotlib palette.\n        color_count (int): Number of colors to sample from the palette.\n\n    Returns:\n        ColorPalette: A ColorPalette instance.\n\n    Example:\n        ```python\n        import supervision as sv\n\n        sv.ColorPalette.from_matplotlib('viridis', 5)\n        # ColorPalette(colors=[Color(r=68, g=1, b=84), Color(r=59, g=82, b=139), ...])\n        ```\n\n    ![visualized_color_palette](https://media.roboflow.com/\n    supervision-annotator-examples/visualized_color_palette.png)\n    \"\"\"  # noqa: E501 // docs\n    mpl_palette = plt.get_cmap(palette_name, color_count)\n\n    if hasattr(mpl_palette, \"colors\"):\n        colors = mpl_palette.colors\n    else:\n        colors = [mpl_palette(i / (color_count - 1)) for i in range(color_count)]\n\n    return cls(\n        [Color(int(r * 255), int(g * 255), int(b * 255)) for r, g, b, _ in colors]\n    )\n</code></pre>"},{"location":"utils/file/","title":"File Utils","text":"list_files_with_extensions <p>List files in a directory with specified extensions or     all files if no extensions are provided.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Union[str, Path]</code> <p>The directory path as a string or Path object.</p> required <code>extensions</code> <code>Optional[List[str]]</code> <p>A list of file extensions to filter. Default is None, which lists all files.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Path]</code> <p>A list of Path objects for the matching files.</p> <p>Examples:</p> <pre><code>import supervision as sv\n\n# List all files in the directory\nfiles = sv.list_files_with_extensions(directory='my_directory')\n\n# List only files with '.txt' and '.md' extensions\nfiles = sv.list_files_with_extensions(\n    directory='my_directory', extensions=['txt', 'md'])\n</code></pre> Source code in <code>supervision/utils/file.py</code> <pre><code>def list_files_with_extensions(\n    directory: Union[str, Path], extensions: Optional[List[str]] = None\n) -&gt; List[Path]:\n    \"\"\"\n    List files in a directory with specified extensions or\n        all files if no extensions are provided.\n\n    Args:\n        directory (Union[str, Path]): The directory path as a string or Path object.\n        extensions (Optional[List[str]]): A list of file extensions to filter.\n            Default is None, which lists all files.\n\n    Returns:\n        (List[Path]): A list of Path objects for the matching files.\n\n    Examples:\n        ```python\n        import supervision as sv\n\n        # List all files in the directory\n        files = sv.list_files_with_extensions(directory='my_directory')\n\n        # List only files with '.txt' and '.md' extensions\n        files = sv.list_files_with_extensions(\n            directory='my_directory', extensions=['txt', 'md'])\n        ```\n    \"\"\"\n\n    directory = Path(directory)\n    files_with_extensions = []\n\n    if extensions is not None:\n        for ext in extensions:\n            files_with_extensions.extend(directory.glob(f\"*.{ext}\"))\n    else:\n        files_with_extensions.extend(directory.glob(\"*\"))\n\n    return files_with_extensions\n</code></pre>"},{"location":"utils/geometry/","title":"Geometry","text":"get_polygon_center <p>Calculate the center of a polygon. The center is calculated as the center of the solid figure formed by the points of the polygon</p> <p>Parameters:</p> Name Type Description Default <code>polygon</code> <code>ndarray</code> <p>A 2-dimensional numpy ndarray representing the vertices of the polygon.</p> required <p>Returns:</p> Name Type Description <code>Point</code> <code>Point</code> <p>The center of the polygon, represented as a Point object with x and y attributes.</p> <p>Examples:</p> <pre><code>import numpy as np\nimport supervision as sv\n\npolygon = np.array([[0, 0], [0, 2], [2, 2], [2, 0]])\nsv.get_polygon_center(polygon=polygon)\n# Point(x=1, y=1)\n</code></pre> Source code in <code>supervision/geometry/utils.py</code> <pre><code>def get_polygon_center(polygon: np.ndarray) -&gt; Point:\n    \"\"\"\n    Calculate the center of a polygon. The center is calculated as the center\n    of the solid figure formed by the points of the polygon\n\n    Parameters:\n        polygon (np.ndarray): A 2-dimensional numpy ndarray representing the\n            vertices of the polygon.\n\n    Returns:\n        Point: The center of the polygon, represented as a\n            Point object with x and y attributes.\n\n    Examples:\n        ```python\n        import numpy as np\n        import supervision as sv\n\n        polygon = np.array([[0, 0], [0, 2], [2, 2], [2, 0]])\n        sv.get_polygon_center(polygon=polygon)\n        # Point(x=1, y=1)\n        ```\n    \"\"\"\n\n    # This is one of the 3 candidate algorithms considered for centroid calculation.\n    # For a more detailed discussion, see PR #1084 and commit eb33176\n\n    shift_polygon = np.roll(polygon, -1, axis=0)\n    signed_areas = np.cross(polygon, shift_polygon) / 2\n    if signed_areas.sum() == 0:\n        center = np.mean(polygon, axis=0).round()\n        return Point(x=center[0], y=center[1])\n    centroids = (polygon + shift_polygon) / 3.0\n    center = np.average(centroids, axis=0, weights=signed_areas).round()\n\n    return Point(x=center[0], y=center[1])\n</code></pre> Position <p>               Bases: <code>Enum</code></p> <p>Enum representing the position of an anchor point.</p> Source code in <code>supervision/geometry/core.py</code> <pre><code>class Position(Enum):\n    \"\"\"\n    Enum representing the position of an anchor point.\n    \"\"\"\n\n    CENTER = \"CENTER\"\n    CENTER_LEFT = \"CENTER_LEFT\"\n    CENTER_RIGHT = \"CENTER_RIGHT\"\n    TOP_CENTER = \"TOP_CENTER\"\n    TOP_LEFT = \"TOP_LEFT\"\n    TOP_RIGHT = \"TOP_RIGHT\"\n    BOTTOM_LEFT = \"BOTTOM_LEFT\"\n    BOTTOM_CENTER = \"BOTTOM_CENTER\"\n    BOTTOM_RIGHT = \"BOTTOM_RIGHT\"\n    CENTER_OF_MASS = \"CENTER_OF_MASS\"\n\n    @classmethod\n    def list(cls):\n        return list(map(lambda c: c.value, cls))\n</code></pre>"},{"location":"utils/image/","title":"Image Utils","text":"crop_image <p>Crops the given image based on the given bounding box.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ImageType</code> <p>The image to be cropped. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>xyxy</code> <code>Union[ndarray, List[int], Tuple[int, int, int, int]]</code> <p>A bounding box coordinates in the format <code>(x_min, y_min, x_max, y_max)</code>, accepted as either a <code>numpy.ndarray</code>, a <code>list</code>, or a <code>tuple</code>.</p> required <p>Returns:</p> Type Description <code>ImageType</code> <p>The cropped image. The type is determined by the input type and may be either a <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> OpenCVPillow <pre><code>import cv2\nimport supervision as sv\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nimage.shape\n# (1080, 1920, 3)\n\nxyxy = [200, 400, 600, 800]\ncropped_image = sv.crop_image(image=image, xyxy=xyxy)\ncropped_image.shape\n# (400, 400, 3)\n</code></pre> <pre><code>from PIL import Image\nimport supervision as sv\n\nimage = Image.open(&lt;SOURCE_IMAGE_PATH&gt;)\nimage.size\n# (1920, 1080)\n\nxyxy = [200, 400, 600, 800]\ncropped_image = sv.crop_image(image=image, xyxy=xyxy)\ncropped_image.size\n# (400, 400)\n</code></pre> <p></p> Source code in <code>supervision/utils/image.py</code> <pre><code>@ensure_cv2_image_for_processing\ndef crop_image(\n    image: ImageType,\n    xyxy: Union[npt.NDArray[int], List[int], Tuple[int, int, int, int]],\n) -&gt; ImageType:\n    \"\"\"\n    Crops the given image based on the given bounding box.\n\n    Args:\n        image (ImageType): The image to be cropped. `ImageType` is a flexible type,\n            accepting either `numpy.ndarray` or `PIL.Image.Image`.\n        xyxy (Union[np.ndarray, List[int], Tuple[int, int, int, int]]): A bounding box\n            coordinates in the format `(x_min, y_min, x_max, y_max)`, accepted as either\n            a `numpy.ndarray`, a `list`, or a `tuple`.\n\n    Returns:\n        (ImageType): The cropped image. The type is determined by the input type and\n            may be either a `numpy.ndarray` or `PIL.Image.Image`.\n\n    === \"OpenCV\"\n\n        ```python\n        import cv2\n        import supervision as sv\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        image.shape\n        # (1080, 1920, 3)\n\n        xyxy = [200, 400, 600, 800]\n        cropped_image = sv.crop_image(image=image, xyxy=xyxy)\n        cropped_image.shape\n        # (400, 400, 3)\n        ```\n\n    === \"Pillow\"\n\n        ```python\n        from PIL import Image\n        import supervision as sv\n\n        image = Image.open(&lt;SOURCE_IMAGE_PATH&gt;)\n        image.size\n        # (1920, 1080)\n\n        xyxy = [200, 400, 600, 800]\n        cropped_image = sv.crop_image(image=image, xyxy=xyxy)\n        cropped_image.size\n        # (400, 400)\n        ```\n\n    ![crop_image](https://media.roboflow.com/supervision-docs/crop-image.png){ align=center width=\"800\" }\n    \"\"\"  # noqa E501 // docs\n\n    if isinstance(xyxy, (list, tuple)):\n        xyxy = np.array(xyxy)\n    xyxy = np.round(xyxy).astype(int)\n    x_min, y_min, x_max, y_max = xyxy.flatten()\n    return image[y_min:y_max, x_min:x_max]\n</code></pre> scale_image <p>Scales the given image based on the given scale factor.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ImageType</code> <p>The image to be scaled. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>scale_factor</code> <code>float</code> <p>The factor by which the image will be scaled. Scale factor &gt; <code>1.0</code> zooms in, &lt; <code>1.0</code> zooms out.</p> required <p>Returns:</p> Type Description <code>ImageType</code> <p>The scaled image. The type is determined by the input type and may be either a <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the scale factor is non-positive.</p> OpenCVPillow <pre><code>import cv2\nimport supervision as sv\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nimage.shape\n# (1080, 1920, 3)\n\nscaled_image = sv.scale_image(image=image, scale_factor=0.5)\nscaled_image.shape\n# (540, 960, 3)\n</code></pre> <pre><code>from PIL import Image\nimport supervision as sv\n\nimage = Image.open(&lt;SOURCE_IMAGE_PATH&gt;)\nimage.size\n# (1920, 1080)\n\nscaled_image = sv.scale_image(image=image, scale_factor=0.5)\nscaled_image.size\n# (960, 540)\n</code></pre> Source code in <code>supervision/utils/image.py</code> <pre><code>@ensure_cv2_image_for_processing\ndef scale_image(image: ImageType, scale_factor: float) -&gt; ImageType:\n    \"\"\"\n    Scales the given image based on the given scale factor.\n\n    Args:\n        image (ImageType): The image to be scaled. `ImageType` is a flexible type,\n            accepting either `numpy.ndarray` or `PIL.Image.Image`.\n        scale_factor (float): The factor by which the image will be scaled. Scale\n            factor &gt; `1.0` zooms in, &lt; `1.0` zooms out.\n\n    Returns:\n        (ImageType): The scaled image. The type is determined by the input type and\n            may be either a `numpy.ndarray` or `PIL.Image.Image`.\n\n    Raises:\n        ValueError: If the scale factor is non-positive.\n\n    === \"OpenCV\"\n\n        ```python\n        import cv2\n        import supervision as sv\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        image.shape\n        # (1080, 1920, 3)\n\n        scaled_image = sv.scale_image(image=image, scale_factor=0.5)\n        scaled_image.shape\n        # (540, 960, 3)\n        ```\n\n    === \"Pillow\"\n\n        ```python\n        from PIL import Image\n        import supervision as sv\n\n        image = Image.open(&lt;SOURCE_IMAGE_PATH&gt;)\n        image.size\n        # (1920, 1080)\n\n        scaled_image = sv.scale_image(image=image, scale_factor=0.5)\n        scaled_image.size\n        # (960, 540)\n        ```\n    \"\"\"\n    if scale_factor &lt;= 0:\n        raise ValueError(\"Scale factor must be positive.\")\n\n    width_old, height_old = image.shape[1], image.shape[0]\n    width_new = int(width_old * scale_factor)\n    height_new = int(height_old * scale_factor)\n    return cv2.resize(image, (width_new, height_new), interpolation=cv2.INTER_LINEAR)\n</code></pre> resize_image <p>Resizes the given image to a specified resolution. Can maintain the original aspect ratio or resize directly to the desired dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ImageType</code> <p>The image to be resized. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>resolution_wh</code> <code>Tuple[int, int]</code> <p>The target resolution as <code>(width, height)</code>.</p> required <code>keep_aspect_ratio</code> <code>bool</code> <p>Flag to maintain the image's original aspect ratio. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The resized image. The type is determined by the input type and may be either a <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> OpenCVPillow <pre><code>import cv2\nimport supervision as sv\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nimage.shape\n# (1080, 1920, 3)\n\nresized_image = sv.resize_image(\n    image=image, resolution_wh=(1000, 1000), keep_aspect_ratio=True\n)\nresized_image.shape\n# (562, 1000, 3)\n</code></pre> <pre><code>from PIL import Image\nimport supervision as sv\n\nimage = Image.open(&lt;SOURCE_IMAGE_PATH&gt;)\nimage.size\n# (1920, 1080)\n\nresized_image = sv.resize_image(\n    image=image, resolution_wh=(1000, 1000), keep_aspect_ratio=True\n)\nresized_image.size\n# (1000, 562)\n</code></pre> <p></p> Source code in <code>supervision/utils/image.py</code> <pre><code>@ensure_cv2_image_for_processing\ndef resize_image(\n    image: ImageType,\n    resolution_wh: Tuple[int, int],\n    keep_aspect_ratio: bool = False,\n) -&gt; ImageType:\n    \"\"\"\n    Resizes the given image to a specified resolution. Can maintain the original aspect\n    ratio or resize directly to the desired dimensions.\n\n    Args:\n        image (ImageType): The image to be resized. `ImageType` is a flexible type,\n            accepting either `numpy.ndarray` or `PIL.Image.Image`.\n        resolution_wh (Tuple[int, int]): The target resolution as\n            `(width, height)`.\n        keep_aspect_ratio (bool, optional): Flag to maintain the image's original\n            aspect ratio. Defaults to `False`.\n\n    Returns:\n        (ImageType): The resized image. The type is determined by the input type and\n            may be either a `numpy.ndarray` or `PIL.Image.Image`.\n\n    === \"OpenCV\"\n\n        ```python\n        import cv2\n        import supervision as sv\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        image.shape\n        # (1080, 1920, 3)\n\n        resized_image = sv.resize_image(\n            image=image, resolution_wh=(1000, 1000), keep_aspect_ratio=True\n        )\n        resized_image.shape\n        # (562, 1000, 3)\n        ```\n\n    === \"Pillow\"\n\n        ```python\n        from PIL import Image\n        import supervision as sv\n\n        image = Image.open(&lt;SOURCE_IMAGE_PATH&gt;)\n        image.size\n        # (1920, 1080)\n\n        resized_image = sv.resize_image(\n            image=image, resolution_wh=(1000, 1000), keep_aspect_ratio=True\n        )\n        resized_image.size\n        # (1000, 562)\n        ```\n\n    ![resize_image](https://media.roboflow.com/supervision-docs/resize-image.png){ align=center width=\"800\" }\n    \"\"\"  # noqa E501 // docs\n    if keep_aspect_ratio:\n        image_ratio = image.shape[1] / image.shape[0]\n        target_ratio = resolution_wh[0] / resolution_wh[1]\n        if image_ratio &gt;= target_ratio:\n            width_new = resolution_wh[0]\n            height_new = int(resolution_wh[0] / image_ratio)\n        else:\n            height_new = resolution_wh[1]\n            width_new = int(resolution_wh[1] * image_ratio)\n    else:\n        width_new, height_new = resolution_wh\n\n    return cv2.resize(image, (width_new, height_new), interpolation=cv2.INTER_LINEAR)\n</code></pre> letterbox_image <p>Resizes and pads an image to a specified resolution with a given color, maintaining the original aspect ratio.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ImageType</code> <p>The image to be resized. <code>ImageType</code> is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>resolution_wh</code> <code>Tuple[int, int]</code> <p>The target resolution as <code>(width, height)</code>.</p> required <code>color</code> <code>Union[Tuple[int, int, int], Color]</code> <p>The color to pad with. If tuple provided it should be in BGR format.</p> <code>BLACK</code> <p>Returns:</p> Type Description <code>ImageType</code> <p>The resized image. The type is determined by the input type and may be either a <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> OpenCVPillow <pre><code>import cv2\nimport supervision as sv\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\nimage.shape\n# (1080, 1920, 3)\n\nletterboxed_image = sv.letterbox_image(image=image, resolution_wh=(1000, 1000))\nletterboxed_image.shape\n# (1000, 1000, 3)\n</code></pre> <pre><code>from PIL import Image\nimport supervision as sv\n\nimage = Image.open(&lt;SOURCE_IMAGE_PATH&gt;)\nimage.size\n# (1920, 1080)\n\nletterboxed_image = sv.letterbox_image(image=image, resolution_wh=(1000, 1000))\nletterboxed_image.size\n# (1000, 1000)\n</code></pre> <p></p> Source code in <code>supervision/utils/image.py</code> <pre><code>@ensure_cv2_image_for_processing\ndef letterbox_image(\n    image: ImageType,\n    resolution_wh: Tuple[int, int],\n    color: Union[Tuple[int, int, int], Color] = Color.BLACK,\n) -&gt; ImageType:\n    \"\"\"\n    Resizes and pads an image to a specified resolution with a given color, maintaining\n    the original aspect ratio.\n\n    Args:\n        image (ImageType): The image to be resized. `ImageType` is a flexible type,\n            accepting either `numpy.ndarray` or `PIL.Image.Image`.\n        resolution_wh (Tuple[int, int]): The target resolution as\n            `(width, height)`.\n        color (Union[Tuple[int, int, int], Color]): The color to pad with. If tuple\n            provided it should be in BGR format.\n\n    Returns:\n        (ImageType): The resized image. The type is determined by the input type and\n            may be either a `numpy.ndarray` or `PIL.Image.Image`.\n\n    === \"OpenCV\"\n\n        ```python\n        import cv2\n        import supervision as sv\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        image.shape\n        # (1080, 1920, 3)\n\n        letterboxed_image = sv.letterbox_image(image=image, resolution_wh=(1000, 1000))\n        letterboxed_image.shape\n        # (1000, 1000, 3)\n        ```\n\n    === \"Pillow\"\n\n        ```python\n        from PIL import Image\n        import supervision as sv\n\n        image = Image.open(&lt;SOURCE_IMAGE_PATH&gt;)\n        image.size\n        # (1920, 1080)\n\n        letterboxed_image = sv.letterbox_image(image=image, resolution_wh=(1000, 1000))\n        letterboxed_image.size\n        # (1000, 1000)\n        ```\n\n    ![letterbox_image](https://media.roboflow.com/supervision-docs/letterbox-image.png){ align=center width=\"800\" }\n    \"\"\"  # noqa E501 // docs\n    color = unify_to_bgr(color=color)\n    resized_image = resize_image(\n        image=image, resolution_wh=resolution_wh, keep_aspect_ratio=True\n    )\n    height_new, width_new = resized_image.shape[:2]\n    padding_top = (resolution_wh[1] - height_new) // 2\n    padding_bottom = resolution_wh[1] - height_new - padding_top\n    padding_left = (resolution_wh[0] - width_new) // 2\n    padding_right = resolution_wh[0] - width_new - padding_left\n    return cv2.copyMakeBorder(\n        resized_image,\n        padding_top,\n        padding_bottom,\n        padding_left,\n        padding_right,\n        cv2.BORDER_CONSTANT,\n        value=color,\n    )\n</code></pre> overlay_image <p>Places an image onto a scene at a given anchor point, handling cases where the image's position is partially or completely outside the scene's bounds.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The background scene onto which the image is placed.</p> required <code>overlay</code> <code>ndarray</code> <p>The image to be placed onto the scene.</p> required <code>anchor</code> <code>Tuple[int, int]</code> <p>The <code>(x, y)</code> coordinates in the scene where the top-left corner of the image will be placed.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The result image with overlay.</p> <p>Examples:</p> <pre><code>import cv2\nimport numpy as np\nimport supervision as sv\n\nimage = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\noverlay = np.zeros((400, 400, 3), dtype=np.uint8)\nresult_image = sv.overlay_image(image=image, overlay=overlay, anchor=(200, 400))\n</code></pre> <p></p> Source code in <code>supervision/utils/image.py</code> <pre><code>def overlay_image(\n    image: npt.NDArray[np.uint8],\n    overlay: npt.NDArray[np.uint8],\n    anchor: Tuple[int, int],\n) -&gt; npt.NDArray[np.uint8]:\n    \"\"\"\n    Places an image onto a scene at a given anchor point, handling cases where\n    the image's position is partially or completely outside the scene's bounds.\n\n    Args:\n        image (np.ndarray): The background scene onto which the image is placed.\n        overlay (np.ndarray): The image to be placed onto the scene.\n        anchor (Tuple[int, int]): The `(x, y)` coordinates in the scene where the\n            top-left corner of the image will be placed.\n\n    Returns:\n        (np.ndarray): The result image with overlay.\n\n    Examples:\n        ```python\n        import cv2\n        import numpy as np\n        import supervision as sv\n\n        image = cv2.imread(&lt;SOURCE_IMAGE_PATH&gt;)\n        overlay = np.zeros((400, 400, 3), dtype=np.uint8)\n        result_image = sv.overlay_image(image=image, overlay=overlay, anchor=(200, 400))\n        ```\n\n    ![overlay_image](https://media.roboflow.com/supervision-docs/overlay-image.png){ align=center width=\"800\" }\n    \"\"\"  # noqa E501 // docs\n    scene_height, scene_width = image.shape[:2]\n    image_height, image_width = overlay.shape[:2]\n    anchor_x, anchor_y = anchor\n\n    is_out_horizontally = anchor_x + image_width &lt;= 0 or anchor_x &gt;= scene_width\n    is_out_vertically = anchor_y + image_height &lt;= 0 or anchor_y &gt;= scene_height\n\n    if is_out_horizontally or is_out_vertically:\n        return image\n\n    x_min = max(anchor_x, 0)\n    y_min = max(anchor_y, 0)\n    x_max = min(scene_width, anchor_x + image_width)\n    y_max = min(scene_height, anchor_y + image_height)\n\n    crop_x_min = max(-anchor_x, 0)\n    crop_y_min = max(-anchor_y, 0)\n    crop_x_max = image_width - max((anchor_x + image_width) - scene_width, 0)\n    crop_y_max = image_height - max((anchor_y + image_height) - scene_height, 0)\n\n    image[y_min:y_max, x_min:x_max] = overlay[\n        crop_y_min:crop_y_max, crop_x_min:crop_x_max\n    ]\n\n    return image\n</code></pre> ImageSink Source code in <code>supervision/utils/image.py</code> <pre><code>class ImageSink:\n    def __init__(\n        self,\n        target_dir_path: str,\n        overwrite: bool = False,\n        image_name_pattern: str = \"image_{:05d}.png\",\n    ):\n        \"\"\"\n        Initialize a context manager for saving images.\n\n        Args:\n            target_dir_path (str): The target directory where images will be saved.\n            overwrite (bool, optional): Whether to overwrite the existing directory.\n                Defaults to False.\n            image_name_pattern (str, optional): The image file name pattern.\n                Defaults to \"image_{:05d}.png\".\n\n        Examples:\n            ```python\n            import supervision as sv\n\n            frames_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;, stride=2)\n\n            with sv.ImageSink(target_dir_path=&lt;TARGET_CROPS_DIRECTORY&gt;) as sink:\n                for image in frames_generator:\n                    sink.save_image(image=image)\n            ```\n        \"\"\"  # noqa E501 // docs\n\n        self.target_dir_path = target_dir_path\n        self.overwrite = overwrite\n        self.image_name_pattern = image_name_pattern\n        self.image_count = 0\n\n    def __enter__(self):\n        if os.path.exists(self.target_dir_path):\n            if self.overwrite:\n                shutil.rmtree(self.target_dir_path)\n                os.makedirs(self.target_dir_path)\n        else:\n            os.makedirs(self.target_dir_path)\n\n        return self\n\n    def save_image(self, image: np.ndarray, image_name: Optional[str] = None):\n        \"\"\"\n        Save a given image in the target directory.\n\n        Args:\n            image (np.ndarray): The image to be saved. The image must be in BGR color\n                format.\n            image_name (str, optional): The name to use for the saved image.\n                If not provided, a name will be\n                generated using the `image_name_pattern`.\n        \"\"\"\n        if image_name is None:\n            image_name = self.image_name_pattern.format(self.image_count)\n\n        image_path = os.path.join(self.target_dir_path, image_name)\n        cv2.imwrite(image_path, image)\n        self.image_count += 1\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        pass\n</code></pre>"},{"location":"utils/image/#supervision.utils.image.ImageSink-functions","title":"Functions","text":""},{"location":"utils/image/#supervision.utils.image.ImageSink.__init__","title":"<code>__init__(target_dir_path, overwrite=False, image_name_pattern='image_{:05d}.png')</code>","text":"<p>Initialize a context manager for saving images.</p> <p>Parameters:</p> Name Type Description Default <code>target_dir_path</code> <code>str</code> <p>The target directory where images will be saved.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the existing directory. Defaults to False.</p> <code>False</code> <code>image_name_pattern</code> <code>str</code> <p>The image file name pattern. Defaults to \"image_{:05d}.png\".</p> <code>'image_{:05d}.png'</code> <p>Examples:</p> <pre><code>import supervision as sv\n\nframes_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;, stride=2)\n\nwith sv.ImageSink(target_dir_path=&lt;TARGET_CROPS_DIRECTORY&gt;) as sink:\n    for image in frames_generator:\n        sink.save_image(image=image)\n</code></pre> Source code in <code>supervision/utils/image.py</code> <pre><code>def __init__(\n    self,\n    target_dir_path: str,\n    overwrite: bool = False,\n    image_name_pattern: str = \"image_{:05d}.png\",\n):\n    \"\"\"\n    Initialize a context manager for saving images.\n\n    Args:\n        target_dir_path (str): The target directory where images will be saved.\n        overwrite (bool, optional): Whether to overwrite the existing directory.\n            Defaults to False.\n        image_name_pattern (str, optional): The image file name pattern.\n            Defaults to \"image_{:05d}.png\".\n\n    Examples:\n        ```python\n        import supervision as sv\n\n        frames_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;, stride=2)\n\n        with sv.ImageSink(target_dir_path=&lt;TARGET_CROPS_DIRECTORY&gt;) as sink:\n            for image in frames_generator:\n                sink.save_image(image=image)\n        ```\n    \"\"\"  # noqa E501 // docs\n\n    self.target_dir_path = target_dir_path\n    self.overwrite = overwrite\n    self.image_name_pattern = image_name_pattern\n    self.image_count = 0\n</code></pre>"},{"location":"utils/image/#supervision.utils.image.ImageSink.save_image","title":"<code>save_image(image, image_name=None)</code>","text":"<p>Save a given image in the target directory.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>The image to be saved. The image must be in BGR color format.</p> required <code>image_name</code> <code>str</code> <p>The name to use for the saved image. If not provided, a name will be generated using the <code>image_name_pattern</code>.</p> <code>None</code> Source code in <code>supervision/utils/image.py</code> <pre><code>def save_image(self, image: np.ndarray, image_name: Optional[str] = None):\n    \"\"\"\n    Save a given image in the target directory.\n\n    Args:\n        image (np.ndarray): The image to be saved. The image must be in BGR color\n            format.\n        image_name (str, optional): The name to use for the saved image.\n            If not provided, a name will be\n            generated using the `image_name_pattern`.\n    \"\"\"\n    if image_name is None:\n        image_name = self.image_name_pattern.format(self.image_count)\n\n    image_path = os.path.join(self.target_dir_path, image_name)\n    cv2.imwrite(image_path, image)\n    self.image_count += 1\n</code></pre>"},{"location":"utils/iterables/","title":"Iterables Utils","text":"create_batches <p>Provides a generator that yields chunks of the input sequence of the size specified by the <code>batch_size</code> parameter. The last chunk may be a smaller batch.</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <code>Iterable[V]</code> <p>The sequence to be split into batches.</p> required <code>batch_size</code> <code>int</code> <p>The expected size of a batch.</p> required <p>Returns:</p> Type Description <code>Generator[List[V], None, None]</code> <p>A generator that yields chunks of <code>sequence</code> of size <code>batch_size</code>, up to the length of the input <code>sequence</code>.</p> <p>Examples:</p> <pre><code>list(create_batches([1, 2, 3, 4, 5], 2))\n# [[1, 2], [3, 4], [5]]\n\nlist(create_batches(\"abcde\", 3))\n# [['a', 'b', 'c'], ['d', 'e']]\n</code></pre> Source code in <code>supervision/utils/iterables.py</code> <pre><code>def create_batches(\n    sequence: Iterable[V], batch_size: int\n) -&gt; Generator[List[V], None, None]:\n    \"\"\"\n    Provides a generator that yields chunks of the input sequence\n    of the size specified by the `batch_size` parameter. The last\n    chunk may be a smaller batch.\n\n    Args:\n        sequence (Iterable[V]): The sequence to be split into batches.\n        batch_size (int): The expected size of a batch.\n\n    Returns:\n        (Generator[List[V], None, None]): A generator that yields chunks\n            of `sequence` of size `batch_size`, up to the length of\n            the input `sequence`.\n\n    Examples:\n        ```python\n        list(create_batches([1, 2, 3, 4, 5], 2))\n        # [[1, 2], [3, 4], [5]]\n\n        list(create_batches(\"abcde\", 3))\n        # [['a', 'b', 'c'], ['d', 'e']]\n        ```\n    \"\"\"\n    batch_size = max(batch_size, 1)\n    current_batch = []\n    for element in sequence:\n        if len(current_batch) == batch_size:\n            yield current_batch\n            current_batch = []\n        current_batch.append(element)\n    if current_batch:\n        yield current_batch\n</code></pre> fill <p>Fill the sequence with padding elements until the sequence reaches the desired size.</p> <p>Parameters:</p> Name Type Description Default <code>sequence</code> <code>List[V]</code> <p>The input sequence.</p> required <code>desired_size</code> <code>int</code> <p>The expected size of the output list. The difference between this value and the actual length of <code>sequence</code> (if positive) dictates how many elements will be added as padding.</p> required <code>content</code> <code>V</code> <p>The element to be placed at the end of the input <code>sequence</code> as padding.</p> required <p>Returns:</p> Type Description <code>List[V]</code> <p>A padded version of the input <code>sequence</code> (if needed).</p> <p>Examples:</p> <pre><code>fill([1, 2], 4, 0)\n# [1, 2, 0, 0]\n\nfill(['a', 'b'], 3, 'c')\n# ['a', 'b', 'c']\n</code></pre> Source code in <code>supervision/utils/iterables.py</code> <pre><code>def fill(sequence: List[V], desired_size: int, content: V) -&gt; List[V]:\n    \"\"\"\n    Fill the sequence with padding elements until the sequence reaches\n    the desired size.\n\n    Args:\n        sequence (List[V]): The input sequence.\n        desired_size (int): The expected size of the output list. The\n            difference between this value and the actual length of `sequence`\n            (if positive) dictates how many elements will be added as padding.\n        content (V): The element to be placed at the end of the input\n            `sequence` as padding.\n\n    Returns:\n        (List[V]): A padded version of the input `sequence` (if needed).\n\n    Examples:\n        ```python\n        fill([1, 2], 4, 0)\n        # [1, 2, 0, 0]\n\n        fill(['a', 'b'], 3, 'c')\n        # ['a', 'b', 'c']\n        ```\n    \"\"\"\n    missing_size = max(0, desired_size - len(sequence))\n    sequence.extend([content] * missing_size)\n    return sequence\n</code></pre>"},{"location":"utils/notebook/","title":"Notebooks Utils","text":"plot_image <p>Plots image using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ImageType</code> <p>The frame to be displayed ImageType  is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>size</code> <code>Tuple[int, int]</code> <p>The size of the plot in inches.</p> <code>(12, 12)</code> <code>cmap</code> <code>str</code> <p>the colormap to use for single channel images.</p> <code>'gray'</code> <p>Examples:</p> <pre><code>import cv2\nimport supervision as sv\n\nimage = cv2.imread(\"path/to/image.jpg\")\n\n%matplotlib inline\nsv.plot_image(image=image, size=(16, 16))\n</code></pre> Source code in <code>supervision/utils/notebook.py</code> <pre><code>def plot_image(\n    image: ImageType, size: Tuple[int, int] = (12, 12), cmap: Optional[str] = \"gray\"\n) -&gt; None:\n    \"\"\"\n    Plots image using matplotlib.\n\n    Args:\n        image (ImageType): The frame to be displayed ImageType\n             is a flexible type, accepting either `numpy.ndarray` or `PIL.Image.Image`.\n        size (Tuple[int, int]): The size of the plot in inches.\n        cmap (str): the colormap to use for single channel images.\n\n    Examples:\n        ```python\n        import cv2\n        import supervision as sv\n\n        image = cv2.imread(\"path/to/image.jpg\")\n\n        %matplotlib inline\n        sv.plot_image(image=image, size=(16, 16))\n        ```\n    \"\"\"\n    if isinstance(image, Image.Image):\n        image = pillow_to_cv2(image)\n\n    plt.figure(figsize=size)\n\n    if image.ndim == 2:\n        plt.imshow(image, cmap=cmap)\n    else:\n        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n    plt.axis(\"off\")\n    plt.show()\n</code></pre> plot_images_grid <p>Plots images in a grid using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>List[ImageType]</code> <p>A list of images as ImageType    is a flexible type, accepting either <code>numpy.ndarray</code> or <code>PIL.Image.Image</code>.</p> required <code>grid_size</code> <code>Tuple[int, int]</code> <p>A tuple specifying the number   of rows and columns for the grid.</p> required <code>titles</code> <code>Optional[List[str]]</code> <p>A list of titles for each image.   Defaults to None.</p> <code>None</code> <code>size</code> <code>Tuple[int, int]</code> <p>A tuple specifying the width and   height of the entire plot in inches.</p> <code>(12, 12)</code> <code>cmap</code> <code>str</code> <p>the colormap to use for single channel images.</p> <code>'gray'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of images exceeds the grid size.</p> <p>Examples:</p> <pre><code>import cv2\nimport supervision as sv\nfrom PIL import Image\n\nimage1 = cv2.imread(\"path/to/image1.jpg\")\nimage2 = Image.open(\"path/to/image2.jpg\")\nimage3 = cv2.imread(\"path/to/image3.jpg\")\n\nimages = [image1, image2, image3]\ntitles = [\"Image 1\", \"Image 2\", \"Image 3\"]\n\n%matplotlib inline\nplot_images_grid(images, grid_size=(2, 2), titles=titles, size=(16, 16))\n</code></pre> Source code in <code>supervision/utils/notebook.py</code> <pre><code>def plot_images_grid(\n    images: List[ImageType],\n    grid_size: Tuple[int, int],\n    titles: Optional[List[str]] = None,\n    size: Tuple[int, int] = (12, 12),\n    cmap: Optional[str] = \"gray\",\n) -&gt; None:\n    \"\"\"\n    Plots images in a grid using matplotlib.\n\n    Args:\n       images (List[ImageType]): A list of images as ImageType\n             is a flexible type, accepting either `numpy.ndarray` or `PIL.Image.Image`.\n       grid_size (Tuple[int, int]): A tuple specifying the number\n            of rows and columns for the grid.\n       titles (Optional[List[str]]): A list of titles for each image.\n            Defaults to None.\n       size (Tuple[int, int]): A tuple specifying the width and\n            height of the entire plot in inches.\n       cmap (str): the colormap to use for single channel images.\n\n    Raises:\n       ValueError: If the number of images exceeds the grid size.\n\n    Examples:\n        ```python\n        import cv2\n        import supervision as sv\n        from PIL import Image\n\n        image1 = cv2.imread(\"path/to/image1.jpg\")\n        image2 = Image.open(\"path/to/image2.jpg\")\n        image3 = cv2.imread(\"path/to/image3.jpg\")\n\n        images = [image1, image2, image3]\n        titles = [\"Image 1\", \"Image 2\", \"Image 3\"]\n\n        %matplotlib inline\n        plot_images_grid(images, grid_size=(2, 2), titles=titles, size=(16, 16))\n        ```\n    \"\"\"\n    nrows, ncols = grid_size\n\n    for idx, img in enumerate(images):\n        if isinstance(img, Image.Image):\n            images[idx] = pillow_to_cv2(img)\n\n    if len(images) &gt; nrows * ncols:\n        raise ValueError(\n            \"The number of images exceeds the grid size. Please increase the grid size\"\n            \" or reduce the number of images.\"\n        )\n\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=size)\n\n    for idx, ax in enumerate(axes.flat):\n        if idx &lt; len(images):\n            if images[idx].ndim == 2:\n                ax.imshow(images[idx], cmap=cmap)\n            else:\n                ax.imshow(cv2.cvtColor(images[idx], cv2.COLOR_BGR2RGB))\n\n            if titles is not None and idx &lt; len(titles):\n                ax.set_title(titles[idx])\n\n        ax.axis(\"off\")\n    plt.show()\n</code></pre>"},{"location":"utils/video/","title":"Video Utils","text":"VideoInfo <p>A class to store video information, including width, height, fps and     total number of frames.</p> <p>Attributes:</p> Name Type Description <code>width</code> <code>int</code> <p>width of the video in pixels</p> <code>height</code> <code>int</code> <p>height of the video in pixels</p> <code>fps</code> <code>int</code> <p>frames per second of the video</p> <code>total_frames</code> <code>int</code> <p>total number of frames in the video, default is None</p> <p>Examples:</p> <pre><code>import supervision as sv\n\nvideo_info = sv.VideoInfo.from_video_path(video_path=&lt;SOURCE_VIDEO_FILE&gt;)\n\nvideo_info\n# VideoInfo(width=3840, height=2160, fps=25, total_frames=538)\n\nvideo_info.resolution_wh\n# (3840, 2160)\n</code></pre> Source code in <code>supervision/utils/video.py</code> <pre><code>@dataclass\nclass VideoInfo:\n    \"\"\"\n    A class to store video information, including width, height, fps and\n        total number of frames.\n\n    Attributes:\n        width (int): width of the video in pixels\n        height (int): height of the video in pixels\n        fps (int): frames per second of the video\n        total_frames (int, optional): total number of frames in the video,\n            default is None\n\n    Examples:\n        ```python\n        import supervision as sv\n\n        video_info = sv.VideoInfo.from_video_path(video_path=&lt;SOURCE_VIDEO_FILE&gt;)\n\n        video_info\n        # VideoInfo(width=3840, height=2160, fps=25, total_frames=538)\n\n        video_info.resolution_wh\n        # (3840, 2160)\n        ```\n    \"\"\"\n\n    width: int\n    height: int\n    fps: int\n    total_frames: Optional[int] = None\n\n    @classmethod\n    def from_video_path(cls, video_path: str) -&gt; VideoInfo:\n        video = cv2.VideoCapture(video_path)\n        if not video.isOpened():\n            raise Exception(f\"Could not open video at {video_path}\")\n\n        width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        fps = int(video.get(cv2.CAP_PROP_FPS))\n        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n        video.release()\n        return VideoInfo(width, height, fps, total_frames)\n\n    @property\n    def resolution_wh(self) -&gt; Tuple[int, int]:\n        return self.width, self.height\n</code></pre> VideoSink <p>Context manager that saves video frames to a file using OpenCV.</p> <p>Attributes:</p> Name Type Description <code>target_path</code> <code>str</code> <p>The path to the output file where the video will be saved.</p> <code>video_info</code> <code>VideoInfo</code> <p>Information about the video resolution, fps, and total frame count.</p> <code>codec</code> <code>str</code> <p>FOURCC code for video format</p> Example <pre><code>import supervision as sv\n\nvideo_info = sv.VideoInfo.from_video_path(&lt;SOURCE_VIDEO_PATH&gt;)\nframes_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;)\n\nwith sv.VideoSink(target_path=&lt;TARGET_VIDEO_PATH&gt;, video_info=video_info) as sink:\n    for frame in frames_generator:\n        sink.write_frame(frame=frame)\n</code></pre> Source code in <code>supervision/utils/video.py</code> <pre><code>class VideoSink:\n    \"\"\"\n    Context manager that saves video frames to a file using OpenCV.\n\n    Attributes:\n        target_path (str): The path to the output file where the video will be saved.\n        video_info (VideoInfo): Information about the video resolution, fps,\n            and total frame count.\n        codec (str): FOURCC code for video format\n\n    Example:\n        ```python\n        import supervision as sv\n\n        video_info = sv.VideoInfo.from_video_path(&lt;SOURCE_VIDEO_PATH&gt;)\n        frames_generator = sv.get_video_frames_generator(&lt;SOURCE_VIDEO_PATH&gt;)\n\n        with sv.VideoSink(target_path=&lt;TARGET_VIDEO_PATH&gt;, video_info=video_info) as sink:\n            for frame in frames_generator:\n                sink.write_frame(frame=frame)\n        ```\n    \"\"\"  # noqa: E501 // docs\n\n    def __init__(self, target_path: str, video_info: VideoInfo, codec: str = \"mp4v\"):\n        self.target_path = target_path\n        self.video_info = video_info\n        self.__codec = codec\n        self.__writer = None\n\n    def __enter__(self):\n        try:\n            self.__fourcc = cv2.VideoWriter_fourcc(*self.__codec)\n        except TypeError as e:\n            print(str(e) + \". Defaulting to mp4v...\")\n            self.__fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n        self.__writer = cv2.VideoWriter(\n            self.target_path,\n            self.__fourcc,\n            self.video_info.fps,\n            self.video_info.resolution_wh,\n        )\n        return self\n\n    def write_frame(self, frame: np.ndarray):\n        \"\"\"\n        Writes a single video frame to the target video file.\n\n        Args:\n            frame (np.ndarray): The video frame to be written to the file. The frame\n                must be in BGR color format.\n        \"\"\"\n        self.__writer.write(frame)\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        self.__writer.release()\n</code></pre> FPSMonitor <p>A class for monitoring frames per second (FPS) to benchmark latency.</p> Source code in <code>supervision/utils/video.py</code> <pre><code>class FPSMonitor:\n    \"\"\"\n    A class for monitoring frames per second (FPS) to benchmark latency.\n    \"\"\"\n\n    def __init__(self, sample_size: int = 30):\n        \"\"\"\n        Args:\n            sample_size (int): The maximum number of observations for latency\n                benchmarking.\n\n        Examples:\n            ```python\n            import supervision as sv\n\n            frames_generator = sv.get_video_frames_generator(source_path=&lt;SOURCE_FILE_PATH&gt;)\n            fps_monitor = sv.FPSMonitor()\n\n            for frame in frames_generator:\n                # your processing code here\n                fps_monitor.tick()\n                fps = fps_monitor.fps\n            ```\n        \"\"\"  # noqa: E501 // docs\n        self.all_timestamps = deque(maxlen=sample_size)\n\n    @property\n    def fps(self) -&gt; float:\n        \"\"\"\n        Computes and returns the average FPS based on the stored time stamps.\n\n        Returns:\n            float: The average FPS. Returns 0.0 if no time stamps are stored.\n        \"\"\"\n        if not self.all_timestamps:\n            return 0.0\n        taken_time = self.all_timestamps[-1] - self.all_timestamps[0]\n        return (len(self.all_timestamps)) / taken_time if taken_time != 0 else 0.0\n\n    def tick(self) -&gt; None:\n        \"\"\"\n        Adds a new time stamp to the deque for FPS calculation.\n        \"\"\"\n        self.all_timestamps.append(time.monotonic())\n\n    def reset(self) -&gt; None:\n        \"\"\"\n        Clears all the time stamps from the deque.\n        \"\"\"\n        self.all_timestamps.clear()\n</code></pre> get_video_frames_generator <p>Get a generator that yields the frames of the video.</p> <p>Parameters:</p> Name Type Description Default <code>source_path</code> <code>str</code> <p>The path of the video file.</p> required <code>stride</code> <code>int</code> <p>Indicates the interval at which frames are returned, skipping stride - 1 frames between each.</p> <code>1</code> <code>start</code> <code>int</code> <p>Indicates the starting position from which video should generate frames</p> <code>0</code> <code>end</code> <code>Optional[int]</code> <p>Indicates the ending position at which video should stop generating frames. If None, video will be read to the end.</p> <code>None</code> <p>Returns:</p> Type Description <code>Generator[ndarray, None, None]</code> <p>A generator that yields the frames of the video.</p> <p>Examples:</p> <pre><code>import supervision as sv\n\nfor frame in sv.get_video_frames_generator(source_path=&lt;SOURCE_VIDEO_PATH&gt;):\n    ...\n</code></pre> Source code in <code>supervision/utils/video.py</code> <pre><code>def get_video_frames_generator(\n    source_path: str, stride: int = 1, start: int = 0, end: Optional[int] = None\n) -&gt; Generator[np.ndarray, None, None]:\n    \"\"\"\n    Get a generator that yields the frames of the video.\n\n    Args:\n        source_path (str): The path of the video file.\n        stride (int): Indicates the interval at which frames are returned,\n            skipping stride - 1 frames between each.\n        start (int): Indicates the starting position from which\n            video should generate frames\n        end (Optional[int]): Indicates the ending position at which video\n            should stop generating frames. If None, video will be read to the end.\n\n    Returns:\n        (Generator[np.ndarray, None, None]): A generator that yields the\n            frames of the video.\n\n    Examples:\n        ```python\n        import supervision as sv\n\n        for frame in sv.get_video_frames_generator(source_path=&lt;SOURCE_VIDEO_PATH&gt;):\n            ...\n        ```\n    \"\"\"\n    video, start, end = _validate_and_setup_video(source_path, start, end)\n    frame_position = start\n    while True:\n        success, frame = video.read()\n        if not success or frame_position &gt;= end:\n            break\n        yield frame\n        for _ in range(stride - 1):\n            success = video.grab()\n            if not success:\n                break\n        frame_position += stride\n    video.release()\n</code></pre> process_video <p>Process a video file by applying a callback function on each frame     and saving the result to a target video file.</p> <p>Parameters:</p> Name Type Description Default <code>source_path</code> <code>str</code> <p>The path to the source video file.</p> required <code>target_path</code> <code>str</code> <p>The path to the target video file.</p> required <code>callback</code> <code>Callable[[ndarray, int], ndarray]</code> <p>A function that takes in a numpy ndarray representation of a video frame and an int index of the frame and returns a processed numpy ndarray representation of the frame.</p> required <p>Examples:</p> <pre><code>import supervision as sv\n\ndef callback(scene: np.ndarray, index: int) -&gt; np.ndarray:\n    ...\n\nprocess_video(\n    source_path=&lt;SOURCE_VIDEO_PATH&gt;,\n    target_path=&lt;TARGET_VIDEO_PATH&gt;,\n    callback=callback\n)\n</code></pre> Source code in <code>supervision/utils/video.py</code> <pre><code>def process_video(\n    source_path: str,\n    target_path: str,\n    callback: Callable[[np.ndarray, int], np.ndarray],\n) -&gt; None:\n    \"\"\"\n    Process a video file by applying a callback function on each frame\n        and saving the result to a target video file.\n\n    Args:\n        source_path (str): The path to the source video file.\n        target_path (str): The path to the target video file.\n        callback (Callable[[np.ndarray, int], np.ndarray]): A function that takes in\n            a numpy ndarray representation of a video frame and an\n            int index of the frame and returns a processed numpy ndarray\n            representation of the frame.\n\n    Examples:\n        ```python\n        import supervision as sv\n\n        def callback(scene: np.ndarray, index: int) -&gt; np.ndarray:\n            ...\n\n        process_video(\n            source_path=&lt;SOURCE_VIDEO_PATH&gt;,\n            target_path=&lt;TARGET_VIDEO_PATH&gt;,\n            callback=callback\n        )\n        ```\n    \"\"\"\n    source_video_info = VideoInfo.from_video_path(video_path=source_path)\n    with VideoSink(target_path=target_path, video_info=source_video_info) as sink:\n        for index, frame in enumerate(\n            get_video_frames_generator(source_path=source_path)\n        ):\n            result_frame = callback(frame, index)\n            sink.write_frame(frame=result_frame)\n</code></pre>"},{"location":"utils/video/#supervision.utils.video.VideoSink-functions","title":"Functions","text":""},{"location":"utils/video/#supervision.utils.video.VideoSink.write_frame","title":"<code>write_frame(frame)</code>","text":"<p>Writes a single video frame to the target video file.</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>ndarray</code> <p>The video frame to be written to the file. The frame must be in BGR color format.</p> required Source code in <code>supervision/utils/video.py</code> <pre><code>def write_frame(self, frame: np.ndarray):\n    \"\"\"\n    Writes a single video frame to the target video file.\n\n    Args:\n        frame (np.ndarray): The video frame to be written to the file. The frame\n            must be in BGR color format.\n    \"\"\"\n    self.__writer.write(frame)\n</code></pre>"},{"location":"utils/video/#supervision.utils.video.FPSMonitor-attributes","title":"Attributes","text":""},{"location":"utils/video/#supervision.utils.video.FPSMonitor.fps","title":"<code>fps: float</code>  <code>property</code>","text":"<p>Computes and returns the average FPS based on the stored time stamps.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The average FPS. Returns 0.0 if no time stamps are stored.</p>"},{"location":"utils/video/#supervision.utils.video.FPSMonitor-functions","title":"Functions","text":""},{"location":"utils/video/#supervision.utils.video.FPSMonitor.__init__","title":"<code>__init__(sample_size=30)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>sample_size</code> <code>int</code> <p>The maximum number of observations for latency benchmarking.</p> <code>30</code> <p>Examples:</p> <pre><code>import supervision as sv\n\nframes_generator = sv.get_video_frames_generator(source_path=&lt;SOURCE_FILE_PATH&gt;)\nfps_monitor = sv.FPSMonitor()\n\nfor frame in frames_generator:\n    # your processing code here\n    fps_monitor.tick()\n    fps = fps_monitor.fps\n</code></pre> Source code in <code>supervision/utils/video.py</code> <pre><code>def __init__(self, sample_size: int = 30):\n    \"\"\"\n    Args:\n        sample_size (int): The maximum number of observations for latency\n            benchmarking.\n\n    Examples:\n        ```python\n        import supervision as sv\n\n        frames_generator = sv.get_video_frames_generator(source_path=&lt;SOURCE_FILE_PATH&gt;)\n        fps_monitor = sv.FPSMonitor()\n\n        for frame in frames_generator:\n            # your processing code here\n            fps_monitor.tick()\n            fps = fps_monitor.fps\n        ```\n    \"\"\"  # noqa: E501 // docs\n    self.all_timestamps = deque(maxlen=sample_size)\n</code></pre>"},{"location":"utils/video/#supervision.utils.video.FPSMonitor.reset","title":"<code>reset()</code>","text":"<p>Clears all the time stamps from the deque.</p> Source code in <code>supervision/utils/video.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"\n    Clears all the time stamps from the deque.\n    \"\"\"\n    self.all_timestamps.clear()\n</code></pre>"},{"location":"utils/video/#supervision.utils.video.FPSMonitor.tick","title":"<code>tick()</code>","text":"<p>Adds a new time stamp to the deque for FPS calculation.</p> Source code in <code>supervision/utils/video.py</code> <pre><code>def tick(self) -&gt; None:\n    \"\"\"\n    Adds a new time stamp to the deque for FPS calculation.\n    \"\"\"\n    self.all_timestamps.append(time.monotonic())\n</code></pre>"}]}