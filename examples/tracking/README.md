# tracking

## üëã hello

This script provides functionality for processing videos using YOLOv8 for object
detection and Supervision for tracking and annotation.

## üíª install

- clone repository and navigate to example directory

  ```bash
  git clone https://github.com/roboflow/supervision.git
  cd supervision/examples/tracking
  ```

- setup python environment and activate it [optional]

  ```bash
  python3 -m venv venv
  source venv/bin/activate
  ```

- install required dependencies

  ```bash
  pip install -r requirements.txt
  ```

## üõ†Ô∏è script arguments

- ultralytics

  - `--source_weights_path`: Required. Specifies the path to the YOLO model's weights
    file, which is essential for the object detection process. This file contains the data
    that the model uses to identify objects in the video.

  - `--source_video_path`: Required. The path to the source video file to be processed.
    This is the video on which object detection and annotation will be performed.
  - `--target_video_path`: Required. The path where the processed video, with annotations
    added, will be saved. This is your output video file.
  - `--confidence_threshold` (optional): Sets the confidence level at which the model
    identifies objects in the video. Default is `0.3`. A higher threshold makes the model
    more selective, while a lower threshold makes it more inclusive in identifying objects.
  - `--iou_threshold` (optional): Specifies the IOU (Intersection Over Union) threshold
    for the model, defaulting to `0.7`. This parameter helps in differentiating between
    distinct objects, especially in crowded scenes.

- inference

  - `--roboflow_api_key` (optional): The API key for Roboflow services. If not provided
    directly, the script tries to fetch it from the `ROBOFLOW_API_KEY` environment
    variable. Follow [this guide](https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key)
    to acquire your `API KEY`.
  - `--model_id` (optional): Designates the Roboflow model ID to be used. The default
    value is `"yolov8x-1280"`.

  - `--source_video_path`: Required. The path to the source video file to be processed.
    This is the video on which object detection and annotation will be performed.
  - `--target_video_path`: Required. The path where the processed video, with annotations
    added, will be saved. This is your output video file.
  - `--confidence_threshold` (optional): Sets the confidence level at which the model
    identifies objects in the video. Default is `0.3`. A higher threshold makes the model
    more selective, while a lower threshold makes it more inclusive in identifying objects.
  - `--iou_threshold` (optional): Specifies the IOU (Intersection Over Union) threshold
    for the model, defaulting to `0.7`. This parameter helps in differentiating between
    distinct objects, especially in crowded scenes.

## ‚öôÔ∏è run

- inference

  ```bash
  python inference_example.py \
  --roboflow_api_key <ROBOFLOW API KEY> \
  --source_video_path input.mp4 \
  --target_video_path tracking_result.mp4
  ```

- ultralytics

  ```bash
  python ultralytics_example.py \
  --source_weights_path yolov8s.pt \
  --source_video_path input.mp4 \
  --target_video_path tracking_result.mp4
  ```

## ¬© license

This demo integrates two main components, each with its own licensing:

- ultralytics: The object detection model used in this demo, YOLOv8, is distributed
  under the [AGPL-3.0 license](https://github.com/ultralytics/ultralytics/blob/main/LICENSE).
  You can find more details about this license here.

- supervision: The analytics code that powers the zone-based analysis in this demo is
  based on the Supervision library, which is licensed under the
  [MIT license](https://github.com/roboflow/supervision/blob/develop/LICENSE.md). This
  makes the Supervision part of the code fully open source and freely usable in your
  projects.
