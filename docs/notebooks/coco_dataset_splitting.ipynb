{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e61a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "COCO Dataset Splitting with Supervision ğŸ“Š\n",
    "===========================================\n",
    "\n",
    "This notebook demonstrates how to efficiently split COCO format datasets\n",
    "for machine learning model training, validation, and testing.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow/supervision/blob/develop/examples/notebooks/coco_dataset_splitting.ipynb)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7037ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Installation and Imports\n",
    "print(\"ğŸ”§ Installing required packages...\")\n",
    "\n",
    "# Install supervision and dependencies\n",
    "!pip install supervision pycocotools pillow\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import supervision as sv\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"âœ… Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd63de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Download Sample Dataset\n",
    "print(\"ğŸ“¥ Setting up sample COCO dataset...\")\n",
    "\n",
    "# Create sample COCO dataset for demonstration\n",
    "sample_data = {\n",
    "    \"info\": {\n",
    "        \"description\": \"Sample COCO Dataset for Splitting Demo\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"year\": 2024,\n",
    "        \"contributor\": \"Supervision Community\",\n",
    "        \"date_created\": \"2024-01-01T00:00:00+00:00\"\n",
    "    },\n",
    "    \"licenses\": [\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"name\": \"Sample License\",\n",
    "            \"url\": \"https://github.com/roboflow/supervision\"\n",
    "        }\n",
    "    ],\n",
    "    \"categories\": [\n",
    "        {\"id\": 1, \"name\": \"person\", \"supercategory\": \"person\"},\n",
    "        {\"id\": 2, \"name\": \"bicycle\", \"supercategory\": \"vehicle\"},\n",
    "        {\"id\": 3, \"name\": \"car\", \"supercategory\": \"vehicle\"},\n",
    "        {\"id\": 4, \"name\": \"dog\", \"supercategory\": \"animal\"}\n",
    "    ],\n",
    "    \"images\": [],\n",
    "    \"annotations\": []\n",
    "}\n",
    "\n",
    "# Generate sample images and annotations\n",
    "np.random.seed(42)  # For reproducible results\n",
    "num_images = 100\n",
    "\n",
    "print(f\"ğŸ–¼ï¸ Generating {num_images} sample images with annotations...\")\n",
    "\n",
    "# Create images directory\n",
    "images_dir = Path(\"sample_dataset/images\")\n",
    "images_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "annotation_id = 1\n",
    "\n",
    "for img_id in range(1, num_images + 1):\n",
    "    # Create sample image metadata\n",
    "    img_info = {\n",
    "        \"id\": img_id,\n",
    "        \"file_name\": f\"sample_{img_id:03d}.jpg\",\n",
    "        \"width\": 640,\n",
    "        \"height\": 480,\n",
    "        \"date_captured\": \"2024-01-01T12:00:00+00:00\"\n",
    "    }\n",
    "    sample_data[\"images\"].append(img_info)\n",
    "    \n",
    "    # Create a simple colored image for demonstration\n",
    "    img = Image.new('RGB', (640, 480), \n",
    "                    color=(np.random.randint(100, 255), \n",
    "                           np.random.randint(100, 255), \n",
    "                           np.random.randint(100, 255)))\n",
    "    img.save(images_dir / img_info[\"file_name\"])\n",
    "    \n",
    "    # Generate random annotations for this image\n",
    "    num_annotations = np.random.randint(1, 5)  # 1-4 annotations per image\n",
    "    \n",
    "    for _ in range(num_annotations):\n",
    "        # Random bounding box\n",
    "        x = np.random.randint(0, 500)\n",
    "        y = np.random.randint(0, 350)\n",
    "        w = np.random.randint(50, 140)\n",
    "        h = np.random.randint(50, 130)\n",
    "        \n",
    "        annotation = {\n",
    "            \"id\": annotation_id,\n",
    "            \"image_id\": img_id,\n",
    "            \"category_id\": np.random.randint(1, 5),  # Random category\n",
    "            \"bbox\": [x, y, w, h],\n",
    "            \"area\": w * h,\n",
    "            \"iscrowd\": 0,\n",
    "            \"segmentation\": []  # Empty for bounding box only\n",
    "        }\n",
    "        sample_data[\"annotations\"].append(annotation)\n",
    "        annotation_id += 1\n",
    "\n",
    "# Save annotations file\n",
    "annotations_file = Path(\"sample_dataset/annotations.json\")\n",
    "with open(annotations_file, 'w') as f:\n",
    "    json.dump(sample_data, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Created sample dataset:\")\n",
    "print(f\"   ğŸ“ {len(sample_data['images'])} images\")\n",
    "print(f\"   ğŸ·ï¸ {len(sample_data['annotations'])} annotations\") \n",
    "print(f\"   ğŸ“‚ {len(sample_data['categories'])} categories\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdeeb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Basic Dataset Splitting\n",
    "print(\"âœ‚ï¸ Splitting COCO dataset with Supervision...\")\n",
    "\n",
    "# Perform basic dataset split\n",
    "stats = sv.split_coco_dataset(\n",
    "    annotations_path=\"sample_dataset/annotations.json\",\n",
    "    images_directory=\"sample_dataset/images\",\n",
    "    output_directory=\"dataset_splits\",\n",
    "    val_percentage=0.2,    # 20% for validation\n",
    "    test_percentage=0.1,   # 10% for testing (70% for training)\n",
    "    random_state=42,       # For reproducible splits\n",
    "    verify_images=True     # Verify image files exist\n",
    ")\n",
    "\n",
    "print(\"ğŸ“Š Split Statistics:\")\n",
    "for split_name, split_stats in stats.items():\n",
    "    if split_name != 'total':\n",
    "        print(f\"   {split_name.capitalize()}: {split_stats['images']} images, {split_stats['annotations']} annotations\")\n",
    "\n",
    "print(f\"   Total: {stats['total']['images']} images, {stats['total']['annotations']} annotations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e724de65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Visualize Split Distribution\n",
    "print(\"ğŸ“ˆ Visualizing dataset splits...\")\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Images distribution\n",
    "splits = ['Train', 'Val', 'Test']\n",
    "image_counts = [stats['train']['images'], stats['val']['images'], stats['test']['images']]\n",
    "colors = ['#2E8B57', '#FF6347', '#4169E1']\n",
    "\n",
    "ax1.pie(image_counts, labels=splits, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "ax1.set_title('Image Distribution Across Splits')\n",
    "\n",
    "# Annotations distribution\n",
    "annotation_counts = [stats['train']['annotations'], stats['val']['annotations'], stats['test']['annotations']]\n",
    "\n",
    "ax2.bar(splits, annotation_counts, color=colors)\n",
    "ax2.set_title('Annotation Count by Split')\n",
    "ax2.set_ylabel('Number of Annotations')\n",
    "ax2.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, count in enumerate(annotation_counts):\n",
    "    ax2.text(i, count + max(annotation_counts) * 0.01, str(count), \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87e745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Advanced Splitting Options\n",
    "print(\"âš™ï¸ Demonstrating advanced splitting options...\")\n",
    "\n",
    "# Split with custom parameters\n",
    "advanced_stats = sv.split_coco_dataset(\n",
    "    annotations_path=\"sample_dataset/annotations.json\",\n",
    "    images_directory=\"sample_dataset/images\",\n",
    "    output_directory=\"advanced_splits\",\n",
    "    val_percentage=0.15,         # 15% validation\n",
    "    test_percentage=0.15,        # 15% test (70% train)\n",
    "    random_state=123,            # Different seed\n",
    "    verify_images=True,          # Verify images exist\n",
    "    min_annotations_per_image=2  # Only include images with 2+ annotations\n",
    ")\n",
    "\n",
    "print(\"ğŸ” Advanced Split (min 2 annotations per image):\")\n",
    "for split_name, split_stats in advanced_stats.items():\n",
    "    if split_name != 'total':\n",
    "        print(f\"   {split_name.capitalize()}: {split_stats['images']} images, {split_stats['annotations']} annotations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce72241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Verify Split Integrity\n",
    "print(\"ğŸ” Verifying split integrity...\")\n",
    "\n",
    "verification = sv.verify_coco_split(\n",
    "    train_annotations=\"dataset_splits/train_annotations.json\",\n",
    "    val_annotations=\"dataset_splits/val_annotations.json\",\n",
    "    test_annotations=\"dataset_splits/test_annotations.json\",\n",
    "    original_annotations=\"sample_dataset/annotations.json\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Verification Results:\")\n",
    "checks = [\n",
    "    (\"Valid COCO Format\", verification['valid_format']),\n",
    "    (\"No Duplicate Images\", verification['no_duplicates']),\n",
    "    (\"Complete Split\", verification['complete_split']),\n",
    "    (\"Categories Preserved\", verification['categories_preserved'])\n",
    "]\n",
    "\n",
    "for check_name, result in checks:\n",
    "    status = \"âœ… PASS\" if result else \"âŒ FAIL\"\n",
    "    print(f\"   {check_name}: {status}\")\n",
    "\n",
    "if verification['details']:\n",
    "    print(\"\\nğŸ“‹ Detailed Statistics:\")\n",
    "    details = verification['details']\n",
    "    print(f\"   Original: {details.get('original_images', 'N/A')} images, {details.get('original_annotations', 'N/A')} annotations\")\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        img_key = f'{split}_images'\n",
    "        ann_key = f'{split}_annotations'\n",
    "        if img_key in details:\n",
    "            print(f\"   {split.capitalize()}: {details[img_key]} images, {details[ann_key]} annotations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9db715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Examine Split Files\n",
    "print(\"ğŸ” Examining generated split files...\")\n",
    "\n",
    "# Load and examine a split file\n",
    "with open(\"dataset_splits/train_annotations.json\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "print(\"ğŸ“„ Train Split Structure:\")\n",
    "print(f\"   Info: {train_data['info']['description']}\")\n",
    "print(f\"   Categories: {len(train_data['categories'])}\")\n",
    "print(f\"   Images: {len(train_data['images'])}\")\n",
    "print(f\"   Annotations: {len(train_data['annotations'])}\")\n",
    "\n",
    "# Show sample image from train split\n",
    "if train_data['images']:\n",
    "    sample_img = train_data['images'][0]\n",
    "    print(f\"\\nğŸ–¼ï¸ Sample Image: {sample_img['file_name']}\")\n",
    "    print(f\"   Dimensions: {sample_img['width']}x{sample_img['height']}\")\n",
    "    \n",
    "    # Count annotations for this image\n",
    "    img_annotations = [ann for ann in train_data['annotations'] \n",
    "                      if ann['image_id'] == sample_img['id']]\n",
    "    print(f\"   Annotations: {len(img_annotations)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a3a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Category Distribution Analysis\n",
    "print(\"ğŸ“Š Analyzing category distribution across splits...\")\n",
    "\n",
    "def analyze_category_distribution(annotations_file, split_name):\n",
    "    \"\"\"Analyze category distribution in a split.\"\"\"\n",
    "    with open(annotations_file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Count annotations per category\n",
    "    category_counts = {}\n",
    "    for ann in data['annotations']:\n",
    "        cat_id = ann['category_id']\n",
    "        category_counts[cat_id] = category_counts.get(cat_id, 0) + 1\n",
    "    \n",
    "    # Get category names\n",
    "    cat_id_to_name = {cat['id']: cat['name'] for cat in data['categories']}\n",
    "    \n",
    "    print(f\"ğŸ“ˆ {split_name} Category Distribution:\")\n",
    "    for cat_id, count in sorted(category_counts.items()):\n",
    "        cat_name = cat_id_to_name.get(cat_id, f\"Unknown_{cat_id}\")\n",
    "        print(f\"   {cat_name}: {count} annotations\")\n",
    "    \n",
    "    return category_counts\n",
    "\n",
    "# Analyze all splits\n",
    "splits_to_analyze = [\n",
    "    (\"dataset_splits/train_annotations.json\", \"Train\"),\n",
    "    (\"dataset_splits/val_annotations.json\", \"Validation\"),\n",
    "    (\"dataset_splits/test_annotations.json\", \"Test\")\n",
    "]\n",
    "\n",
    "all_distributions = {}\n",
    "for file_path, split_name in splits_to_analyze:\n",
    "    if Path(file_path).exists():\n",
    "        all_distributions[split_name] = analyze_category_distribution(file_path, split_name)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f39b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Best Practices and Tips\n",
    "print(\"\"\"\n",
    "ğŸ’¡ Best Practices for COCO Dataset Splitting:\n",
    "\n",
    "1. ğŸ¯ **Consistent Random Seed**: Always use the same random_state for reproducible experiments\n",
    "   \n",
    "2. ğŸ” **Verify Images**: Enable verify_images=True to catch missing or corrupted images early\n",
    "   \n",
    "3. ğŸ“Š **Check Distribution**: Ensure each split has sufficient examples of all categories\n",
    "   \n",
    "4. ğŸ’¾ **Backup Original**: Keep your original annotations file safe before splitting\n",
    "   \n",
    "5. ğŸ›ï¸ **Adjust Ratios**: Common splits are 70/15/15 or 80/10/10 (train/val/test)\n",
    "   \n",
    "6. ğŸ“ **Minimum Annotations**: Use min_annotations_per_image to filter low-quality images\n",
    "   \n",
    "7. âœ… **Verify Splits**: Always run verify_coco_split() to ensure integrity\n",
    "   \n",
    "8. ğŸ“ **Organized Structure**: Keep split files in separate directories for clarity\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3435ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Using Splits with Popular Frameworks\n",
    "print(\"ğŸ”§ Example: Loading splits with popular frameworks...\")\n",
    "\n",
    "print(\"\"\"\n",
    "# PyTorch Example\n",
    "from torch.utils.data import Dataset\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "class COCODataset(Dataset):\n",
    "    def __init__(self, annotations_file, images_dir, transform=None):\n",
    "        self.coco = COCO(annotations_file)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.image_ids = list(self.coco.imgs.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_info = self.coco.imgs[img_id]\n",
    "        # ... load and process image ...\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = COCODataset('dataset_splits/train_annotations.json', 'sample_dataset/images')\n",
    "val_dataset = COCODataset('dataset_splits/val_annotations.json', 'sample_dataset/images')\n",
    "\n",
    "# Supervision Integration\n",
    "train_detections = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path='sample_dataset/images',\n",
    "    annotations_path='dataset_splits/train_annotations.json'\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cae819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Clean Up (Optional)\n",
    "cleanup_choice = input(\"ğŸ—‘ï¸ Clean up generated files? (y/N): \").lower().strip()\n",
    "\n",
    "if cleanup_choice == 'y':\n",
    "    import shutil\n",
    "    \n",
    "    # Remove generated directories\n",
    "    dirs_to_remove = ['sample_dataset', 'dataset_splits', 'advanced_splits']\n",
    "    \n",
    "    for dir_path in dirs_to_remove:\n",
    "        if Path(dir_path).exists():\n",
    "            shutil.rmtree(dir_path)\n",
    "            print(f\"ğŸ—‘ï¸ Removed {dir_path}\")\n",
    "    \n",
    "    print(\"âœ… Cleanup complete!\")\n",
    "else:\n",
    "    print(\"ğŸ“ Files preserved for further exploration\")\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ‰ Tutorial Complete!\n",
    "\n",
    "You've learned how to:\n",
    "âœ… Split COCO datasets efficiently with Supervision\n",
    "âœ… Verify split integrity and distribution\n",
    "âœ… Handle edge cases and missing images\n",
    "âœ… Integrate with popular ML frameworks\n",
    "\n",
    "ğŸ“š Next Steps:\n",
    "- Try with your own COCO dataset\n",
    "- Experiment with different split ratios\n",
    "- Integrate with your training pipeline\n",
    "- Explore other Supervision features\n",
    "\n",
    "ğŸ”— Resources:\n",
    "- Supervision Documentation: https://supervision.roboflow.com/\n",
    "- GitHub Repository: https://github.com/roboflow/supervision\n",
    "- Community: https://github.com/roboflow/supervision/discussions\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
